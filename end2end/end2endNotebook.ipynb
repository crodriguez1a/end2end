{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import librosa\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from model import EnvNet\n",
    "from train import train_model\n",
    "from data_preprocess import make_frames,make_frames_folder\n",
    "from tensorflow.keras.models import Sequential,Model\n",
    "from tensorflow.keras.layers import Conv1D,MaxPooling1D,Dense,Flatten,BatchNormalization,Dropout, Activation\n",
    "from gammatone_init import GammatoneInit\n",
    "from gammatone_init import generate_filters\n",
    "from model_config import *\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import pandas as pd\n",
    "import ast\n",
    "import wandb\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "!export CUDA_VISIBLE_DEVICES=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([54033, 16001])\n"
     ]
    }
   ],
   "source": [
    "frame_length = 16000\n",
    "overlapping_fraction = 0.5\n",
    "data = torch.load('./torch_dataset_16khz/all_audio_data.pt')\n",
    "print(data.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([54033, 16001])\n",
      "54033\n"
     ]
    }
   ],
   "source": [
    "def to_categorical(tensors, num_classes=10):\n",
    "    return torch.eye(num_classes)[y.int()]\n",
    "print(data.size())\n",
    "tensor_size = (data.size())[0]\n",
    "print(tensor_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = data[:,0:frame_length].clone()\n",
    "test_portion = int(0.8*((data.size())[0]))\n",
    "#print(test_portion)\n",
    "X_train = data[:test_portion, 0:frame_length].clone()\n",
    "#print(X_train.size())\n",
    "Y_train = data[:test_portion,frame_length:].clone()\n",
    "\n",
    "X_train = X_train.reshape(-1,16,1000)\n",
    "\n",
    "#print(X_train.size())\n",
    "#print(Y_train.size())\n",
    "#print(Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train = Y_train.type(torch.LongTensor)\n",
    "Y_train_one_hot = F.one_hot(Y_train)\n",
    "#print(Y_train_one_hot)\n",
    "#print(Y_train)\n",
    "audio_dataset = TensorDataset (X_train, Y_train)\n",
    "audio_dataloader = DataLoader (audio_dataset, batch_size = 300, shuffle= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_rate = 16000\n",
    "min_center_freq = 100\n",
    "order = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMSLELoss(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.mse = torch.nn.MSELoss()\n",
    "        \n",
    "    def forward(self, pred, actual):\n",
    "        return self.mse(torch.log(pred.float() + 1), torch.log(actual.float() + 1))\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 10])\n",
      "torch.Size([100])\n"
     ]
    }
   ],
   "source": [
    "loss_function = torch.nn.CrossEntropyLoss()\n",
    "input = torch.randn(100, 10, requires_grad=True)\n",
    "target = torch.empty(100, dtype=torch.long).random_(5)\n",
    "print(input.size())\n",
    "print(target.size())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch-0 lr: 0.001\n",
      "iteration:0 loss:2.3018529415130615 \n",
      "iteration:1 loss:2.2982075214385986 \n",
      "iteration:2 loss:2.3008296489715576 \n",
      "iteration:3 loss:2.298231840133667 \n",
      "iteration:4 loss:2.2983298301696777 \n",
      "iteration:5 loss:2.2966344356536865 \n",
      "iteration:6 loss:2.294978380203247 \n",
      "iteration:7 loss:2.287743091583252 \n",
      "iteration:8 loss:2.2920918464660645 \n",
      "iteration:9 loss:2.2912333011627197 \n",
      "iteration:10 loss:2.2845652103424072 \n",
      "iteration:11 loss:2.278446674346924 \n",
      "iteration:12 loss:2.2861390113830566 \n",
      "iteration:13 loss:2.2782137393951416 \n",
      "iteration:14 loss:2.277071237564087 \n",
      "iteration:15 loss:2.2739617824554443 \n",
      "iteration:16 loss:2.269221544265747 \n",
      "iteration:17 loss:2.2611501216888428 \n",
      "iteration:18 loss:2.2705183029174805 \n",
      "iteration:19 loss:2.262052536010742 \n",
      "iteration:20 loss:2.2374393939971924 \n",
      "iteration:21 loss:2.247450590133667 \n",
      "iteration:22 loss:2.2481741905212402 \n",
      "iteration:23 loss:2.256747007369995 \n",
      "iteration:24 loss:2.2415664196014404 \n",
      "iteration:25 loss:2.242741584777832 \n",
      "iteration:26 loss:2.2409489154815674 \n",
      "iteration:27 loss:2.230849504470825 \n",
      "iteration:28 loss:2.2352614402770996 \n",
      "iteration:29 loss:2.232983112335205 \n",
      "iteration:30 loss:2.215684413909912 \n",
      "iteration:31 loss:2.234623908996582 \n",
      "iteration:32 loss:2.232227087020874 \n",
      "iteration:33 loss:2.2252237796783447 \n",
      "iteration:34 loss:2.1863694190979004 \n",
      "iteration:35 loss:2.224337339401245 \n",
      "iteration:36 loss:2.2383720874786377 \n",
      "iteration:37 loss:2.236208200454712 \n",
      "iteration:38 loss:2.2300755977630615 \n",
      "iteration:39 loss:2.218219518661499 \n",
      "iteration:40 loss:2.223278522491455 \n",
      "iteration:41 loss:2.2012977600097656 \n",
      "iteration:42 loss:2.188471794128418 \n",
      "iteration:43 loss:2.2061405181884766 \n",
      "iteration:44 loss:2.2138657569885254 \n",
      "iteration:45 loss:2.208833932876587 \n",
      "iteration:46 loss:2.1931798458099365 \n",
      "iteration:47 loss:2.200254440307617 \n",
      "iteration:48 loss:2.1950833797454834 \n",
      "iteration:49 loss:2.173239231109619 \n",
      "iteration:50 loss:2.2017481327056885 \n",
      "iteration:51 loss:2.1977901458740234 \n",
      "iteration:52 loss:2.1733264923095703 \n",
      "iteration:53 loss:2.1859138011932373 \n",
      "iteration:54 loss:2.2277772426605225 \n",
      "iteration:55 loss:2.192650079727173 \n",
      "iteration:56 loss:2.2049436569213867 \n",
      "iteration:57 loss:2.1715543270111084 \n",
      "iteration:58 loss:2.166146755218506 \n",
      "iteration:59 loss:2.1388330459594727 \n",
      "iteration:60 loss:2.1741180419921875 \n",
      "iteration:61 loss:2.183420181274414 \n",
      "iteration:62 loss:2.153773546218872 \n",
      "iteration:63 loss:2.165170431137085 \n",
      "iteration:64 loss:2.161111831665039 \n",
      "iteration:65 loss:2.1620845794677734 \n",
      "iteration:66 loss:2.1595730781555176 \n",
      "iteration:67 loss:2.1782612800598145 \n",
      "iteration:68 loss:2.189826726913452 \n",
      "iteration:69 loss:2.1702208518981934 \n",
      "iteration:70 loss:2.1741206645965576 \n",
      "iteration:71 loss:2.1587603092193604 \n",
      "iteration:72 loss:2.1635828018188477 \n",
      "iteration:73 loss:2.1349973678588867 \n",
      "iteration:74 loss:2.1482667922973633 \n",
      "iteration:75 loss:2.1259379386901855 \n",
      "iteration:76 loss:2.154599905014038 \n",
      "iteration:77 loss:2.154743194580078 \n",
      "iteration:78 loss:2.1412713527679443 \n",
      "iteration:79 loss:2.1826043128967285 \n",
      "iteration:80 loss:2.133194923400879 \n",
      "iteration:81 loss:2.142944097518921 \n",
      "iteration:82 loss:2.139458656311035 \n",
      "iteration:83 loss:2.137791872024536 \n",
      "iteration:84 loss:2.1446433067321777 \n",
      "iteration:85 loss:2.1386680603027344 \n",
      "iteration:86 loss:2.124824285507202 \n",
      "iteration:87 loss:2.1616244316101074 \n",
      "iteration:88 loss:2.125121831893921 \n",
      "iteration:89 loss:2.1231460571289062 \n",
      "iteration:90 loss:2.1444382667541504 \n",
      "iteration:91 loss:2.1673595905303955 \n",
      "iteration:92 loss:2.13443922996521 \n",
      "iteration:93 loss:2.125454902648926 \n",
      "iteration:94 loss:2.141385793685913 \n",
      "iteration:95 loss:2.1340725421905518 \n",
      "iteration:96 loss:2.087449550628662 \n",
      "iteration:97 loss:2.147374391555786 \n",
      "iteration:98 loss:2.1001107692718506 \n",
      "iteration:99 loss:2.139273166656494 \n",
      "iteration:100 loss:2.1143643856048584 \n",
      "iteration:101 loss:2.1132383346557617 \n",
      "iteration:102 loss:2.115546226501465 \n",
      "iteration:103 loss:2.082559585571289 \n",
      "iteration:104 loss:2.1201236248016357 \n",
      "iteration:105 loss:2.0827457904815674 \n",
      "iteration:106 loss:2.1218271255493164 \n",
      "iteration:107 loss:2.083812952041626 \n",
      "iteration:108 loss:2.1356313228607178 \n",
      "iteration:109 loss:2.11000919342041 \n",
      "iteration:110 loss:2.120288133621216 \n",
      "iteration:111 loss:2.120314836502075 \n",
      "iteration:112 loss:2.1096065044403076 \n",
      "iteration:113 loss:2.1124134063720703 \n",
      "iteration:114 loss:2.1532649993896484 \n",
      "iteration:115 loss:2.1047520637512207 \n",
      "iteration:116 loss:2.0964832305908203 \n",
      "iteration:117 loss:2.113438606262207 \n",
      "iteration:118 loss:2.103408098220825 \n",
      "iteration:119 loss:2.1257171630859375 \n",
      "iteration:120 loss:2.073216676712036 \n",
      "iteration:121 loss:2.0704216957092285 \n",
      "iteration:122 loss:2.139132022857666 \n",
      "iteration:123 loss:2.108992099761963 \n",
      "iteration:124 loss:2.1061129570007324 \n",
      "iteration:125 loss:2.0489730834960938 \n",
      "iteration:126 loss:2.10111141204834 \n",
      "iteration:127 loss:2.1255264282226562 \n",
      "iteration:128 loss:2.051401138305664 \n",
      "iteration:129 loss:2.120349645614624 \n",
      "iteration:130 loss:2.1333465576171875 \n",
      "iteration:131 loss:2.1038975715637207 \n",
      "iteration:132 loss:2.066727638244629 \n",
      "iteration:133 loss:2.0822718143463135 \n",
      "iteration:134 loss:2.0793919563293457 \n",
      "iteration:135 loss:2.0331127643585205 \n",
      "iteration:136 loss:2.0597708225250244 \n",
      "iteration:137 loss:2.056406021118164 \n",
      "iteration:138 loss:2.0497255325317383 \n",
      "iteration:139 loss:2.0564990043640137 \n",
      "iteration:140 loss:2.0531017780303955 \n",
      "iteration:141 loss:2.050776720046997 \n",
      "iteration:142 loss:2.1018779277801514 \n",
      "iteration:143 loss:2.089945077896118 \n",
      "iteration:144 loss:2.0846478939056396 \n",
      "Epoch-1 lr: 0.0009990133642141358\n",
      "iteration:145 loss:2.0884299278259277 \n",
      "iteration:146 loss:2.084498405456543 \n",
      "iteration:147 loss:2.03536319732666 \n",
      "iteration:148 loss:2.08608341217041 \n",
      "iteration:149 loss:2.0098581314086914 \n",
      "iteration:150 loss:2.0653791427612305 \n",
      "iteration:151 loss:2.1017849445343018 \n",
      "iteration:152 loss:2.0749261379241943 \n",
      "iteration:153 loss:2.0975823402404785 \n",
      "iteration:154 loss:2.067101240158081 \n",
      "iteration:155 loss:2.0346224308013916 \n",
      "iteration:156 loss:2.1155529022216797 \n",
      "iteration:157 loss:2.071211338043213 \n",
      "iteration:158 loss:2.0541117191314697 \n",
      "iteration:159 loss:2.0855438709259033 \n",
      "iteration:160 loss:2.099405527114868 \n",
      "iteration:161 loss:2.0194334983825684 \n",
      "iteration:162 loss:2.033245325088501 \n",
      "iteration:163 loss:2.0618505477905273 \n",
      "iteration:164 loss:2.060795783996582 \n",
      "iteration:165 loss:2.0094761848449707 \n",
      "iteration:166 loss:2.0586013793945312 \n",
      "iteration:167 loss:2.0320141315460205 \n",
      "iteration:168 loss:2.0845656394958496 \n",
      "iteration:169 loss:2.0461835861206055 \n",
      "iteration:170 loss:2.065713405609131 \n",
      "iteration:171 loss:2.0757455825805664 \n",
      "iteration:172 loss:2.0163679122924805 \n",
      "iteration:173 loss:2.0985336303710938 \n",
      "iteration:174 loss:2.1049718856811523 \n",
      "iteration:175 loss:2.0950400829315186 \n",
      "iteration:176 loss:2.0475733280181885 \n",
      "iteration:177 loss:2.056966543197632 \n",
      "iteration:178 loss:2.1060454845428467 \n",
      "iteration:179 loss:2.0420775413513184 \n",
      "iteration:180 loss:2.009223461151123 \n",
      "iteration:181 loss:2.0543951988220215 \n",
      "iteration:182 loss:2.0708887577056885 \n",
      "iteration:183 loss:2.097053289413452 \n",
      "iteration:184 loss:2.0371780395507812 \n",
      "iteration:185 loss:2.0779266357421875 \n",
      "iteration:186 loss:2.0703961849212646 \n",
      "iteration:187 loss:2.046663284301758 \n",
      "iteration:188 loss:2.028215169906616 \n",
      "iteration:189 loss:2.0599842071533203 \n",
      "iteration:190 loss:2.041919231414795 \n",
      "iteration:191 loss:2.051434278488159 \n",
      "iteration:192 loss:2.01068377494812 \n",
      "iteration:193 loss:2.0452680587768555 \n",
      "iteration:194 loss:2.0580697059631348 \n",
      "iteration:195 loss:2.0353994369506836 \n",
      "iteration:196 loss:2.054499387741089 \n",
      "iteration:197 loss:2.0091729164123535 \n",
      "iteration:198 loss:2.0879364013671875 \n",
      "iteration:199 loss:2.001887321472168 \n",
      "iteration:200 loss:2.0397772789001465 \n",
      "iteration:201 loss:2.100391149520874 \n",
      "iteration:202 loss:2.0539965629577637 \n",
      "iteration:203 loss:2.106062650680542 \n",
      "iteration:204 loss:2.011481523513794 \n",
      "iteration:205 loss:2.01755952835083 \n",
      "iteration:206 loss:2.0738158226013184 \n",
      "iteration:207 loss:1.9938527345657349 \n",
      "iteration:208 loss:2.0366899967193604 \n",
      "iteration:209 loss:2.0203957557678223 \n",
      "iteration:210 loss:2.0317611694335938 \n",
      "iteration:211 loss:2.037534475326538 \n",
      "iteration:212 loss:2.069815158843994 \n",
      "iteration:213 loss:2.0420351028442383 \n",
      "iteration:214 loss:2.0540192127227783 \n",
      "iteration:215 loss:2.035310983657837 \n",
      "iteration:216 loss:2.053527593612671 \n",
      "iteration:217 loss:2.0367953777313232 \n",
      "iteration:218 loss:2.0446109771728516 \n",
      "iteration:219 loss:2.0588154792785645 \n",
      "iteration:220 loss:2.051117181777954 \n",
      "iteration:221 loss:2.0448157787323 \n",
      "iteration:222 loss:2.0168650150299072 \n",
      "iteration:223 loss:1.9736183881759644 \n",
      "iteration:224 loss:2.0939249992370605 \n",
      "iteration:225 loss:2.0555427074432373 \n",
      "iteration:226 loss:2.1047861576080322 \n",
      "iteration:227 loss:2.048110008239746 \n",
      "iteration:228 loss:2.0472588539123535 \n",
      "iteration:229 loss:2.0361249446868896 \n",
      "iteration:230 loss:2.0131421089172363 \n",
      "iteration:231 loss:2.0639357566833496 \n",
      "iteration:232 loss:2.0432119369506836 \n",
      "iteration:233 loss:2.0403499603271484 \n",
      "iteration:234 loss:2.0746374130249023 \n",
      "iteration:235 loss:1.9737805128097534 \n",
      "iteration:236 loss:2.062866687774658 \n",
      "iteration:237 loss:1.9907716512680054 \n",
      "iteration:238 loss:2.042759418487549 \n",
      "iteration:239 loss:2.017914295196533 \n",
      "iteration:240 loss:2.0685224533081055 \n",
      "iteration:241 loss:2.055544137954712 \n",
      "iteration:242 loss:2.0626766681671143 \n",
      "iteration:243 loss:1.999312162399292 \n",
      "iteration:244 loss:2.0492749214172363 \n",
      "iteration:245 loss:2.0124034881591797 \n",
      "iteration:246 loss:2.0443456172943115 \n",
      "iteration:247 loss:1.990996241569519 \n",
      "iteration:248 loss:2.042013645172119 \n",
      "iteration:249 loss:2.053779363632202 \n",
      "iteration:250 loss:2.028996467590332 \n",
      "iteration:251 loss:2.070686101913452 \n",
      "iteration:252 loss:2.0574378967285156 \n",
      "iteration:253 loss:2.0530834197998047 \n",
      "iteration:254 loss:2.0367212295532227 \n",
      "iteration:255 loss:2.0274195671081543 \n",
      "iteration:256 loss:2.0686166286468506 \n",
      "iteration:257 loss:2.0519189834594727 \n",
      "iteration:258 loss:2.033595561981201 \n",
      "iteration:259 loss:2.038747787475586 \n",
      "iteration:260 loss:2.029919147491455 \n",
      "iteration:261 loss:2.0323104858398438 \n",
      "iteration:262 loss:2.0341548919677734 \n",
      "iteration:263 loss:2.0551564693450928 \n",
      "iteration:264 loss:1.9708369970321655 \n",
      "iteration:265 loss:2.0347838401794434 \n",
      "iteration:266 loss:2.0257298946380615 \n",
      "iteration:267 loss:2.0229902267456055 \n",
      "iteration:268 loss:2.0010881423950195 \n",
      "iteration:269 loss:2.0103750228881836 \n",
      "iteration:270 loss:2.0206804275512695 \n",
      "iteration:271 loss:2.0092639923095703 \n",
      "iteration:272 loss:2.021256685256958 \n",
      "iteration:273 loss:2.0307083129882812 \n",
      "iteration:274 loss:2.0445425510406494 \n",
      "iteration:275 loss:2.0498740673065186 \n",
      "iteration:276 loss:2.0004451274871826 \n",
      "iteration:277 loss:2.0426340103149414 \n",
      "iteration:278 loss:2.0364201068878174 \n",
      "iteration:279 loss:1.9996016025543213 \n",
      "iteration:280 loss:2.0477521419525146 \n",
      "iteration:281 loss:2.0157015323638916 \n",
      "iteration:282 loss:2.0743489265441895 \n",
      "iteration:283 loss:1.9782519340515137 \n",
      "iteration:284 loss:2.0447657108306885 \n",
      "iteration:285 loss:2.0183651447296143 \n",
      "iteration:286 loss:2.0612292289733887 \n",
      "iteration:287 loss:2.023077964782715 \n",
      "iteration:288 loss:2.002484083175659 \n",
      "iteration:289 loss:2.000633478164673 \n",
      "Epoch-2 lr: 0.000996057350657239\n",
      "iteration:290 loss:2.023287057876587 \n",
      "iteration:291 loss:2.0133066177368164 \n",
      "iteration:292 loss:1.9692360162734985 \n",
      "iteration:293 loss:2.019573926925659 \n",
      "iteration:294 loss:2.0168490409851074 \n",
      "iteration:295 loss:2.0183722972869873 \n",
      "iteration:296 loss:2.054109811782837 \n",
      "iteration:297 loss:1.9984793663024902 \n",
      "iteration:298 loss:2.004074811935425 \n",
      "iteration:299 loss:1.9692305326461792 \n",
      "iteration:300 loss:1.9586189985275269 \n",
      "iteration:301 loss:2.030282735824585 \n",
      "iteration:302 loss:1.9432560205459595 \n",
      "iteration:303 loss:2.000311851501465 \n",
      "iteration:304 loss:2.0245308876037598 \n",
      "iteration:305 loss:1.9838889837265015 \n",
      "iteration:306 loss:2.0050787925720215 \n",
      "iteration:307 loss:2.007723808288574 \n",
      "iteration:308 loss:1.9545624256134033 \n",
      "iteration:309 loss:2.04185152053833 \n",
      "iteration:310 loss:1.996978759765625 \n",
      "iteration:311 loss:2.02998423576355 \n",
      "iteration:312 loss:1.9791518449783325 \n",
      "iteration:313 loss:1.9695721864700317 \n",
      "iteration:314 loss:1.9770007133483887 \n",
      "iteration:315 loss:2.0439655780792236 \n",
      "iteration:316 loss:1.999566674232483 \n",
      "iteration:317 loss:2.003838062286377 \n",
      "iteration:318 loss:2.034315347671509 \n",
      "iteration:319 loss:2.022543430328369 \n",
      "iteration:320 loss:2.034303665161133 \n",
      "iteration:321 loss:2.038837194442749 \n",
      "iteration:322 loss:2.0191023349761963 \n",
      "iteration:323 loss:1.9854695796966553 \n",
      "iteration:324 loss:1.9827988147735596 \n",
      "iteration:325 loss:2.0335352420806885 \n",
      "iteration:326 loss:2.0257840156555176 \n",
      "iteration:327 loss:1.9757460355758667 \n",
      "iteration:328 loss:1.9949098825454712 \n",
      "iteration:329 loss:2.014246940612793 \n",
      "iteration:330 loss:2.038331985473633 \n",
      "iteration:331 loss:2.0049490928649902 \n",
      "iteration:332 loss:1.9587281942367554 \n",
      "iteration:333 loss:2.0174806118011475 \n",
      "iteration:334 loss:2.034153938293457 \n",
      "iteration:335 loss:1.9857454299926758 \n",
      "iteration:336 loss:2.022526979446411 \n",
      "iteration:337 loss:2.0103073120117188 \n",
      "iteration:338 loss:2.0022079944610596 \n",
      "iteration:339 loss:2.0163395404815674 \n",
      "iteration:340 loss:2.0396132469177246 \n",
      "iteration:341 loss:2.0399649143218994 \n",
      "iteration:342 loss:2.0059945583343506 \n",
      "iteration:343 loss:2.0619616508483887 \n",
      "iteration:344 loss:1.9782181978225708 \n",
      "iteration:345 loss:2.0726280212402344 \n",
      "iteration:346 loss:1.9975528717041016 \n",
      "iteration:347 loss:1.9926881790161133 \n",
      "iteration:348 loss:2.0402255058288574 \n",
      "iteration:349 loss:2.0705044269561768 \n",
      "iteration:350 loss:2.019697427749634 \n",
      "iteration:351 loss:1.9848295450210571 \n",
      "iteration:352 loss:2.0140609741210938 \n",
      "iteration:353 loss:2.012042760848999 \n",
      "iteration:354 loss:2.001255512237549 \n",
      "iteration:355 loss:1.9933805465698242 \n",
      "iteration:356 loss:1.984517216682434 \n",
      "iteration:357 loss:1.9760099649429321 \n",
      "iteration:358 loss:1.987989068031311 \n",
      "iteration:359 loss:2.0564076900482178 \n",
      "iteration:360 loss:2.0137827396392822 \n",
      "iteration:361 loss:1.9970282316207886 \n",
      "iteration:362 loss:1.9545207023620605 \n",
      "iteration:363 loss:1.9936710596084595 \n",
      "iteration:364 loss:2.038686990737915 \n",
      "iteration:365 loss:2.0115318298339844 \n",
      "iteration:366 loss:1.9606136083602905 \n",
      "iteration:367 loss:2.0182554721832275 \n",
      "iteration:368 loss:2.032533645629883 \n",
      "iteration:369 loss:2.034832000732422 \n",
      "iteration:370 loss:2.001646041870117 \n",
      "iteration:371 loss:1.9585477113723755 \n",
      "iteration:372 loss:2.0244827270507812 \n",
      "iteration:373 loss:2.0052413940429688 \n",
      "iteration:374 loss:1.9867171049118042 \n",
      "iteration:375 loss:2.0178380012512207 \n",
      "iteration:376 loss:1.9461350440979004 \n",
      "iteration:377 loss:1.9977011680603027 \n",
      "iteration:378 loss:2.004568576812744 \n",
      "iteration:379 loss:1.9707822799682617 \n",
      "iteration:380 loss:2.001817226409912 \n",
      "iteration:381 loss:1.9648720026016235 \n",
      "iteration:382 loss:1.995566964149475 \n",
      "iteration:383 loss:1.949950933456421 \n",
      "iteration:384 loss:1.9848718643188477 \n",
      "iteration:385 loss:2.0440216064453125 \n",
      "iteration:386 loss:2.0040080547332764 \n",
      "iteration:387 loss:1.9457498788833618 \n",
      "iteration:388 loss:1.9352264404296875 \n",
      "iteration:389 loss:2.01225209236145 \n",
      "iteration:390 loss:1.9715476036071777 \n",
      "iteration:391 loss:2.040990114212036 \n",
      "iteration:392 loss:2.017305612564087 \n",
      "iteration:393 loss:1.9974141120910645 \n",
      "iteration:394 loss:1.9999263286590576 \n",
      "iteration:395 loss:1.9819244146347046 \n",
      "iteration:396 loss:2.0273048877716064 \n",
      "iteration:397 loss:2.025007724761963 \n",
      "iteration:398 loss:1.9764070510864258 \n",
      "iteration:399 loss:2.0141513347625732 \n",
      "iteration:400 loss:1.9948972463607788 \n",
      "iteration:401 loss:2.000922203063965 \n",
      "iteration:402 loss:2.0040221214294434 \n",
      "iteration:403 loss:2.015138864517212 \n",
      "iteration:404 loss:2.0157310962677 \n",
      "iteration:405 loss:2.0346267223358154 \n",
      "iteration:406 loss:1.990841031074524 \n",
      "iteration:407 loss:2.0368242263793945 \n",
      "iteration:408 loss:2.0251665115356445 \n",
      "iteration:409 loss:1.9446823596954346 \n",
      "iteration:410 loss:1.992777705192566 \n",
      "iteration:411 loss:2.0288150310516357 \n",
      "iteration:412 loss:1.9452303647994995 \n",
      "iteration:413 loss:2.0338399410247803 \n",
      "iteration:414 loss:1.9590002298355103 \n",
      "iteration:415 loss:1.9893568754196167 \n",
      "iteration:416 loss:2.0077531337738037 \n",
      "iteration:417 loss:2.049093723297119 \n",
      "iteration:418 loss:1.986897587776184 \n",
      "iteration:419 loss:1.9897332191467285 \n",
      "iteration:420 loss:2.028377056121826 \n",
      "iteration:421 loss:2.0135154724121094 \n",
      "iteration:422 loss:1.9911938905715942 \n",
      "iteration:423 loss:1.9974641799926758 \n",
      "iteration:424 loss:1.9917869567871094 \n",
      "iteration:425 loss:1.995671033859253 \n",
      "iteration:426 loss:1.9606668949127197 \n",
      "iteration:427 loss:2.008539915084839 \n",
      "iteration:428 loss:1.974457025527954 \n",
      "iteration:429 loss:2.016780138015747 \n",
      "iteration:430 loss:2.0119543075561523 \n",
      "iteration:431 loss:1.9583719968795776 \n",
      "iteration:432 loss:1.9993300437927246 \n",
      "iteration:433 loss:1.9563733339309692 \n",
      "iteration:434 loss:1.9507195949554443 \n",
      "Epoch-3 lr: 0.0009911436253643444\n",
      "iteration:435 loss:1.9676392078399658 \n",
      "iteration:436 loss:2.008068561553955 \n",
      "iteration:437 loss:1.9896934032440186 \n",
      "iteration:438 loss:1.9992702007293701 \n",
      "iteration:439 loss:1.9574129581451416 \n",
      "iteration:440 loss:1.993692398071289 \n",
      "iteration:441 loss:1.980690360069275 \n",
      "iteration:442 loss:1.9596093893051147 \n",
      "iteration:443 loss:1.9920005798339844 \n",
      "iteration:444 loss:1.9685567617416382 \n",
      "iteration:445 loss:1.9954217672348022 \n",
      "iteration:446 loss:1.9814677238464355 \n",
      "iteration:447 loss:1.9474461078643799 \n",
      "iteration:448 loss:1.9538660049438477 \n",
      "iteration:449 loss:1.959106683731079 \n",
      "iteration:450 loss:1.9491442441940308 \n",
      "iteration:451 loss:2.009427785873413 \n",
      "iteration:452 loss:1.9755398035049438 \n",
      "iteration:453 loss:1.959397792816162 \n",
      "iteration:454 loss:2.014249563217163 \n",
      "iteration:455 loss:1.979891300201416 \n",
      "iteration:456 loss:1.9807853698730469 \n",
      "iteration:457 loss:1.9919214248657227 \n",
      "iteration:458 loss:1.967108130455017 \n",
      "iteration:459 loss:1.9942004680633545 \n",
      "iteration:460 loss:1.9883091449737549 \n",
      "iteration:461 loss:2.011787176132202 \n",
      "iteration:462 loss:2.0170209407806396 \n",
      "iteration:463 loss:1.977745532989502 \n",
      "iteration:464 loss:1.9850614070892334 \n",
      "iteration:465 loss:1.9713082313537598 \n",
      "iteration:466 loss:1.966016411781311 \n",
      "iteration:467 loss:1.968245267868042 \n",
      "iteration:468 loss:1.9442368745803833 \n",
      "iteration:469 loss:1.981902003288269 \n",
      "iteration:470 loss:2.014256238937378 \n",
      "iteration:471 loss:1.9163280725479126 \n",
      "iteration:472 loss:1.9443751573562622 \n",
      "iteration:473 loss:1.9492392539978027 \n",
      "iteration:474 loss:1.968275785446167 \n",
      "iteration:475 loss:1.9770853519439697 \n",
      "iteration:476 loss:1.9873836040496826 \n",
      "iteration:477 loss:1.9725093841552734 \n",
      "iteration:478 loss:1.9569313526153564 \n",
      "iteration:479 loss:1.9503545761108398 \n",
      "iteration:480 loss:1.9771205186843872 \n",
      "iteration:481 loss:1.9974365234375 \n",
      "iteration:482 loss:2.0338327884674072 \n",
      "iteration:483 loss:1.986619234085083 \n",
      "iteration:484 loss:1.9806376695632935 \n",
      "iteration:485 loss:1.9813692569732666 \n",
      "iteration:486 loss:1.969057559967041 \n",
      "iteration:487 loss:1.9807578325271606 \n",
      "iteration:488 loss:2.0012996196746826 \n",
      "iteration:489 loss:1.9549016952514648 \n",
      "iteration:490 loss:1.9714021682739258 \n",
      "iteration:491 loss:1.951198697090149 \n",
      "iteration:492 loss:1.9243597984313965 \n",
      "iteration:493 loss:1.9474778175354004 \n",
      "iteration:494 loss:1.9920721054077148 \n",
      "iteration:495 loss:1.9655202627182007 \n",
      "iteration:496 loss:1.9799813032150269 \n",
      "iteration:497 loss:2.013698101043701 \n",
      "iteration:498 loss:2.040903091430664 \n",
      "iteration:499 loss:1.9903297424316406 \n",
      "iteration:500 loss:2.0159924030303955 \n",
      "iteration:501 loss:1.9714772701263428 \n",
      "iteration:502 loss:2.0065040588378906 \n",
      "iteration:503 loss:2.0082738399505615 \n",
      "iteration:504 loss:1.9874953031539917 \n",
      "iteration:505 loss:2.005337715148926 \n",
      "iteration:506 loss:1.963407039642334 \n",
      "iteration:507 loss:1.98454749584198 \n",
      "iteration:508 loss:2.015939235687256 \n",
      "iteration:509 loss:1.999650478363037 \n",
      "iteration:510 loss:1.9938938617706299 \n",
      "iteration:511 loss:2.0299453735351562 \n",
      "iteration:512 loss:1.961052656173706 \n",
      "iteration:513 loss:2.026298761367798 \n",
      "iteration:514 loss:1.9373667240142822 \n",
      "iteration:515 loss:1.9712305068969727 \n",
      "iteration:516 loss:2.020423173904419 \n",
      "iteration:517 loss:1.9858951568603516 \n",
      "iteration:518 loss:1.9163135290145874 \n",
      "iteration:519 loss:2.0002188682556152 \n",
      "iteration:520 loss:1.9851232767105103 \n",
      "iteration:521 loss:1.9940260648727417 \n",
      "iteration:522 loss:1.9927221536636353 \n",
      "iteration:523 loss:1.959563136100769 \n",
      "iteration:524 loss:1.9320204257965088 \n",
      "iteration:525 loss:1.9502313137054443 \n",
      "iteration:526 loss:1.9728834629058838 \n",
      "iteration:527 loss:1.9660149812698364 \n",
      "iteration:528 loss:1.9512298107147217 \n",
      "iteration:529 loss:1.9605937004089355 \n",
      "iteration:530 loss:1.948249101638794 \n",
      "iteration:531 loss:1.987463116645813 \n",
      "iteration:532 loss:2.001352310180664 \n",
      "iteration:533 loss:1.9792730808258057 \n",
      "iteration:534 loss:2.0106308460235596 \n",
      "iteration:535 loss:2.0214684009552 \n",
      "iteration:536 loss:1.966166615486145 \n",
      "iteration:537 loss:1.9819415807724 \n",
      "iteration:538 loss:1.9900121688842773 \n",
      "iteration:539 loss:1.9891481399536133 \n",
      "iteration:540 loss:1.9846516847610474 \n",
      "iteration:541 loss:1.9058247804641724 \n",
      "iteration:542 loss:1.98613440990448 \n",
      "iteration:543 loss:1.9994972944259644 \n",
      "iteration:544 loss:1.9610854387283325 \n",
      "iteration:545 loss:1.9976060390472412 \n",
      "iteration:546 loss:1.9874756336212158 \n",
      "iteration:547 loss:1.9768213033676147 \n",
      "iteration:548 loss:1.9324802160263062 \n",
      "iteration:549 loss:1.9517582654953003 \n",
      "iteration:550 loss:2.0092220306396484 \n",
      "iteration:551 loss:1.992998480796814 \n",
      "iteration:552 loss:1.9790942668914795 \n",
      "iteration:553 loss:2.018488645553589 \n",
      "iteration:554 loss:1.9775620698928833 \n",
      "iteration:555 loss:1.9714837074279785 \n",
      "iteration:556 loss:1.99506413936615 \n",
      "iteration:557 loss:2.0358424186706543 \n",
      "iteration:558 loss:1.9221577644348145 \n",
      "iteration:559 loss:1.9552268981933594 \n",
      "iteration:560 loss:2.0061938762664795 \n",
      "iteration:561 loss:2.0164403915405273 \n",
      "iteration:562 loss:1.9945611953735352 \n",
      "iteration:563 loss:1.982239007949829 \n",
      "iteration:564 loss:1.9233797788619995 \n",
      "iteration:565 loss:1.9554321765899658 \n",
      "iteration:566 loss:1.9691990613937378 \n",
      "iteration:567 loss:1.9572184085845947 \n",
      "iteration:568 loss:1.9742252826690674 \n",
      "iteration:569 loss:1.967738389968872 \n",
      "iteration:570 loss:1.9956268072128296 \n",
      "iteration:571 loss:1.9729681015014648 \n",
      "iteration:572 loss:1.9992588758468628 \n",
      "iteration:573 loss:2.0071752071380615 \n",
      "iteration:574 loss:1.9400758743286133 \n",
      "iteration:575 loss:2.011259078979492 \n",
      "iteration:576 loss:1.9782246351242065 \n",
      "iteration:577 loss:1.985375165939331 \n",
      "iteration:578 loss:1.9885562658309937 \n",
      "iteration:579 loss:2.094050884246826 \n",
      "Epoch-4 lr: 0.0009842915805643156\n",
      "iteration:580 loss:1.9703446626663208 \n",
      "iteration:581 loss:1.9549037218093872 \n",
      "iteration:582 loss:1.9940747022628784 \n",
      "iteration:583 loss:1.9820810556411743 \n",
      "iteration:584 loss:1.9527260065078735 \n",
      "iteration:585 loss:1.9461278915405273 \n",
      "iteration:586 loss:1.9559450149536133 \n",
      "iteration:587 loss:1.9803749322891235 \n",
      "iteration:588 loss:1.9343898296356201 \n",
      "iteration:589 loss:1.9448866844177246 \n",
      "iteration:590 loss:1.9045377969741821 \n",
      "iteration:591 loss:1.9596425294876099 \n",
      "iteration:592 loss:1.9434814453125 \n",
      "iteration:593 loss:1.9509835243225098 \n",
      "iteration:594 loss:1.9316294193267822 \n",
      "iteration:595 loss:1.9792019128799438 \n",
      "iteration:596 loss:2.0037808418273926 \n",
      "iteration:597 loss:1.9200992584228516 \n",
      "iteration:598 loss:1.9676496982574463 \n",
      "iteration:599 loss:1.9923248291015625 \n",
      "iteration:600 loss:1.9327378273010254 \n",
      "iteration:601 loss:1.9862446784973145 \n",
      "iteration:602 loss:1.9283756017684937 \n",
      "iteration:603 loss:2.018995523452759 \n",
      "iteration:604 loss:1.9465277194976807 \n",
      "iteration:605 loss:1.9874131679534912 \n",
      "iteration:606 loss:1.9831435680389404 \n",
      "iteration:607 loss:1.948053002357483 \n",
      "iteration:608 loss:1.972920298576355 \n",
      "iteration:609 loss:1.9729925394058228 \n",
      "iteration:610 loss:1.9434865713119507 \n",
      "iteration:611 loss:1.9533789157867432 \n",
      "iteration:612 loss:1.975904941558838 \n",
      "iteration:613 loss:1.8831239938735962 \n",
      "iteration:614 loss:1.9526890516281128 \n",
      "iteration:615 loss:1.9616888761520386 \n",
      "iteration:616 loss:1.9394124746322632 \n",
      "iteration:617 loss:1.9617139101028442 \n",
      "iteration:618 loss:1.997644305229187 \n",
      "iteration:619 loss:2.0096981525421143 \n",
      "iteration:620 loss:1.9310990571975708 \n",
      "iteration:621 loss:1.9695566892623901 \n",
      "iteration:622 loss:1.9845417737960815 \n",
      "iteration:623 loss:2.009181022644043 \n",
      "iteration:624 loss:1.8673582077026367 \n",
      "iteration:625 loss:1.9706345796585083 \n",
      "iteration:626 loss:1.9736448526382446 \n",
      "iteration:627 loss:1.9601894617080688 \n",
      "iteration:628 loss:2.0138235092163086 \n",
      "iteration:629 loss:1.9551602602005005 \n",
      "iteration:630 loss:1.9597649574279785 \n",
      "iteration:631 loss:1.9079898595809937 \n",
      "iteration:632 loss:1.9244370460510254 \n",
      "iteration:633 loss:1.956037998199463 \n",
      "iteration:634 loss:1.9615110158920288 \n",
      "iteration:635 loss:1.9567701816558838 \n",
      "iteration:636 loss:1.9779735803604126 \n",
      "iteration:637 loss:1.9217034578323364 \n",
      "iteration:638 loss:1.9741343259811401 \n",
      "iteration:639 loss:1.9523332118988037 \n",
      "iteration:640 loss:1.993017315864563 \n",
      "iteration:641 loss:1.9816093444824219 \n",
      "iteration:642 loss:1.9350693225860596 \n",
      "iteration:643 loss:1.9978983402252197 \n",
      "iteration:644 loss:2.013951063156128 \n",
      "iteration:645 loss:1.9561307430267334 \n",
      "iteration:646 loss:2.009099006652832 \n",
      "iteration:647 loss:1.946557879447937 \n",
      "iteration:648 loss:1.937404751777649 \n",
      "iteration:649 loss:2.0130162239074707 \n",
      "iteration:650 loss:1.9669708013534546 \n",
      "iteration:651 loss:1.9210740327835083 \n",
      "iteration:652 loss:1.9009915590286255 \n",
      "iteration:653 loss:2.003295660018921 \n",
      "iteration:654 loss:1.9313303232192993 \n",
      "iteration:655 loss:1.9250205755233765 \n",
      "iteration:656 loss:1.9537808895111084 \n",
      "iteration:657 loss:1.9818965196609497 \n",
      "iteration:658 loss:1.9896478652954102 \n",
      "iteration:659 loss:2.0056588649749756 \n",
      "iteration:660 loss:1.9833587408065796 \n",
      "iteration:661 loss:2.0379178524017334 \n",
      "iteration:662 loss:1.9504504203796387 \n",
      "iteration:663 loss:1.9427152872085571 \n",
      "iteration:664 loss:2.032174587249756 \n",
      "iteration:665 loss:1.944975733757019 \n",
      "iteration:666 loss:1.9414836168289185 \n",
      "iteration:667 loss:1.9601737260818481 \n",
      "iteration:668 loss:1.9192501306533813 \n",
      "iteration:669 loss:1.9824204444885254 \n",
      "iteration:670 loss:2.000629186630249 \n",
      "iteration:671 loss:1.9829020500183105 \n",
      "iteration:672 loss:1.9745558500289917 \n",
      "iteration:673 loss:2.000149726867676 \n",
      "iteration:674 loss:1.9701731204986572 \n",
      "iteration:675 loss:2.0149126052856445 \n",
      "iteration:676 loss:1.920088291168213 \n",
      "iteration:677 loss:1.968232274055481 \n",
      "iteration:678 loss:1.982215166091919 \n",
      "iteration:679 loss:1.9349617958068848 \n",
      "iteration:680 loss:1.9901928901672363 \n",
      "iteration:681 loss:1.9556156396865845 \n",
      "iteration:682 loss:1.9472485780715942 \n",
      "iteration:683 loss:1.9609657526016235 \n",
      "iteration:684 loss:1.973366141319275 \n",
      "iteration:685 loss:1.9468159675598145 \n",
      "iteration:686 loss:1.9514166116714478 \n",
      "iteration:687 loss:1.943406343460083 \n",
      "iteration:688 loss:1.934057593345642 \n",
      "iteration:689 loss:1.9492311477661133 \n",
      "iteration:690 loss:1.9497694969177246 \n",
      "iteration:691 loss:1.991247296333313 \n",
      "iteration:692 loss:1.9191054105758667 \n",
      "iteration:693 loss:1.9431192874908447 \n",
      "iteration:694 loss:1.9141709804534912 \n",
      "iteration:695 loss:1.981217861175537 \n",
      "iteration:696 loss:2.0054664611816406 \n",
      "iteration:697 loss:1.9515999555587769 \n",
      "iteration:698 loss:1.9340647459030151 \n",
      "iteration:699 loss:2.0009727478027344 \n",
      "iteration:700 loss:1.9525474309921265 \n",
      "iteration:701 loss:1.9547430276870728 \n",
      "iteration:702 loss:1.9963505268096924 \n",
      "iteration:703 loss:1.9519122838974 \n",
      "iteration:704 loss:1.9546056985855103 \n",
      "iteration:705 loss:1.9368562698364258 \n",
      "iteration:706 loss:1.9440014362335205 \n",
      "iteration:707 loss:1.9553000926971436 \n",
      "iteration:708 loss:1.9378949403762817 \n",
      "iteration:709 loss:1.9776781797409058 \n",
      "iteration:710 loss:1.9304213523864746 \n",
      "iteration:711 loss:2.001342535018921 \n",
      "iteration:712 loss:1.9457639455795288 \n",
      "iteration:713 loss:1.9753354787826538 \n",
      "iteration:714 loss:1.9028239250183105 \n",
      "iteration:715 loss:1.9878133535385132 \n",
      "iteration:716 loss:2.0088911056518555 \n",
      "iteration:717 loss:1.9175305366516113 \n",
      "iteration:718 loss:1.9877028465270996 \n",
      "iteration:719 loss:1.946655035018921 \n",
      "iteration:720 loss:1.9418058395385742 \n",
      "iteration:721 loss:1.9541707038879395 \n",
      "iteration:722 loss:1.9758021831512451 \n",
      "iteration:723 loss:1.9892008304595947 \n",
      "iteration:724 loss:2.0396499633789062 \n",
      "Epoch-5 lr: 0.0009755282581475769\n",
      "iteration:725 loss:1.9244986772537231 \n",
      "iteration:726 loss:1.9977655410766602 \n",
      "iteration:727 loss:1.9878556728363037 \n",
      "iteration:728 loss:1.959164023399353 \n",
      "iteration:729 loss:1.9759095907211304 \n",
      "iteration:730 loss:1.9337975978851318 \n",
      "iteration:731 loss:1.9690568447113037 \n",
      "iteration:732 loss:1.9621939659118652 \n",
      "iteration:733 loss:1.9341522455215454 \n",
      "iteration:734 loss:1.9822808504104614 \n",
      "iteration:735 loss:1.923113226890564 \n",
      "iteration:736 loss:2.034693717956543 \n",
      "iteration:737 loss:1.9815419912338257 \n",
      "iteration:738 loss:2.029573440551758 \n",
      "iteration:739 loss:1.9174946546554565 \n",
      "iteration:740 loss:1.9586234092712402 \n",
      "iteration:741 loss:1.9865248203277588 \n",
      "iteration:742 loss:1.927891492843628 \n",
      "iteration:743 loss:1.9422359466552734 \n",
      "iteration:744 loss:1.9352046251296997 \n",
      "iteration:745 loss:1.9673298597335815 \n",
      "iteration:746 loss:1.9285448789596558 \n",
      "iteration:747 loss:1.924180269241333 \n",
      "iteration:748 loss:1.9394805431365967 \n",
      "iteration:749 loss:1.9438222646713257 \n",
      "iteration:750 loss:1.941063642501831 \n",
      "iteration:751 loss:1.9524253606796265 \n",
      "iteration:752 loss:1.972999095916748 \n",
      "iteration:753 loss:1.9435657262802124 \n",
      "iteration:754 loss:1.9325932264328003 \n",
      "iteration:755 loss:1.906772255897522 \n",
      "iteration:756 loss:1.9491479396820068 \n",
      "iteration:757 loss:1.941137433052063 \n",
      "iteration:758 loss:1.9370405673980713 \n",
      "iteration:759 loss:1.9500073194503784 \n",
      "iteration:760 loss:1.9566351175308228 \n",
      "iteration:761 loss:1.9701627492904663 \n",
      "iteration:762 loss:1.9553987979888916 \n",
      "iteration:763 loss:1.892353892326355 \n",
      "iteration:764 loss:1.9049886465072632 \n",
      "iteration:765 loss:1.9879579544067383 \n",
      "iteration:766 loss:1.9543088674545288 \n",
      "iteration:767 loss:1.9396618604660034 \n",
      "iteration:768 loss:1.9350838661193848 \n",
      "iteration:769 loss:1.9421364068984985 \n",
      "iteration:770 loss:1.9650431871414185 \n",
      "iteration:771 loss:1.9391635656356812 \n",
      "iteration:772 loss:1.9145485162734985 \n",
      "iteration:773 loss:1.9170814752578735 \n",
      "iteration:774 loss:1.9771006107330322 \n",
      "iteration:775 loss:1.9225976467132568 \n",
      "iteration:776 loss:1.9275480508804321 \n",
      "iteration:777 loss:1.9237052202224731 \n",
      "iteration:778 loss:1.9489268064498901 \n",
      "iteration:779 loss:1.905595302581787 \n",
      "iteration:780 loss:1.9442673921585083 \n",
      "iteration:781 loss:1.9609564542770386 \n",
      "iteration:782 loss:1.9413903951644897 \n",
      "iteration:783 loss:1.952809453010559 \n",
      "iteration:784 loss:1.923862338066101 \n",
      "iteration:785 loss:1.919911503791809 \n",
      "iteration:786 loss:1.9733734130859375 \n",
      "iteration:787 loss:1.9379608631134033 \n",
      "iteration:788 loss:1.9407631158828735 \n",
      "iteration:789 loss:1.9519563913345337 \n",
      "iteration:790 loss:1.9509297609329224 \n",
      "iteration:791 loss:1.938405990600586 \n",
      "iteration:792 loss:1.9457086324691772 \n",
      "iteration:793 loss:1.9430214166641235 \n",
      "iteration:794 loss:1.9955216646194458 \n",
      "iteration:795 loss:1.903619408607483 \n",
      "iteration:796 loss:1.9906710386276245 \n",
      "iteration:797 loss:1.9750699996948242 \n",
      "iteration:798 loss:1.9429882764816284 \n",
      "iteration:799 loss:1.9040069580078125 \n",
      "iteration:800 loss:1.9908366203308105 \n",
      "iteration:801 loss:1.9265620708465576 \n",
      "iteration:802 loss:1.943779468536377 \n",
      "iteration:803 loss:1.981449007987976 \n",
      "iteration:804 loss:1.9660601615905762 \n",
      "iteration:805 loss:1.9774596691131592 \n",
      "iteration:806 loss:1.9149060249328613 \n",
      "iteration:807 loss:1.9378224611282349 \n",
      "iteration:808 loss:1.951509952545166 \n",
      "iteration:809 loss:1.9627630710601807 \n",
      "iteration:810 loss:1.943321943283081 \n",
      "iteration:811 loss:1.9464225769042969 \n",
      "iteration:812 loss:1.9323726892471313 \n",
      "iteration:813 loss:1.9281328916549683 \n",
      "iteration:814 loss:1.9733552932739258 \n",
      "iteration:815 loss:2.010361671447754 \n",
      "iteration:816 loss:1.9336073398590088 \n",
      "iteration:817 loss:1.9652528762817383 \n",
      "iteration:818 loss:1.9465110301971436 \n",
      "iteration:819 loss:1.996431827545166 \n",
      "iteration:820 loss:1.8892722129821777 \n",
      "iteration:821 loss:1.9750515222549438 \n",
      "iteration:822 loss:1.9418542385101318 \n",
      "iteration:823 loss:1.9755806922912598 \n",
      "iteration:824 loss:1.9240429401397705 \n",
      "iteration:825 loss:1.9775842428207397 \n",
      "iteration:826 loss:1.9351332187652588 \n",
      "iteration:827 loss:1.950028657913208 \n",
      "iteration:828 loss:1.9454437494277954 \n",
      "iteration:829 loss:1.9797637462615967 \n",
      "iteration:830 loss:1.9290114641189575 \n",
      "iteration:831 loss:1.9565987586975098 \n",
      "iteration:832 loss:1.9559919834136963 \n",
      "iteration:833 loss:1.9573715925216675 \n",
      "iteration:834 loss:1.9487148523330688 \n",
      "iteration:835 loss:1.9631577730178833 \n",
      "iteration:836 loss:1.9258215427398682 \n",
      "iteration:837 loss:1.9631989002227783 \n",
      "iteration:838 loss:1.9447847604751587 \n",
      "iteration:839 loss:1.9372509717941284 \n",
      "iteration:840 loss:1.896270513534546 \n",
      "iteration:841 loss:1.948752999305725 \n",
      "iteration:842 loss:1.9607192277908325 \n",
      "iteration:843 loss:1.9504752159118652 \n",
      "iteration:844 loss:1.9982906579971313 \n",
      "iteration:845 loss:1.8813505172729492 \n",
      "iteration:846 loss:1.8846665620803833 \n",
      "iteration:847 loss:1.9847276210784912 \n",
      "iteration:848 loss:1.9210125207901 \n",
      "iteration:849 loss:1.9153952598571777 \n",
      "iteration:850 loss:1.9404412508010864 \n",
      "iteration:851 loss:1.927008867263794 \n",
      "iteration:852 loss:1.9613837003707886 \n",
      "iteration:853 loss:1.9448518753051758 \n",
      "iteration:854 loss:1.9065932035446167 \n",
      "iteration:855 loss:1.9670580625534058 \n",
      "iteration:856 loss:2.007106304168701 \n",
      "iteration:857 loss:1.990908145904541 \n",
      "iteration:858 loss:1.9415124654769897 \n",
      "iteration:859 loss:1.9105373620986938 \n",
      "iteration:860 loss:1.9753021001815796 \n",
      "iteration:861 loss:1.9222699403762817 \n",
      "iteration:862 loss:1.9422820806503296 \n",
      "iteration:863 loss:1.9696372747421265 \n",
      "iteration:864 loss:1.9487837553024292 \n",
      "iteration:865 loss:1.9381048679351807 \n",
      "iteration:866 loss:1.953527808189392 \n",
      "iteration:867 loss:1.9741556644439697 \n",
      "iteration:868 loss:1.9289205074310303 \n",
      "iteration:869 loss:1.9804338216781616 \n",
      "Epoch-6 lr: 0.0009648882429441258\n",
      "iteration:870 loss:1.9305226802825928 \n",
      "iteration:871 loss:1.9276869297027588 \n",
      "iteration:872 loss:1.9423741102218628 \n",
      "iteration:873 loss:1.9048407077789307 \n",
      "iteration:874 loss:1.9259271621704102 \n",
      "iteration:875 loss:1.9135371446609497 \n",
      "iteration:876 loss:2.0198557376861572 \n",
      "iteration:877 loss:1.9940686225891113 \n",
      "iteration:878 loss:2.025801420211792 \n",
      "iteration:879 loss:1.9781566858291626 \n",
      "iteration:880 loss:1.9752663373947144 \n",
      "iteration:881 loss:1.9689886569976807 \n",
      "iteration:882 loss:1.9880132675170898 \n",
      "iteration:883 loss:1.979729413986206 \n",
      "iteration:884 loss:1.9422008991241455 \n",
      "iteration:885 loss:1.9499542713165283 \n",
      "iteration:886 loss:1.9257409572601318 \n",
      "iteration:887 loss:1.9365384578704834 \n",
      "iteration:888 loss:1.9699983596801758 \n",
      "iteration:889 loss:1.9451853036880493 \n",
      "iteration:890 loss:1.9668947458267212 \n",
      "iteration:891 loss:1.915224552154541 \n",
      "iteration:892 loss:1.9519760608673096 \n",
      "iteration:893 loss:1.9625955820083618 \n",
      "iteration:894 loss:1.9943249225616455 \n",
      "iteration:895 loss:1.926817774772644 \n",
      "iteration:896 loss:1.9614726305007935 \n",
      "iteration:897 loss:1.9586586952209473 \n",
      "iteration:898 loss:1.9385310411453247 \n",
      "iteration:899 loss:1.9503165483474731 \n",
      "iteration:900 loss:1.9758415222167969 \n",
      "iteration:901 loss:1.94733726978302 \n",
      "iteration:902 loss:1.9239096641540527 \n",
      "iteration:903 loss:1.8890106678009033 \n",
      "iteration:904 loss:1.961694598197937 \n",
      "iteration:905 loss:1.9183683395385742 \n",
      "iteration:906 loss:2.0102412700653076 \n",
      "iteration:907 loss:1.937748670578003 \n",
      "iteration:908 loss:1.9568623304367065 \n",
      "iteration:909 loss:1.9659801721572876 \n",
      "iteration:910 loss:1.9093284606933594 \n",
      "iteration:911 loss:1.9243924617767334 \n",
      "iteration:912 loss:1.9772324562072754 \n",
      "iteration:913 loss:1.9708012342453003 \n",
      "iteration:914 loss:1.9377262592315674 \n",
      "iteration:915 loss:1.9563037157058716 \n",
      "iteration:916 loss:1.9712507724761963 \n",
      "iteration:917 loss:1.9518896341323853 \n",
      "iteration:918 loss:1.963639736175537 \n",
      "iteration:919 loss:1.9287514686584473 \n",
      "iteration:920 loss:1.843781590461731 \n",
      "iteration:921 loss:1.9085803031921387 \n",
      "iteration:922 loss:1.8773366212844849 \n",
      "iteration:923 loss:1.953342318534851 \n",
      "iteration:924 loss:1.94451904296875 \n",
      "iteration:925 loss:1.943088412284851 \n",
      "iteration:926 loss:1.9561588764190674 \n",
      "iteration:927 loss:1.923583745956421 \n",
      "iteration:928 loss:1.9689385890960693 \n",
      "iteration:929 loss:1.9460713863372803 \n",
      "iteration:930 loss:2.005460262298584 \n",
      "iteration:931 loss:1.918531060218811 \n",
      "iteration:932 loss:1.9463051557540894 \n",
      "iteration:933 loss:1.9021966457366943 \n",
      "iteration:934 loss:2.027769088745117 \n",
      "iteration:935 loss:1.9290512800216675 \n",
      "iteration:936 loss:1.923350214958191 \n",
      "iteration:937 loss:1.9152898788452148 \n",
      "iteration:938 loss:1.9254677295684814 \n",
      "iteration:939 loss:1.89812171459198 \n",
      "iteration:940 loss:1.9383357763290405 \n",
      "iteration:941 loss:1.9433737993240356 \n",
      "iteration:942 loss:1.951365351676941 \n",
      "iteration:943 loss:1.8799928426742554 \n",
      "iteration:944 loss:1.9352192878723145 \n",
      "iteration:945 loss:1.9529439210891724 \n",
      "iteration:946 loss:1.9580594301223755 \n",
      "iteration:947 loss:1.9851549863815308 \n",
      "iteration:948 loss:1.9380815029144287 \n",
      "iteration:949 loss:1.9917820692062378 \n",
      "iteration:950 loss:1.9882628917694092 \n",
      "iteration:951 loss:1.9446057081222534 \n",
      "iteration:952 loss:1.9228092432022095 \n",
      "iteration:953 loss:1.9373010396957397 \n",
      "iteration:954 loss:1.995334267616272 \n",
      "iteration:955 loss:1.9249359369277954 \n",
      "iteration:956 loss:1.9194865226745605 \n",
      "iteration:957 loss:1.9481126070022583 \n",
      "iteration:958 loss:1.9675146341323853 \n",
      "iteration:959 loss:1.9727126359939575 \n",
      "iteration:960 loss:1.952338695526123 \n",
      "iteration:961 loss:2.001950979232788 \n",
      "iteration:962 loss:1.901170015335083 \n",
      "iteration:963 loss:1.9351688623428345 \n",
      "iteration:964 loss:1.9219856262207031 \n",
      "iteration:965 loss:1.9402750730514526 \n",
      "iteration:966 loss:1.960768461227417 \n",
      "iteration:967 loss:1.9267065525054932 \n",
      "iteration:968 loss:1.9455808401107788 \n",
      "iteration:969 loss:1.9451802968978882 \n",
      "iteration:970 loss:1.9530009031295776 \n",
      "iteration:971 loss:1.9343538284301758 \n",
      "iteration:972 loss:1.9513843059539795 \n",
      "iteration:973 loss:1.949683427810669 \n",
      "iteration:974 loss:1.9475177526474 \n",
      "iteration:975 loss:1.972373604774475 \n",
      "iteration:976 loss:1.9446970224380493 \n",
      "iteration:977 loss:1.9255952835083008 \n",
      "iteration:978 loss:1.984980821609497 \n",
      "iteration:979 loss:1.9652581214904785 \n",
      "iteration:980 loss:1.9422314167022705 \n",
      "iteration:981 loss:1.9055118560791016 \n",
      "iteration:982 loss:1.9423528909683228 \n",
      "iteration:983 loss:1.9618754386901855 \n",
      "iteration:984 loss:1.944798231124878 \n",
      "iteration:985 loss:1.92939031124115 \n",
      "iteration:986 loss:1.969628095626831 \n",
      "iteration:987 loss:1.9664074182510376 \n",
      "iteration:988 loss:1.8985811471939087 \n",
      "iteration:989 loss:1.956871509552002 \n",
      "iteration:990 loss:1.9403678178787231 \n",
      "iteration:991 loss:1.8994911909103394 \n",
      "iteration:992 loss:1.9122650623321533 \n",
      "iteration:993 loss:1.951109766960144 \n",
      "iteration:994 loss:1.8960965871810913 \n",
      "iteration:995 loss:1.915863275527954 \n",
      "iteration:996 loss:1.9187601804733276 \n",
      "iteration:997 loss:1.979206919670105 \n",
      "iteration:998 loss:1.9687520265579224 \n",
      "iteration:999 loss:1.961445689201355 \n",
      "iteration:1000 loss:1.9238826036453247 \n",
      "iteration:1001 loss:1.9122141599655151 \n",
      "iteration:1002 loss:1.980891466140747 \n",
      "iteration:1003 loss:1.9821932315826416 \n",
      "iteration:1004 loss:1.9733569622039795 \n",
      "iteration:1005 loss:1.9451769590377808 \n",
      "iteration:1006 loss:1.9429845809936523 \n",
      "iteration:1007 loss:1.8958815336227417 \n",
      "iteration:1008 loss:1.9193487167358398 \n",
      "iteration:1009 loss:1.9410115480422974 \n",
      "iteration:1010 loss:1.9125409126281738 \n",
      "iteration:1011 loss:2.0113089084625244 \n",
      "iteration:1012 loss:1.9114327430725098 \n",
      "iteration:1013 loss:1.9404011964797974 \n",
      "iteration:1014 loss:1.9763725996017456 \n",
      "Epoch-7 lr: 0.0009524135262330099\n",
      "iteration:1015 loss:1.9238356351852417 \n",
      "iteration:1016 loss:1.9098427295684814 \n",
      "iteration:1017 loss:1.9533650875091553 \n",
      "iteration:1018 loss:1.945516586303711 \n",
      "iteration:1019 loss:1.9186307191848755 \n",
      "iteration:1020 loss:1.9192101955413818 \n",
      "iteration:1021 loss:1.9598323106765747 \n",
      "iteration:1022 loss:1.9361134767532349 \n",
      "iteration:1023 loss:1.941368818283081 \n",
      "iteration:1024 loss:1.9133706092834473 \n",
      "iteration:1025 loss:1.9595749378204346 \n",
      "iteration:1026 loss:1.9260607957839966 \n",
      "iteration:1027 loss:1.9722633361816406 \n",
      "iteration:1028 loss:1.9251476526260376 \n",
      "iteration:1029 loss:1.9007006883621216 \n",
      "iteration:1030 loss:1.9370520114898682 \n",
      "iteration:1031 loss:1.9287863969802856 \n",
      "iteration:1032 loss:1.963544487953186 \n",
      "iteration:1033 loss:1.9235384464263916 \n",
      "iteration:1034 loss:1.9369195699691772 \n",
      "iteration:1035 loss:1.877427339553833 \n",
      "iteration:1036 loss:1.9098026752471924 \n",
      "iteration:1037 loss:1.914452314376831 \n",
      "iteration:1038 loss:1.9200494289398193 \n",
      "iteration:1039 loss:1.958045482635498 \n",
      "iteration:1040 loss:1.957777738571167 \n",
      "iteration:1041 loss:1.9114573001861572 \n",
      "iteration:1042 loss:1.9128928184509277 \n",
      "iteration:1043 loss:1.888291597366333 \n",
      "iteration:1044 loss:1.9233622550964355 \n",
      "iteration:1045 loss:1.9476100206375122 \n",
      "iteration:1046 loss:1.9042965173721313 \n",
      "iteration:1047 loss:1.9508719444274902 \n",
      "iteration:1048 loss:1.9357248544692993 \n",
      "iteration:1049 loss:1.9569610357284546 \n",
      "iteration:1050 loss:1.9661442041397095 \n",
      "iteration:1051 loss:1.9703550338745117 \n",
      "iteration:1052 loss:1.9423243999481201 \n",
      "iteration:1053 loss:1.9535324573516846 \n",
      "iteration:1054 loss:1.9088106155395508 \n",
      "iteration:1055 loss:1.9069453477859497 \n",
      "iteration:1056 loss:1.9385309219360352 \n",
      "iteration:1057 loss:1.9355145692825317 \n",
      "iteration:1058 loss:1.965250015258789 \n",
      "iteration:1059 loss:1.925953984260559 \n",
      "iteration:1060 loss:1.9576447010040283 \n",
      "iteration:1061 loss:1.894500732421875 \n",
      "iteration:1062 loss:1.8895186185836792 \n",
      "iteration:1063 loss:1.9390995502471924 \n",
      "iteration:1064 loss:1.9349632263183594 \n",
      "iteration:1065 loss:1.9228636026382446 \n",
      "iteration:1066 loss:1.887831449508667 \n",
      "iteration:1067 loss:1.9279872179031372 \n",
      "iteration:1068 loss:1.9072351455688477 \n",
      "iteration:1069 loss:1.959639072418213 \n",
      "iteration:1070 loss:1.9091192483901978 \n",
      "iteration:1071 loss:1.9501971006393433 \n",
      "iteration:1072 loss:1.948791265487671 \n",
      "iteration:1073 loss:1.9284415245056152 \n",
      "iteration:1074 loss:1.9256113767623901 \n",
      "iteration:1075 loss:1.9646813869476318 \n",
      "iteration:1076 loss:1.974238634109497 \n",
      "iteration:1077 loss:1.9217143058776855 \n",
      "iteration:1078 loss:1.9466884136199951 \n",
      "iteration:1079 loss:1.949311375617981 \n",
      "iteration:1080 loss:1.9146380424499512 \n",
      "iteration:1081 loss:1.9279567003250122 \n",
      "iteration:1082 loss:1.9441066980361938 \n",
      "iteration:1083 loss:1.9635831117630005 \n",
      "iteration:1084 loss:1.9009445905685425 \n",
      "iteration:1085 loss:1.9392426013946533 \n",
      "iteration:1086 loss:1.9421571493148804 \n",
      "iteration:1087 loss:1.9201393127441406 \n",
      "iteration:1088 loss:1.9610973596572876 \n",
      "iteration:1089 loss:1.944105625152588 \n",
      "iteration:1090 loss:1.9070265293121338 \n",
      "iteration:1091 loss:1.9180865287780762 \n",
      "iteration:1092 loss:1.9444749355316162 \n",
      "iteration:1093 loss:1.9348446130752563 \n",
      "iteration:1094 loss:1.9719187021255493 \n",
      "iteration:1095 loss:1.9198473691940308 \n",
      "iteration:1096 loss:1.8807021379470825 \n",
      "iteration:1097 loss:1.8903708457946777 \n",
      "iteration:1098 loss:1.907317876815796 \n",
      "iteration:1099 loss:1.9096297025680542 \n",
      "iteration:1100 loss:1.9441094398498535 \n",
      "iteration:1101 loss:1.9148640632629395 \n",
      "iteration:1102 loss:1.9014105796813965 \n",
      "iteration:1103 loss:1.9017962217330933 \n",
      "iteration:1104 loss:1.953596830368042 \n",
      "iteration:1105 loss:1.951833724975586 \n",
      "iteration:1106 loss:1.9602150917053223 \n",
      "iteration:1107 loss:1.9259899854660034 \n",
      "iteration:1108 loss:1.9554725885391235 \n",
      "iteration:1109 loss:1.9363642930984497 \n",
      "iteration:1110 loss:1.8808811902999878 \n",
      "iteration:1111 loss:1.9080116748809814 \n",
      "iteration:1112 loss:1.889387845993042 \n",
      "iteration:1113 loss:1.9020658731460571 \n",
      "iteration:1114 loss:1.9389622211456299 \n",
      "iteration:1115 loss:1.9133626222610474 \n",
      "iteration:1116 loss:1.9022616147994995 \n",
      "iteration:1117 loss:1.9389561414718628 \n",
      "iteration:1118 loss:1.9514799118041992 \n",
      "iteration:1119 loss:1.911590576171875 \n",
      "iteration:1120 loss:1.9441560506820679 \n",
      "iteration:1121 loss:1.927303671836853 \n",
      "iteration:1122 loss:1.9577529430389404 \n",
      "iteration:1123 loss:1.9536800384521484 \n",
      "iteration:1124 loss:1.9457371234893799 \n",
      "iteration:1125 loss:1.9125348329544067 \n",
      "iteration:1126 loss:1.916117548942566 \n",
      "iteration:1127 loss:1.9154084920883179 \n",
      "iteration:1128 loss:1.9387414455413818 \n",
      "iteration:1129 loss:1.9809889793395996 \n",
      "iteration:1130 loss:1.9056968688964844 \n",
      "iteration:1131 loss:1.912275791168213 \n",
      "iteration:1132 loss:1.947411060333252 \n",
      "iteration:1133 loss:1.9736241102218628 \n",
      "iteration:1134 loss:1.8941200971603394 \n",
      "iteration:1135 loss:1.9001517295837402 \n",
      "iteration:1136 loss:1.9098728895187378 \n",
      "iteration:1137 loss:1.9763457775115967 \n",
      "iteration:1138 loss:1.9024224281311035 \n",
      "iteration:1139 loss:1.9454529285430908 \n",
      "iteration:1140 loss:1.916029691696167 \n",
      "iteration:1141 loss:1.9340226650238037 \n",
      "iteration:1142 loss:1.920773983001709 \n",
      "iteration:1143 loss:1.9131726026535034 \n",
      "iteration:1144 loss:1.9406960010528564 \n",
      "iteration:1145 loss:1.8973264694213867 \n",
      "iteration:1146 loss:1.894507646560669 \n",
      "iteration:1147 loss:1.933780550956726 \n",
      "iteration:1148 loss:1.944223403930664 \n",
      "iteration:1149 loss:1.9047021865844727 \n",
      "iteration:1150 loss:1.9027589559555054 \n",
      "iteration:1151 loss:1.909216284751892 \n",
      "iteration:1152 loss:1.9056344032287598 \n",
      "iteration:1153 loss:1.9197005033493042 \n",
      "iteration:1154 loss:1.9198452234268188 \n",
      "iteration:1155 loss:1.9218603372573853 \n",
      "iteration:1156 loss:1.9026167392730713 \n",
      "iteration:1157 loss:1.910528302192688 \n",
      "iteration:1158 loss:1.9342199563980103 \n",
      "iteration:1159 loss:1.9254575967788696 \n",
      "Epoch-8 lr: 0.0009381533400219318\n",
      "iteration:1160 loss:1.934271216392517 \n",
      "iteration:1161 loss:1.8760807514190674 \n",
      "iteration:1162 loss:1.911004900932312 \n",
      "iteration:1163 loss:1.9467597007751465 \n",
      "iteration:1164 loss:1.941776156425476 \n",
      "iteration:1165 loss:1.8941731452941895 \n",
      "iteration:1166 loss:1.9028902053833008 \n",
      "iteration:1167 loss:1.9182379245758057 \n",
      "iteration:1168 loss:1.9246771335601807 \n",
      "iteration:1169 loss:1.8888002634048462 \n",
      "iteration:1170 loss:1.938563585281372 \n",
      "iteration:1171 loss:1.8964811563491821 \n",
      "iteration:1172 loss:1.9329756498336792 \n",
      "iteration:1173 loss:1.9231071472167969 \n",
      "iteration:1174 loss:1.9530470371246338 \n",
      "iteration:1175 loss:1.948177695274353 \n",
      "iteration:1176 loss:1.8964300155639648 \n",
      "iteration:1177 loss:1.9433597326278687 \n",
      "iteration:1178 loss:1.9184513092041016 \n",
      "iteration:1179 loss:1.9328566789627075 \n",
      "iteration:1180 loss:1.8932472467422485 \n",
      "iteration:1181 loss:1.915092945098877 \n",
      "iteration:1182 loss:1.9640653133392334 \n",
      "iteration:1183 loss:1.892675757408142 \n",
      "iteration:1184 loss:1.8872711658477783 \n",
      "iteration:1185 loss:1.8860790729522705 \n",
      "iteration:1186 loss:1.9599155187606812 \n",
      "iteration:1187 loss:1.9634183645248413 \n",
      "iteration:1188 loss:1.8630892038345337 \n",
      "iteration:1189 loss:1.9066325426101685 \n",
      "iteration:1190 loss:1.9227006435394287 \n",
      "iteration:1191 loss:1.9088256359100342 \n",
      "iteration:1192 loss:1.871800184249878 \n",
      "iteration:1193 loss:1.894216537475586 \n",
      "iteration:1194 loss:1.924763560295105 \n",
      "iteration:1195 loss:1.8895643949508667 \n",
      "iteration:1196 loss:1.8919966220855713 \n",
      "iteration:1197 loss:1.894446611404419 \n",
      "iteration:1198 loss:1.869828701019287 \n",
      "iteration:1199 loss:1.9243900775909424 \n",
      "iteration:1200 loss:1.9093466997146606 \n",
      "iteration:1201 loss:1.904911756515503 \n",
      "iteration:1202 loss:1.9518179893493652 \n",
      "iteration:1203 loss:1.8807694911956787 \n",
      "iteration:1204 loss:1.910915732383728 \n",
      "iteration:1205 loss:1.9353784322738647 \n",
      "iteration:1206 loss:1.881229281425476 \n",
      "iteration:1207 loss:1.9219176769256592 \n",
      "iteration:1208 loss:1.8707462549209595 \n",
      "iteration:1209 loss:1.8939805030822754 \n",
      "iteration:1210 loss:1.895798921585083 \n",
      "iteration:1211 loss:1.9667174816131592 \n",
      "iteration:1212 loss:1.9117258787155151 \n",
      "iteration:1213 loss:1.8667703866958618 \n",
      "iteration:1214 loss:1.9023396968841553 \n",
      "iteration:1215 loss:1.9073739051818848 \n",
      "iteration:1216 loss:1.9101605415344238 \n",
      "iteration:1217 loss:1.896233081817627 \n",
      "iteration:1218 loss:1.924109697341919 \n",
      "iteration:1219 loss:1.9357224702835083 \n",
      "iteration:1220 loss:1.8582645654678345 \n",
      "iteration:1221 loss:1.8857660293579102 \n",
      "iteration:1222 loss:1.9144138097763062 \n",
      "iteration:1223 loss:1.8849153518676758 \n",
      "iteration:1224 loss:1.938978910446167 \n",
      "iteration:1225 loss:1.9352298974990845 \n",
      "iteration:1226 loss:1.9596540927886963 \n",
      "iteration:1227 loss:1.8585922718048096 \n",
      "iteration:1228 loss:1.9620819091796875 \n",
      "iteration:1229 loss:1.9365822076797485 \n",
      "iteration:1230 loss:1.8670082092285156 \n",
      "iteration:1231 loss:1.9355381727218628 \n",
      "iteration:1232 loss:1.9462294578552246 \n",
      "iteration:1233 loss:1.9006414413452148 \n",
      "iteration:1234 loss:1.8377602100372314 \n",
      "iteration:1235 loss:1.9176520109176636 \n",
      "iteration:1236 loss:1.9040718078613281 \n",
      "iteration:1237 loss:1.9334158897399902 \n",
      "iteration:1238 loss:1.9479451179504395 \n",
      "iteration:1239 loss:1.9147135019302368 \n",
      "iteration:1240 loss:1.8920222520828247 \n",
      "iteration:1241 loss:1.9049739837646484 \n",
      "iteration:1242 loss:1.9396144151687622 \n",
      "iteration:1243 loss:1.9219443798065186 \n",
      "iteration:1244 loss:1.9054386615753174 \n",
      "iteration:1245 loss:1.9144494533538818 \n",
      "iteration:1246 loss:1.8842583894729614 \n",
      "iteration:1247 loss:1.90459144115448 \n",
      "iteration:1248 loss:1.9367972612380981 \n",
      "iteration:1249 loss:1.8593599796295166 \n",
      "iteration:1250 loss:1.9002127647399902 \n",
      "iteration:1251 loss:1.9006552696228027 \n",
      "iteration:1252 loss:1.9460457563400269 \n",
      "iteration:1253 loss:1.956461787223816 \n",
      "iteration:1254 loss:1.85728919506073 \n",
      "iteration:1255 loss:1.9286712408065796 \n",
      "iteration:1256 loss:1.9035664796829224 \n",
      "iteration:1257 loss:1.9279788732528687 \n",
      "iteration:1258 loss:1.90935218334198 \n",
      "iteration:1259 loss:1.9026192426681519 \n",
      "iteration:1260 loss:1.9436352252960205 \n",
      "iteration:1261 loss:1.9024831056594849 \n",
      "iteration:1262 loss:1.8661832809448242 \n",
      "iteration:1263 loss:1.910367488861084 \n",
      "iteration:1264 loss:1.8779224157333374 \n",
      "iteration:1265 loss:1.9621973037719727 \n",
      "iteration:1266 loss:1.9249792098999023 \n",
      "iteration:1267 loss:1.9030303955078125 \n",
      "iteration:1268 loss:1.9284276962280273 \n",
      "iteration:1269 loss:1.9111855030059814 \n",
      "iteration:1270 loss:1.9406925439834595 \n",
      "iteration:1271 loss:1.9206205606460571 \n",
      "iteration:1272 loss:1.936824917793274 \n",
      "iteration:1273 loss:1.8715057373046875 \n",
      "iteration:1274 loss:1.9181092977523804 \n",
      "iteration:1275 loss:1.9273699522018433 \n",
      "iteration:1276 loss:1.923269271850586 \n",
      "iteration:1277 loss:1.9124870300292969 \n",
      "iteration:1278 loss:1.8769307136535645 \n",
      "iteration:1279 loss:1.8877955675125122 \n",
      "iteration:1280 loss:1.8754868507385254 \n",
      "iteration:1281 loss:1.9407789707183838 \n",
      "iteration:1282 loss:1.9213285446166992 \n",
      "iteration:1283 loss:1.9092286825180054 \n",
      "iteration:1284 loss:1.9125950336456299 \n",
      "iteration:1285 loss:1.9233911037445068 \n",
      "iteration:1286 loss:1.876193642616272 \n",
      "iteration:1287 loss:1.8996003866195679 \n",
      "iteration:1288 loss:1.9515159130096436 \n",
      "iteration:1289 loss:1.888606309890747 \n",
      "iteration:1290 loss:1.8868463039398193 \n",
      "iteration:1291 loss:1.9066798686981201 \n",
      "iteration:1292 loss:1.8872652053833008 \n",
      "iteration:1293 loss:1.886121392250061 \n",
      "iteration:1294 loss:1.9021966457366943 \n",
      "iteration:1295 loss:1.900469183921814 \n",
      "iteration:1296 loss:1.8956388235092163 \n",
      "iteration:1297 loss:1.9030922651290894 \n",
      "iteration:1298 loss:1.9176119565963745 \n",
      "iteration:1299 loss:1.9236167669296265 \n",
      "iteration:1300 loss:1.9248766899108887 \n",
      "iteration:1301 loss:1.8863472938537598 \n",
      "iteration:1302 loss:1.884470820426941 \n",
      "iteration:1303 loss:1.8939793109893799 \n",
      "iteration:1304 loss:2.03509783744812 \n",
      "Epoch-9 lr: 0.0009221639627510075\n",
      "iteration:1305 loss:1.901031732559204 \n",
      "iteration:1306 loss:1.8977856636047363 \n",
      "iteration:1307 loss:1.9013723134994507 \n",
      "iteration:1308 loss:1.8781579732894897 \n",
      "iteration:1309 loss:1.9356073141098022 \n",
      "iteration:1310 loss:1.8728266954421997 \n",
      "iteration:1311 loss:1.893721342086792 \n",
      "iteration:1312 loss:1.9425435066223145 \n",
      "iteration:1313 loss:1.9218796491622925 \n",
      "iteration:1314 loss:1.8549525737762451 \n",
      "iteration:1315 loss:1.8897578716278076 \n",
      "iteration:1316 loss:1.9137040376663208 \n",
      "iteration:1317 loss:1.8775553703308105 \n",
      "iteration:1318 loss:1.8800857067108154 \n",
      "iteration:1319 loss:1.9212428331375122 \n",
      "iteration:1320 loss:1.8705593347549438 \n",
      "iteration:1321 loss:1.8231736421585083 \n",
      "iteration:1322 loss:1.922813892364502 \n",
      "iteration:1323 loss:1.8677802085876465 \n",
      "iteration:1324 loss:1.9182063341140747 \n",
      "iteration:1325 loss:1.9234991073608398 \n",
      "iteration:1326 loss:1.8862888813018799 \n",
      "iteration:1327 loss:1.9185621738433838 \n",
      "iteration:1328 loss:1.8855820894241333 \n",
      "iteration:1329 loss:1.927072525024414 \n",
      "iteration:1330 loss:1.9076696634292603 \n",
      "iteration:1331 loss:1.8805121183395386 \n",
      "iteration:1332 loss:1.9389652013778687 \n",
      "iteration:1333 loss:1.8833359479904175 \n",
      "iteration:1334 loss:1.9109200239181519 \n",
      "iteration:1335 loss:1.9109073877334595 \n",
      "iteration:1336 loss:1.899105429649353 \n",
      "iteration:1337 loss:1.9056261777877808 \n",
      "iteration:1338 loss:1.8586074113845825 \n",
      "iteration:1339 loss:1.902793526649475 \n",
      "iteration:1340 loss:1.9031944274902344 \n",
      "iteration:1341 loss:1.8691192865371704 \n",
      "iteration:1342 loss:1.962683081626892 \n",
      "iteration:1343 loss:1.8591113090515137 \n",
      "iteration:1344 loss:1.9435821771621704 \n",
      "iteration:1345 loss:1.8857232332229614 \n",
      "iteration:1346 loss:1.9118796586990356 \n",
      "iteration:1347 loss:1.9008066654205322 \n",
      "iteration:1348 loss:1.9128906726837158 \n",
      "iteration:1349 loss:1.868174433708191 \n",
      "iteration:1350 loss:1.9474430084228516 \n",
      "iteration:1351 loss:1.8773895502090454 \n",
      "iteration:1352 loss:1.8926777839660645 \n",
      "iteration:1353 loss:1.860201358795166 \n",
      "iteration:1354 loss:1.9169492721557617 \n",
      "iteration:1355 loss:1.9221081733703613 \n",
      "iteration:1356 loss:1.9041119813919067 \n",
      "iteration:1357 loss:1.9380236864089966 \n",
      "iteration:1358 loss:1.8690413236618042 \n",
      "iteration:1359 loss:1.9007225036621094 \n",
      "iteration:1360 loss:1.9085416793823242 \n",
      "iteration:1361 loss:1.906681776046753 \n",
      "iteration:1362 loss:1.9384379386901855 \n",
      "iteration:1363 loss:1.9421826601028442 \n",
      "iteration:1364 loss:1.8995033502578735 \n",
      "iteration:1365 loss:1.8806793689727783 \n",
      "iteration:1366 loss:1.9192783832550049 \n",
      "iteration:1367 loss:1.9161901473999023 \n",
      "iteration:1368 loss:1.8840006589889526 \n",
      "iteration:1369 loss:1.905832290649414 \n",
      "iteration:1370 loss:1.8431730270385742 \n",
      "iteration:1371 loss:1.9319274425506592 \n",
      "iteration:1372 loss:1.9415936470031738 \n",
      "iteration:1373 loss:1.902117133140564 \n",
      "iteration:1374 loss:1.863762378692627 \n",
      "iteration:1375 loss:1.8723211288452148 \n",
      "iteration:1376 loss:1.9231058359146118 \n",
      "iteration:1377 loss:1.8895349502563477 \n",
      "iteration:1378 loss:1.8613059520721436 \n",
      "iteration:1379 loss:1.943210482597351 \n",
      "iteration:1380 loss:1.8929728269577026 \n",
      "iteration:1381 loss:1.8977537155151367 \n",
      "iteration:1382 loss:1.8687539100646973 \n",
      "iteration:1383 loss:1.8409696817398071 \n",
      "iteration:1384 loss:1.929465889930725 \n",
      "iteration:1385 loss:1.9791353940963745 \n",
      "iteration:1386 loss:1.8844088315963745 \n",
      "iteration:1387 loss:1.8781746625900269 \n",
      "iteration:1388 loss:1.944475531578064 \n",
      "iteration:1389 loss:1.9198724031448364 \n",
      "iteration:1390 loss:1.9256364107131958 \n",
      "iteration:1391 loss:1.9480432271957397 \n",
      "iteration:1392 loss:1.8684585094451904 \n",
      "iteration:1393 loss:1.906351089477539 \n",
      "iteration:1394 loss:1.8828951120376587 \n",
      "iteration:1395 loss:1.896372675895691 \n",
      "iteration:1396 loss:1.8586454391479492 \n",
      "iteration:1397 loss:1.933226466178894 \n",
      "iteration:1398 loss:1.8800921440124512 \n",
      "iteration:1399 loss:1.8380144834518433 \n",
      "iteration:1400 loss:1.883301854133606 \n",
      "iteration:1401 loss:1.9106367826461792 \n",
      "iteration:1402 loss:1.8950337171554565 \n",
      "iteration:1403 loss:1.9196183681488037 \n",
      "iteration:1404 loss:1.8735322952270508 \n",
      "iteration:1405 loss:1.9396740198135376 \n",
      "iteration:1406 loss:1.8594058752059937 \n",
      "iteration:1407 loss:1.9115856885910034 \n",
      "iteration:1408 loss:1.9094551801681519 \n",
      "iteration:1409 loss:1.9065539836883545 \n",
      "iteration:1410 loss:1.8984936475753784 \n",
      "iteration:1411 loss:1.8965840339660645 \n",
      "iteration:1412 loss:1.9173399209976196 \n",
      "iteration:1413 loss:1.9068474769592285 \n",
      "iteration:1414 loss:1.8953520059585571 \n",
      "iteration:1415 loss:1.9231994152069092 \n",
      "iteration:1416 loss:1.9429746866226196 \n",
      "iteration:1417 loss:1.9057141542434692 \n",
      "iteration:1418 loss:1.8781107664108276 \n",
      "iteration:1419 loss:1.8928672075271606 \n",
      "iteration:1420 loss:1.860505223274231 \n",
      "iteration:1421 loss:1.931288242340088 \n",
      "iteration:1422 loss:1.844791054725647 \n",
      "iteration:1423 loss:1.8642126321792603 \n",
      "iteration:1424 loss:1.8353161811828613 \n",
      "iteration:1425 loss:1.9242438077926636 \n",
      "iteration:1426 loss:1.9275925159454346 \n",
      "iteration:1427 loss:1.9077980518341064 \n",
      "iteration:1428 loss:1.939215064048767 \n",
      "iteration:1429 loss:1.949562430381775 \n",
      "iteration:1430 loss:1.8700671195983887 \n",
      "iteration:1431 loss:1.8978495597839355 \n",
      "iteration:1432 loss:1.880777359008789 \n",
      "iteration:1433 loss:1.9068024158477783 \n",
      "iteration:1434 loss:1.870598554611206 \n",
      "iteration:1435 loss:1.9387845993041992 \n",
      "iteration:1436 loss:1.922434687614441 \n",
      "iteration:1437 loss:1.8614245653152466 \n",
      "iteration:1438 loss:1.9147416353225708 \n",
      "iteration:1439 loss:1.8808414936065674 \n",
      "iteration:1440 loss:1.8852535486221313 \n",
      "iteration:1441 loss:1.819915771484375 \n",
      "iteration:1442 loss:1.8930773735046387 \n",
      "iteration:1443 loss:1.8815906047821045 \n",
      "iteration:1444 loss:1.947950005531311 \n",
      "iteration:1445 loss:1.9350407123565674 \n",
      "iteration:1446 loss:1.8985501527786255 \n",
      "iteration:1447 loss:1.9139196872711182 \n",
      "iteration:1448 loss:1.8625850677490234 \n",
      "iteration:1449 loss:1.948789358139038 \n",
      "Epoch-10 lr: 0.0009045084971874736\n",
      "iteration:1450 loss:1.8533198833465576 \n",
      "iteration:1451 loss:1.878778338432312 \n",
      "iteration:1452 loss:1.907861351966858 \n",
      "iteration:1453 loss:1.9614753723144531 \n",
      "iteration:1454 loss:1.9195210933685303 \n",
      "iteration:1455 loss:1.8637937307357788 \n",
      "iteration:1456 loss:1.9509037733078003 \n",
      "iteration:1457 loss:1.9010013341903687 \n",
      "iteration:1458 loss:1.9160614013671875 \n",
      "iteration:1459 loss:1.8913508653640747 \n",
      "iteration:1460 loss:1.8834259510040283 \n",
      "iteration:1461 loss:1.9321321249008179 \n",
      "iteration:1462 loss:1.8762390613555908 \n",
      "iteration:1463 loss:1.9385660886764526 \n",
      "iteration:1464 loss:1.903730869293213 \n",
      "iteration:1465 loss:1.883897066116333 \n",
      "iteration:1466 loss:1.9188966751098633 \n",
      "iteration:1467 loss:1.921208143234253 \n",
      "iteration:1468 loss:1.9261975288391113 \n",
      "iteration:1469 loss:1.9005568027496338 \n",
      "iteration:1470 loss:1.8933719396591187 \n",
      "iteration:1471 loss:1.883415937423706 \n",
      "iteration:1472 loss:1.9033827781677246 \n",
      "iteration:1473 loss:1.91047203540802 \n",
      "iteration:1474 loss:1.9848685264587402 \n",
      "iteration:1475 loss:1.9241306781768799 \n",
      "iteration:1476 loss:1.971028447151184 \n",
      "iteration:1477 loss:1.920440673828125 \n",
      "iteration:1478 loss:1.8964569568634033 \n",
      "iteration:1479 loss:1.8415216207504272 \n",
      "iteration:1480 loss:1.9401835203170776 \n",
      "iteration:1481 loss:1.9360032081604004 \n",
      "iteration:1482 loss:1.9138908386230469 \n",
      "iteration:1483 loss:1.8689457178115845 \n",
      "iteration:1484 loss:1.8637884855270386 \n",
      "iteration:1485 loss:1.9025028944015503 \n",
      "iteration:1486 loss:1.892479419708252 \n",
      "iteration:1487 loss:1.950171947479248 \n",
      "iteration:1488 loss:1.9377435445785522 \n",
      "iteration:1489 loss:1.9033526182174683 \n",
      "iteration:1490 loss:1.8930203914642334 \n",
      "iteration:1491 loss:1.9251859188079834 \n",
      "iteration:1492 loss:1.9025911092758179 \n",
      "iteration:1493 loss:1.9407336711883545 \n",
      "iteration:1494 loss:1.9132716655731201 \n",
      "iteration:1495 loss:1.8530641794204712 \n",
      "iteration:1496 loss:1.8949668407440186 \n",
      "iteration:1497 loss:1.9400182962417603 \n",
      "iteration:1498 loss:1.892279863357544 \n",
      "iteration:1499 loss:1.8788588047027588 \n",
      "iteration:1500 loss:1.8848899602890015 \n",
      "iteration:1501 loss:1.8821699619293213 \n",
      "iteration:1502 loss:1.898417592048645 \n",
      "iteration:1503 loss:1.904276728630066 \n",
      "iteration:1504 loss:1.8728718757629395 \n",
      "iteration:1505 loss:1.905439019203186 \n",
      "iteration:1506 loss:1.8586623668670654 \n",
      "iteration:1507 loss:1.929226040840149 \n",
      "iteration:1508 loss:1.8929299116134644 \n",
      "iteration:1509 loss:1.9035123586654663 \n",
      "iteration:1510 loss:1.965726375579834 \n",
      "iteration:1511 loss:1.8795053958892822 \n",
      "iteration:1512 loss:1.8900203704833984 \n",
      "iteration:1513 loss:1.9142167568206787 \n",
      "iteration:1514 loss:1.8984472751617432 \n",
      "iteration:1515 loss:1.9060689210891724 \n",
      "iteration:1516 loss:1.9155635833740234 \n",
      "iteration:1517 loss:1.8373997211456299 \n",
      "iteration:1518 loss:1.8713312149047852 \n",
      "iteration:1519 loss:1.9150559902191162 \n",
      "iteration:1520 loss:1.8738852739334106 \n",
      "iteration:1521 loss:1.9306217432022095 \n",
      "iteration:1522 loss:1.9162113666534424 \n",
      "iteration:1523 loss:1.9227854013442993 \n",
      "iteration:1524 loss:1.918053388595581 \n",
      "iteration:1525 loss:1.8269014358520508 \n",
      "iteration:1526 loss:1.943790316581726 \n",
      "iteration:1527 loss:1.8521593809127808 \n",
      "iteration:1528 loss:1.8989953994750977 \n",
      "iteration:1529 loss:1.9355275630950928 \n",
      "iteration:1530 loss:1.9005755186080933 \n",
      "iteration:1531 loss:1.948191523551941 \n",
      "iteration:1532 loss:1.9133609533309937 \n",
      "iteration:1533 loss:1.8921284675598145 \n",
      "iteration:1534 loss:1.8661818504333496 \n",
      "iteration:1535 loss:1.928189754486084 \n",
      "iteration:1536 loss:1.929964542388916 \n",
      "iteration:1537 loss:1.9289703369140625 \n",
      "iteration:1538 loss:1.9314817190170288 \n",
      "iteration:1539 loss:1.8854992389678955 \n",
      "iteration:1540 loss:1.89134681224823 \n",
      "iteration:1541 loss:1.9473679065704346 \n",
      "iteration:1542 loss:1.8746922016143799 \n",
      "iteration:1543 loss:1.9471549987792969 \n",
      "iteration:1544 loss:1.8919856548309326 \n",
      "iteration:1545 loss:1.9100704193115234 \n",
      "iteration:1546 loss:1.8580065965652466 \n",
      "iteration:1547 loss:1.8907246589660645 \n",
      "iteration:1548 loss:1.8526544570922852 \n",
      "iteration:1549 loss:1.8939003944396973 \n",
      "iteration:1550 loss:1.8941748142242432 \n",
      "iteration:1551 loss:1.9148818254470825 \n",
      "iteration:1552 loss:1.8846657276153564 \n",
      "iteration:1553 loss:1.9258365631103516 \n",
      "iteration:1554 loss:1.8779785633087158 \n",
      "iteration:1555 loss:1.9393057823181152 \n",
      "iteration:1556 loss:1.9405076503753662 \n",
      "iteration:1557 loss:1.9019585847854614 \n",
      "iteration:1558 loss:1.8870049715042114 \n",
      "iteration:1559 loss:1.9136885404586792 \n",
      "iteration:1560 loss:1.9036808013916016 \n",
      "iteration:1561 loss:1.9001752138137817 \n",
      "iteration:1562 loss:1.8956316709518433 \n",
      "iteration:1563 loss:1.8888877630233765 \n",
      "iteration:1564 loss:1.9274559020996094 \n",
      "iteration:1565 loss:1.9123451709747314 \n",
      "iteration:1566 loss:1.8663206100463867 \n",
      "iteration:1567 loss:1.910210371017456 \n",
      "iteration:1568 loss:1.9064587354660034 \n",
      "iteration:1569 loss:1.910738468170166 \n",
      "iteration:1570 loss:1.8738682270050049 \n",
      "iteration:1571 loss:1.9367517232894897 \n",
      "iteration:1572 loss:1.8525885343551636 \n",
      "iteration:1573 loss:1.8889520168304443 \n",
      "iteration:1574 loss:1.859613060951233 \n",
      "iteration:1575 loss:1.8969916105270386 \n",
      "iteration:1576 loss:1.8627992868423462 \n",
      "iteration:1577 loss:1.8383952379226685 \n",
      "iteration:1578 loss:1.9225587844848633 \n",
      "iteration:1579 loss:1.897314429283142 \n",
      "iteration:1580 loss:1.924659013748169 \n",
      "iteration:1581 loss:1.8729196786880493 \n",
      "iteration:1582 loss:1.9009079933166504 \n",
      "iteration:1583 loss:1.853772759437561 \n",
      "iteration:1584 loss:1.879665493965149 \n",
      "iteration:1585 loss:1.856187343597412 \n",
      "iteration:1586 loss:1.841234564781189 \n",
      "iteration:1587 loss:1.8833168745040894 \n",
      "iteration:1588 loss:1.9020776748657227 \n",
      "iteration:1589 loss:1.865593433380127 \n",
      "iteration:1590 loss:1.9189507961273193 \n",
      "iteration:1591 loss:1.878312349319458 \n",
      "iteration:1592 loss:1.8900858163833618 \n",
      "iteration:1593 loss:1.848564863204956 \n",
      "iteration:1594 loss:1.7688484191894531 \n",
      "Epoch-11 lr: 0.0008852566213878945\n",
      "iteration:1595 loss:1.841834545135498 \n",
      "iteration:1596 loss:1.9059666395187378 \n",
      "iteration:1597 loss:1.8938748836517334 \n",
      "iteration:1598 loss:1.8858919143676758 \n",
      "iteration:1599 loss:1.8748033046722412 \n",
      "iteration:1600 loss:1.9275106191635132 \n",
      "iteration:1601 loss:1.8919488191604614 \n",
      "iteration:1602 loss:1.9358428716659546 \n",
      "iteration:1603 loss:1.9229594469070435 \n",
      "iteration:1604 loss:1.8907922506332397 \n",
      "iteration:1605 loss:1.9021649360656738 \n",
      "iteration:1606 loss:1.8810691833496094 \n",
      "iteration:1607 loss:1.9095510244369507 \n",
      "iteration:1608 loss:1.8885886669158936 \n",
      "iteration:1609 loss:1.9089962244033813 \n",
      "iteration:1610 loss:1.844907283782959 \n",
      "iteration:1611 loss:1.8963873386383057 \n",
      "iteration:1612 loss:1.8478224277496338 \n",
      "iteration:1613 loss:1.8886045217514038 \n",
      "iteration:1614 loss:1.9285621643066406 \n",
      "iteration:1615 loss:1.846645712852478 \n",
      "iteration:1616 loss:1.9052766561508179 \n",
      "iteration:1617 loss:1.8625553846359253 \n",
      "iteration:1618 loss:1.9018287658691406 \n",
      "iteration:1619 loss:1.8914544582366943 \n",
      "iteration:1620 loss:1.9215850830078125 \n",
      "iteration:1621 loss:1.8611942529678345 \n",
      "iteration:1622 loss:1.9189658164978027 \n",
      "iteration:1623 loss:1.8910928964614868 \n",
      "iteration:1624 loss:1.8781251907348633 \n",
      "iteration:1625 loss:1.9104835987091064 \n",
      "iteration:1626 loss:1.8688572645187378 \n",
      "iteration:1627 loss:1.8514338731765747 \n",
      "iteration:1628 loss:1.9428431987762451 \n",
      "iteration:1629 loss:1.8578747510910034 \n",
      "iteration:1630 loss:1.9246677160263062 \n",
      "iteration:1631 loss:1.9131184816360474 \n",
      "iteration:1632 loss:1.8730690479278564 \n",
      "iteration:1633 loss:1.8393566608428955 \n",
      "iteration:1634 loss:1.8772119283676147 \n",
      "iteration:1635 loss:1.822200059890747 \n",
      "iteration:1636 loss:1.8208215236663818 \n",
      "iteration:1637 loss:1.8878729343414307 \n",
      "iteration:1638 loss:1.860689640045166 \n",
      "iteration:1639 loss:1.9047833681106567 \n",
      "iteration:1640 loss:1.8798844814300537 \n",
      "iteration:1641 loss:1.8879536390304565 \n",
      "iteration:1642 loss:1.8710994720458984 \n",
      "iteration:1643 loss:1.8573651313781738 \n",
      "iteration:1644 loss:1.8462648391723633 \n",
      "iteration:1645 loss:1.9006038904190063 \n",
      "iteration:1646 loss:1.9052667617797852 \n",
      "iteration:1647 loss:1.918310523033142 \n",
      "iteration:1648 loss:1.8957387208938599 \n",
      "iteration:1649 loss:1.8482470512390137 \n",
      "iteration:1650 loss:1.8558547496795654 \n",
      "iteration:1651 loss:1.8699066638946533 \n",
      "iteration:1652 loss:1.8339377641677856 \n",
      "iteration:1653 loss:1.8998929262161255 \n",
      "iteration:1654 loss:1.9190469980239868 \n",
      "iteration:1655 loss:1.9120336771011353 \n",
      "iteration:1656 loss:1.860205054283142 \n",
      "iteration:1657 loss:1.9081472158432007 \n",
      "iteration:1658 loss:1.8849714994430542 \n",
      "iteration:1659 loss:1.8485054969787598 \n",
      "iteration:1660 loss:1.8969334363937378 \n",
      "iteration:1661 loss:1.824394702911377 \n",
      "iteration:1662 loss:1.8772222995758057 \n",
      "iteration:1663 loss:1.923858642578125 \n",
      "iteration:1664 loss:1.8978991508483887 \n",
      "iteration:1665 loss:1.8862725496292114 \n",
      "iteration:1666 loss:1.9015860557556152 \n",
      "iteration:1667 loss:1.8877921104431152 \n",
      "iteration:1668 loss:1.8787277936935425 \n",
      "iteration:1669 loss:1.901096224784851 \n",
      "iteration:1670 loss:1.9073392152786255 \n",
      "iteration:1671 loss:1.8763151168823242 \n",
      "iteration:1672 loss:1.8773599863052368 \n",
      "iteration:1673 loss:1.8766447305679321 \n",
      "iteration:1674 loss:1.9443902969360352 \n",
      "iteration:1675 loss:1.8618195056915283 \n",
      "iteration:1676 loss:1.8657399415969849 \n",
      "iteration:1677 loss:1.8952155113220215 \n",
      "iteration:1678 loss:1.9397265911102295 \n",
      "iteration:1679 loss:1.9391945600509644 \n",
      "iteration:1680 loss:1.9198685884475708 \n",
      "iteration:1681 loss:1.8562766313552856 \n",
      "iteration:1682 loss:1.8765897750854492 \n",
      "iteration:1683 loss:1.8568048477172852 \n",
      "iteration:1684 loss:1.8782472610473633 \n",
      "iteration:1685 loss:1.9026631116867065 \n",
      "iteration:1686 loss:1.9188848733901978 \n",
      "iteration:1687 loss:1.886015772819519 \n",
      "iteration:1688 loss:1.9087532758712769 \n",
      "iteration:1689 loss:1.841491937637329 \n",
      "iteration:1690 loss:1.8706337213516235 \n",
      "iteration:1691 loss:1.8847495317459106 \n",
      "iteration:1692 loss:1.8297971487045288 \n",
      "iteration:1693 loss:1.9298866987228394 \n",
      "iteration:1694 loss:1.8694734573364258 \n",
      "iteration:1695 loss:1.8892319202423096 \n",
      "iteration:1696 loss:1.9119110107421875 \n",
      "iteration:1697 loss:1.8715975284576416 \n",
      "iteration:1698 loss:1.8619917631149292 \n",
      "iteration:1699 loss:1.9213781356811523 \n",
      "iteration:1700 loss:1.8573222160339355 \n",
      "iteration:1701 loss:1.880005121231079 \n",
      "iteration:1702 loss:1.8438259363174438 \n",
      "iteration:1703 loss:1.875535249710083 \n",
      "iteration:1704 loss:1.928391933441162 \n",
      "iteration:1705 loss:1.9052538871765137 \n",
      "iteration:1706 loss:1.8848447799682617 \n",
      "iteration:1707 loss:1.894636869430542 \n",
      "iteration:1708 loss:1.9054850339889526 \n",
      "iteration:1709 loss:1.868872880935669 \n",
      "iteration:1710 loss:1.8970979452133179 \n",
      "iteration:1711 loss:1.8767973184585571 \n",
      "iteration:1712 loss:1.8956512212753296 \n",
      "iteration:1713 loss:1.909333348274231 \n",
      "iteration:1714 loss:1.8616796731948853 \n",
      "iteration:1715 loss:1.8587396144866943 \n",
      "iteration:1716 loss:1.9173026084899902 \n",
      "iteration:1717 loss:1.8967598676681519 \n",
      "iteration:1718 loss:1.9132739305496216 \n",
      "iteration:1719 loss:1.863399863243103 \n",
      "iteration:1720 loss:1.8531508445739746 \n",
      "iteration:1721 loss:1.892197847366333 \n",
      "iteration:1722 loss:1.8717014789581299 \n",
      "iteration:1723 loss:1.9672120809555054 \n",
      "iteration:1724 loss:1.8828778266906738 \n",
      "iteration:1725 loss:1.9052520990371704 \n",
      "iteration:1726 loss:1.8773458003997803 \n",
      "iteration:1727 loss:1.8719240427017212 \n",
      "iteration:1728 loss:1.9281086921691895 \n",
      "iteration:1729 loss:1.904418706893921 \n",
      "iteration:1730 loss:1.8625880479812622 \n",
      "iteration:1731 loss:1.8876368999481201 \n",
      "iteration:1732 loss:1.8693208694458008 \n",
      "iteration:1733 loss:1.8729366064071655 \n",
      "iteration:1734 loss:1.9081000089645386 \n",
      "iteration:1735 loss:1.953073501586914 \n",
      "iteration:1736 loss:1.8925657272338867 \n",
      "iteration:1737 loss:1.9162516593933105 \n",
      "iteration:1738 loss:1.8931821584701538 \n",
      "iteration:1739 loss:1.9604613780975342 \n",
      "Epoch-12 lr: 0.0008644843137107056\n",
      "iteration:1740 loss:1.834776759147644 \n",
      "iteration:1741 loss:1.8399957418441772 \n",
      "iteration:1742 loss:1.8486140966415405 \n",
      "iteration:1743 loss:1.8686062097549438 \n",
      "iteration:1744 loss:1.870164394378662 \n",
      "iteration:1745 loss:1.8806687593460083 \n",
      "iteration:1746 loss:1.8659698963165283 \n",
      "iteration:1747 loss:1.9006315469741821 \n",
      "iteration:1748 loss:1.9102445840835571 \n",
      "iteration:1749 loss:1.8934682607650757 \n",
      "iteration:1750 loss:1.8731200695037842 \n",
      "iteration:1751 loss:1.8563125133514404 \n",
      "iteration:1752 loss:1.902256727218628 \n",
      "iteration:1753 loss:1.8534159660339355 \n",
      "iteration:1754 loss:1.8680152893066406 \n",
      "iteration:1755 loss:1.8756284713745117 \n",
      "iteration:1756 loss:1.8977872133255005 \n",
      "iteration:1757 loss:1.8879245519638062 \n",
      "iteration:1758 loss:1.855615258216858 \n",
      "iteration:1759 loss:1.8637213706970215 \n",
      "iteration:1760 loss:1.9059932231903076 \n",
      "iteration:1761 loss:1.8955717086791992 \n",
      "iteration:1762 loss:1.8757747411727905 \n",
      "iteration:1763 loss:1.856547474861145 \n",
      "iteration:1764 loss:1.8117207288742065 \n",
      "iteration:1765 loss:1.9196345806121826 \n",
      "iteration:1766 loss:1.8703985214233398 \n",
      "iteration:1767 loss:1.906121015548706 \n",
      "iteration:1768 loss:1.875875473022461 \n",
      "iteration:1769 loss:1.9197255373001099 \n",
      "iteration:1770 loss:1.8522310256958008 \n",
      "iteration:1771 loss:1.8975645303726196 \n",
      "iteration:1772 loss:1.88744056224823 \n",
      "iteration:1773 loss:1.8904943466186523 \n",
      "iteration:1774 loss:1.8856229782104492 \n",
      "iteration:1775 loss:1.8851914405822754 \n",
      "iteration:1776 loss:1.8739218711853027 \n",
      "iteration:1777 loss:1.9040149450302124 \n",
      "iteration:1778 loss:1.8179816007614136 \n",
      "iteration:1779 loss:1.850103735923767 \n",
      "iteration:1780 loss:1.8795689344406128 \n",
      "iteration:1781 loss:1.9014509916305542 \n",
      "iteration:1782 loss:1.9055256843566895 \n",
      "iteration:1783 loss:1.8865879774093628 \n",
      "iteration:1784 loss:1.9179267883300781 \n",
      "iteration:1785 loss:1.9052311182022095 \n",
      "iteration:1786 loss:1.889931082725525 \n",
      "iteration:1787 loss:1.826859951019287 \n",
      "iteration:1788 loss:1.8406221866607666 \n",
      "iteration:1789 loss:1.890289545059204 \n",
      "iteration:1790 loss:1.907741904258728 \n",
      "iteration:1791 loss:1.9085935354232788 \n",
      "iteration:1792 loss:1.8817081451416016 \n",
      "iteration:1793 loss:1.8733344078063965 \n",
      "iteration:1794 loss:1.865621566772461 \n",
      "iteration:1795 loss:1.8749853372573853 \n",
      "iteration:1796 loss:1.882606029510498 \n",
      "iteration:1797 loss:1.8735522031784058 \n",
      "iteration:1798 loss:1.8360083103179932 \n",
      "iteration:1799 loss:1.874539852142334 \n",
      "iteration:1800 loss:1.857110857963562 \n",
      "iteration:1801 loss:1.8485783338546753 \n",
      "iteration:1802 loss:1.8139145374298096 \n",
      "iteration:1803 loss:1.9100641012191772 \n",
      "iteration:1804 loss:1.8647477626800537 \n",
      "iteration:1805 loss:1.927344799041748 \n",
      "iteration:1806 loss:1.918005347251892 \n",
      "iteration:1807 loss:1.8824223279953003 \n",
      "iteration:1808 loss:1.9107329845428467 \n",
      "iteration:1809 loss:1.8759753704071045 \n",
      "iteration:1810 loss:1.8759491443634033 \n",
      "iteration:1811 loss:1.859049916267395 \n",
      "iteration:1812 loss:1.8583641052246094 \n",
      "iteration:1813 loss:1.8397406339645386 \n",
      "iteration:1814 loss:1.915550708770752 \n",
      "iteration:1815 loss:1.8557270765304565 \n",
      "iteration:1816 loss:1.865727186203003 \n",
      "iteration:1817 loss:1.8538751602172852 \n",
      "iteration:1818 loss:1.888997197151184 \n",
      "iteration:1819 loss:1.8479681015014648 \n",
      "iteration:1820 loss:1.8604367971420288 \n",
      "iteration:1821 loss:1.864508867263794 \n",
      "iteration:1822 loss:1.8879425525665283 \n",
      "iteration:1823 loss:1.8098268508911133 \n",
      "iteration:1824 loss:1.8477510213851929 \n",
      "iteration:1825 loss:1.8772079944610596 \n",
      "iteration:1826 loss:1.8680644035339355 \n",
      "iteration:1827 loss:1.9173393249511719 \n",
      "iteration:1828 loss:1.8466891050338745 \n",
      "iteration:1829 loss:1.8925563097000122 \n",
      "iteration:1830 loss:1.891350269317627 \n",
      "iteration:1831 loss:1.9383056163787842 \n",
      "iteration:1832 loss:1.879326581954956 \n",
      "iteration:1833 loss:1.8978259563446045 \n",
      "iteration:1834 loss:1.8821537494659424 \n",
      "iteration:1835 loss:1.8655741214752197 \n",
      "iteration:1836 loss:1.8630057573318481 \n",
      "iteration:1837 loss:1.8856180906295776 \n",
      "iteration:1838 loss:1.9409668445587158 \n",
      "iteration:1839 loss:1.9067356586456299 \n",
      "iteration:1840 loss:1.881531834602356 \n",
      "iteration:1841 loss:1.8997992277145386 \n",
      "iteration:1842 loss:1.8363722562789917 \n",
      "iteration:1843 loss:1.9108319282531738 \n",
      "iteration:1844 loss:1.8915295600891113 \n",
      "iteration:1845 loss:1.9196720123291016 \n",
      "iteration:1846 loss:1.873294711112976 \n",
      "iteration:1847 loss:1.9050726890563965 \n",
      "iteration:1848 loss:1.880021572113037 \n",
      "iteration:1849 loss:1.8794305324554443 \n",
      "iteration:1850 loss:1.8658959865570068 \n",
      "iteration:1851 loss:1.8673369884490967 \n",
      "iteration:1852 loss:1.8757790327072144 \n",
      "iteration:1853 loss:1.9114727973937988 \n",
      "iteration:1854 loss:1.8253965377807617 \n",
      "iteration:1855 loss:1.87440025806427 \n",
      "iteration:1856 loss:1.8971425294876099 \n",
      "iteration:1857 loss:1.9022542238235474 \n",
      "iteration:1858 loss:1.9073989391326904 \n",
      "iteration:1859 loss:1.8830735683441162 \n",
      "iteration:1860 loss:1.8632761240005493 \n",
      "iteration:1861 loss:1.9057039022445679 \n",
      "iteration:1862 loss:1.8319523334503174 \n",
      "iteration:1863 loss:1.82135009765625 \n",
      "iteration:1864 loss:1.8919885158538818 \n",
      "iteration:1865 loss:1.9044281244277954 \n",
      "iteration:1866 loss:1.8653796911239624 \n",
      "iteration:1867 loss:1.8936914205551147 \n",
      "iteration:1868 loss:1.9101619720458984 \n",
      "iteration:1869 loss:1.9164762496948242 \n",
      "iteration:1870 loss:1.9178047180175781 \n",
      "iteration:1871 loss:1.9161964654922485 \n",
      "iteration:1872 loss:1.9146080017089844 \n",
      "iteration:1873 loss:1.945258617401123 \n",
      "iteration:1874 loss:1.925329327583313 \n",
      "iteration:1875 loss:1.9088854789733887 \n",
      "iteration:1876 loss:1.8834271430969238 \n",
      "iteration:1877 loss:1.8980278968811035 \n",
      "iteration:1878 loss:1.9036611318588257 \n",
      "iteration:1879 loss:1.8804073333740234 \n",
      "iteration:1880 loss:1.9259635210037231 \n",
      "iteration:1881 loss:1.9436169862747192 \n",
      "iteration:1882 loss:1.9161802530288696 \n",
      "iteration:1883 loss:1.8963669538497925 \n",
      "iteration:1884 loss:2.055384874343872 \n",
      "Epoch-13 lr: 0.0008422735529643443\n",
      "iteration:1885 loss:1.8878792524337769 \n",
      "iteration:1886 loss:1.8517537117004395 \n",
      "iteration:1887 loss:1.8595483303070068 \n",
      "iteration:1888 loss:1.8943666219711304 \n",
      "iteration:1889 loss:1.8780319690704346 \n",
      "iteration:1890 loss:1.9748843908309937 \n",
      "iteration:1891 loss:1.8531335592269897 \n",
      "iteration:1892 loss:1.8878450393676758 \n",
      "iteration:1893 loss:1.8992499113082886 \n",
      "iteration:1894 loss:1.8973621129989624 \n",
      "iteration:1895 loss:1.8750360012054443 \n",
      "iteration:1896 loss:1.9176747798919678 \n",
      "iteration:1897 loss:1.943761944770813 \n",
      "iteration:1898 loss:1.900048851966858 \n",
      "iteration:1899 loss:1.9026259183883667 \n",
      "iteration:1900 loss:1.8679821491241455 \n",
      "iteration:1901 loss:1.908128261566162 \n",
      "iteration:1902 loss:1.8804837465286255 \n",
      "iteration:1903 loss:1.8771553039550781 \n",
      "iteration:1904 loss:1.8887524604797363 \n",
      "iteration:1905 loss:1.9133713245391846 \n",
      "iteration:1906 loss:1.9590466022491455 \n",
      "iteration:1907 loss:1.850538730621338 \n",
      "iteration:1908 loss:1.8911018371582031 \n",
      "iteration:1909 loss:1.9038708209991455 \n",
      "iteration:1910 loss:1.8531427383422852 \n",
      "iteration:1911 loss:1.8934789896011353 \n",
      "iteration:1912 loss:1.8784674406051636 \n",
      "iteration:1913 loss:1.8851914405822754 \n",
      "iteration:1914 loss:1.8536165952682495 \n",
      "iteration:1915 loss:1.8587266206741333 \n",
      "iteration:1916 loss:1.916233777999878 \n",
      "iteration:1917 loss:1.8574137687683105 \n",
      "iteration:1918 loss:1.8498728275299072 \n",
      "iteration:1919 loss:1.8414899110794067 \n",
      "iteration:1920 loss:1.9484450817108154 \n",
      "iteration:1921 loss:1.9044374227523804 \n",
      "iteration:1922 loss:1.8326283693313599 \n",
      "iteration:1923 loss:1.8470046520233154 \n",
      "iteration:1924 loss:1.858296513557434 \n",
      "iteration:1925 loss:1.873195767402649 \n",
      "iteration:1926 loss:1.8691633939743042 \n",
      "iteration:1927 loss:1.858821988105774 \n",
      "iteration:1928 loss:1.9228843450546265 \n",
      "iteration:1929 loss:1.8415988683700562 \n",
      "iteration:1930 loss:1.8292458057403564 \n",
      "iteration:1931 loss:1.8752264976501465 \n",
      "iteration:1932 loss:1.8855328559875488 \n",
      "iteration:1933 loss:1.896627426147461 \n",
      "iteration:1934 loss:1.8709564208984375 \n",
      "iteration:1935 loss:1.9061541557312012 \n",
      "iteration:1936 loss:1.8948533535003662 \n",
      "iteration:1937 loss:1.8946950435638428 \n",
      "iteration:1938 loss:1.8730673789978027 \n",
      "iteration:1939 loss:1.8331149816513062 \n",
      "iteration:1940 loss:1.869883418083191 \n",
      "iteration:1941 loss:1.8484059572219849 \n",
      "iteration:1942 loss:1.8441731929779053 \n",
      "iteration:1943 loss:1.8281517028808594 \n",
      "iteration:1944 loss:1.8876408338546753 \n",
      "iteration:1945 loss:1.9425204992294312 \n",
      "iteration:1946 loss:1.864661693572998 \n",
      "iteration:1947 loss:1.8522638082504272 \n",
      "iteration:1948 loss:1.8762595653533936 \n",
      "iteration:1949 loss:1.9028714895248413 \n",
      "iteration:1950 loss:1.853070855140686 \n",
      "iteration:1951 loss:1.896644115447998 \n",
      "iteration:1952 loss:1.853505253791809 \n",
      "iteration:1953 loss:1.843934178352356 \n",
      "iteration:1954 loss:1.8463506698608398 \n",
      "iteration:1955 loss:1.9106295108795166 \n",
      "iteration:1956 loss:1.8642468452453613 \n",
      "iteration:1957 loss:1.8963897228240967 \n",
      "iteration:1958 loss:1.8574308156967163 \n",
      "iteration:1959 loss:1.886053442955017 \n",
      "iteration:1960 loss:1.8871935606002808 \n",
      "iteration:1961 loss:1.8845330476760864 \n",
      "iteration:1962 loss:1.8715726137161255 \n",
      "iteration:1963 loss:1.8946051597595215 \n",
      "iteration:1964 loss:1.8736748695373535 \n",
      "iteration:1965 loss:1.890852689743042 \n",
      "iteration:1966 loss:1.8864527940750122 \n",
      "iteration:1967 loss:1.863488793373108 \n",
      "iteration:1968 loss:1.860388994216919 \n",
      "iteration:1969 loss:1.8419572114944458 \n",
      "iteration:1970 loss:1.857028603553772 \n",
      "iteration:1971 loss:1.93183171749115 \n",
      "iteration:1972 loss:1.8620553016662598 \n",
      "iteration:1973 loss:1.9157841205596924 \n",
      "iteration:1974 loss:1.884609580039978 \n",
      "iteration:1975 loss:1.897070288658142 \n",
      "iteration:1976 loss:1.8563258647918701 \n",
      "iteration:1977 loss:1.8615293502807617 \n",
      "iteration:1978 loss:1.8907438516616821 \n",
      "iteration:1979 loss:1.8843865394592285 \n",
      "iteration:1980 loss:1.8869099617004395 \n",
      "iteration:1981 loss:1.8918646574020386 \n",
      "iteration:1982 loss:1.8415461778640747 \n",
      "iteration:1983 loss:1.876915693283081 \n",
      "iteration:1984 loss:1.851361870765686 \n",
      "iteration:1985 loss:1.8980070352554321 \n",
      "iteration:1986 loss:1.8430962562561035 \n",
      "iteration:1987 loss:1.8592402935028076 \n",
      "iteration:1988 loss:1.826972484588623 \n",
      "iteration:1989 loss:1.8469172716140747 \n",
      "iteration:1990 loss:1.8618489503860474 \n",
      "iteration:1991 loss:1.8251385688781738 \n",
      "iteration:1992 loss:1.8722989559173584 \n",
      "iteration:1993 loss:1.9099513292312622 \n",
      "iteration:1994 loss:1.8596543073654175 \n",
      "iteration:1995 loss:1.8728169202804565 \n",
      "iteration:1996 loss:1.8735229969024658 \n",
      "iteration:1997 loss:1.9059776067733765 \n",
      "iteration:1998 loss:1.895365595817566 \n",
      "iteration:1999 loss:1.868268609046936 \n",
      "iteration:2000 loss:1.873487949371338 \n",
      "iteration:2001 loss:1.8404581546783447 \n",
      "iteration:2002 loss:1.8767006397247314 \n",
      "iteration:2003 loss:1.921081304550171 \n",
      "iteration:2004 loss:1.8443900346755981 \n",
      "iteration:2005 loss:1.8567456007003784 \n",
      "iteration:2006 loss:1.9107328653335571 \n",
      "iteration:2007 loss:1.8484830856323242 \n",
      "iteration:2008 loss:1.8674206733703613 \n",
      "iteration:2009 loss:1.8452427387237549 \n",
      "iteration:2010 loss:1.8732216358184814 \n",
      "iteration:2011 loss:1.906087875366211 \n",
      "iteration:2012 loss:1.8876898288726807 \n",
      "iteration:2013 loss:1.8099465370178223 \n",
      "iteration:2014 loss:1.8573030233383179 \n",
      "iteration:2015 loss:1.8537477254867554 \n",
      "iteration:2016 loss:1.8896925449371338 \n",
      "iteration:2017 loss:1.8860664367675781 \n",
      "iteration:2018 loss:1.862389326095581 \n",
      "iteration:2019 loss:1.8371057510375977 \n",
      "iteration:2020 loss:1.8477898836135864 \n",
      "iteration:2021 loss:1.864872694015503 \n",
      "iteration:2022 loss:1.8379770517349243 \n",
      "iteration:2023 loss:1.8494608402252197 \n",
      "iteration:2024 loss:1.8889245986938477 \n",
      "iteration:2025 loss:1.8731870651245117 \n",
      "iteration:2026 loss:1.905433177947998 \n",
      "iteration:2027 loss:1.8497918844223022 \n",
      "iteration:2028 loss:1.901058554649353 \n",
      "iteration:2029 loss:1.992132306098938 \n",
      "Epoch-14 lr: 0.0008187119948743448\n",
      "iteration:2030 loss:1.843062162399292 \n",
      "iteration:2031 loss:1.8622674942016602 \n",
      "iteration:2032 loss:1.868770956993103 \n",
      "iteration:2033 loss:1.8736279010772705 \n",
      "iteration:2034 loss:1.842954158782959 \n",
      "iteration:2035 loss:1.881514072418213 \n",
      "iteration:2036 loss:1.89018714427948 \n",
      "iteration:2037 loss:1.8780800104141235 \n",
      "iteration:2038 loss:1.8821252584457397 \n",
      "iteration:2039 loss:1.875333309173584 \n",
      "iteration:2040 loss:1.834733486175537 \n",
      "iteration:2041 loss:1.8791109323501587 \n",
      "iteration:2042 loss:1.9118387699127197 \n",
      "iteration:2043 loss:1.8649741411209106 \n",
      "iteration:2044 loss:1.8727238178253174 \n",
      "iteration:2045 loss:1.8875256776809692 \n",
      "iteration:2046 loss:1.810682773590088 \n",
      "iteration:2047 loss:1.8821189403533936 \n",
      "iteration:2048 loss:1.87370765209198 \n",
      "iteration:2049 loss:1.913237452507019 \n",
      "iteration:2050 loss:1.86639404296875 \n",
      "iteration:2051 loss:1.8934807777404785 \n",
      "iteration:2052 loss:1.902704119682312 \n",
      "iteration:2053 loss:1.8708239793777466 \n",
      "iteration:2054 loss:1.8334224224090576 \n",
      "iteration:2055 loss:1.8684049844741821 \n",
      "iteration:2056 loss:1.8465849161148071 \n",
      "iteration:2057 loss:1.876738429069519 \n",
      "iteration:2058 loss:1.8671687841415405 \n",
      "iteration:2059 loss:1.8708231449127197 \n",
      "iteration:2060 loss:1.8700135946273804 \n",
      "iteration:2061 loss:1.8662610054016113 \n",
      "iteration:2062 loss:1.8225739002227783 \n",
      "iteration:2063 loss:1.8655871152877808 \n",
      "iteration:2064 loss:1.843241810798645 \n",
      "iteration:2065 loss:1.8686943054199219 \n",
      "iteration:2066 loss:1.8548816442489624 \n",
      "iteration:2067 loss:1.874725580215454 \n",
      "iteration:2068 loss:1.836206078529358 \n",
      "iteration:2069 loss:1.8880776166915894 \n",
      "iteration:2070 loss:1.8168656826019287 \n",
      "iteration:2071 loss:1.8436285257339478 \n",
      "iteration:2072 loss:1.8417701721191406 \n",
      "iteration:2073 loss:1.9456888437271118 \n",
      "iteration:2074 loss:1.8744486570358276 \n",
      "iteration:2075 loss:1.9236747026443481 \n",
      "iteration:2076 loss:1.8733272552490234 \n",
      "iteration:2077 loss:1.9258071184158325 \n",
      "iteration:2078 loss:1.8409950733184814 \n",
      "iteration:2079 loss:1.8658620119094849 \n",
      "iteration:2080 loss:1.8392601013183594 \n",
      "iteration:2081 loss:1.8853912353515625 \n",
      "iteration:2082 loss:1.9024015665054321 \n",
      "iteration:2083 loss:1.8606411218643188 \n",
      "iteration:2084 loss:1.8696225881576538 \n",
      "iteration:2085 loss:1.8105132579803467 \n",
      "iteration:2086 loss:1.8997660875320435 \n",
      "iteration:2087 loss:1.861670732498169 \n",
      "iteration:2088 loss:1.8855663537979126 \n",
      "iteration:2089 loss:1.8523807525634766 \n",
      "iteration:2090 loss:1.875251293182373 \n",
      "iteration:2091 loss:1.9077309370040894 \n",
      "iteration:2092 loss:1.9075437784194946 \n",
      "iteration:2093 loss:1.833869457244873 \n",
      "iteration:2094 loss:1.860250473022461 \n",
      "iteration:2095 loss:1.8785426616668701 \n",
      "iteration:2096 loss:1.7925657033920288 \n",
      "iteration:2097 loss:1.8562979698181152 \n",
      "iteration:2098 loss:1.9271553754806519 \n",
      "iteration:2099 loss:1.8677247762680054 \n",
      "iteration:2100 loss:1.8714386224746704 \n",
      "iteration:2101 loss:1.902147650718689 \n",
      "iteration:2102 loss:1.7968491315841675 \n",
      "iteration:2103 loss:1.8779650926589966 \n",
      "iteration:2104 loss:1.8409334421157837 \n",
      "iteration:2105 loss:1.8590834140777588 \n",
      "iteration:2106 loss:1.8894354104995728 \n",
      "iteration:2107 loss:1.8529996871948242 \n",
      "iteration:2108 loss:1.9174658060073853 \n",
      "iteration:2109 loss:1.9006156921386719 \n",
      "iteration:2110 loss:1.8777620792388916 \n",
      "iteration:2111 loss:1.8760684728622437 \n",
      "iteration:2112 loss:1.847355604171753 \n",
      "iteration:2113 loss:1.9119960069656372 \n",
      "iteration:2114 loss:1.857100248336792 \n",
      "iteration:2115 loss:1.8468375205993652 \n",
      "iteration:2116 loss:1.8781229257583618 \n",
      "iteration:2117 loss:1.940568447113037 \n",
      "iteration:2118 loss:1.9339443445205688 \n",
      "iteration:2119 loss:1.9082082509994507 \n",
      "iteration:2120 loss:1.9076534509658813 \n",
      "iteration:2121 loss:1.9183813333511353 \n",
      "iteration:2122 loss:1.8313645124435425 \n",
      "iteration:2123 loss:1.8770406246185303 \n",
      "iteration:2124 loss:1.8500950336456299 \n",
      "iteration:2125 loss:1.9149353504180908 \n",
      "iteration:2126 loss:1.8807578086853027 \n",
      "iteration:2127 loss:1.869405746459961 \n",
      "iteration:2128 loss:1.8528165817260742 \n",
      "iteration:2129 loss:1.919276475906372 \n",
      "iteration:2130 loss:1.8790168762207031 \n",
      "iteration:2131 loss:1.8920902013778687 \n",
      "iteration:2132 loss:1.895592451095581 \n",
      "iteration:2133 loss:1.8714040517807007 \n",
      "iteration:2134 loss:1.883561372756958 \n",
      "iteration:2135 loss:1.8442898988723755 \n",
      "iteration:2136 loss:1.8781671524047852 \n",
      "iteration:2137 loss:1.8756868839263916 \n",
      "iteration:2138 loss:1.8581675291061401 \n",
      "iteration:2139 loss:1.8708579540252686 \n",
      "iteration:2140 loss:1.8752291202545166 \n",
      "iteration:2141 loss:1.8562331199645996 \n",
      "iteration:2142 loss:1.884128451347351 \n",
      "iteration:2143 loss:1.8771040439605713 \n",
      "iteration:2144 loss:1.9172987937927246 \n",
      "iteration:2145 loss:1.873327374458313 \n",
      "iteration:2146 loss:1.8112525939941406 \n",
      "iteration:2147 loss:1.8639636039733887 \n",
      "iteration:2148 loss:1.8472546339035034 \n",
      "iteration:2149 loss:1.8860019445419312 \n",
      "iteration:2150 loss:1.8705344200134277 \n",
      "iteration:2151 loss:1.7958377599716187 \n",
      "iteration:2152 loss:1.8406909704208374 \n",
      "iteration:2153 loss:1.8937381505966187 \n",
      "iteration:2154 loss:1.8550996780395508 \n",
      "iteration:2155 loss:1.9066815376281738 \n",
      "iteration:2156 loss:1.874626636505127 \n",
      "iteration:2157 loss:1.8474326133728027 \n",
      "iteration:2158 loss:1.8760966062545776 \n",
      "iteration:2159 loss:1.8801203966140747 \n",
      "iteration:2160 loss:1.8443255424499512 \n",
      "iteration:2161 loss:1.884116768836975 \n",
      "iteration:2162 loss:1.902776837348938 \n",
      "iteration:2163 loss:1.832580327987671 \n",
      "iteration:2164 loss:1.8501155376434326 \n",
      "iteration:2165 loss:1.8326119184494019 \n",
      "iteration:2166 loss:1.8863179683685303 \n",
      "iteration:2167 loss:1.8517050743103027 \n",
      "iteration:2168 loss:1.8727394342422485 \n",
      "iteration:2169 loss:1.7963052988052368 \n",
      "iteration:2170 loss:1.8455615043640137 \n",
      "iteration:2171 loss:1.9007354974746704 \n",
      "iteration:2172 loss:1.8498129844665527 \n",
      "iteration:2173 loss:1.8211700916290283 \n",
      "iteration:2174 loss:1.8823440074920654 \n",
      "Epoch-15 lr: 0.0007938926261462366\n",
      "iteration:2175 loss:1.863303780555725 \n",
      "iteration:2176 loss:1.7984037399291992 \n",
      "iteration:2177 loss:1.8539071083068848 \n",
      "iteration:2178 loss:1.841939926147461 \n",
      "iteration:2179 loss:1.8775991201400757 \n",
      "iteration:2180 loss:1.8819977045059204 \n",
      "iteration:2181 loss:1.8680310249328613 \n",
      "iteration:2182 loss:1.8692641258239746 \n",
      "iteration:2183 loss:1.853661060333252 \n",
      "iteration:2184 loss:1.8986254930496216 \n",
      "iteration:2185 loss:1.856584072113037 \n",
      "iteration:2186 loss:1.9121989011764526 \n",
      "iteration:2187 loss:1.894745945930481 \n",
      "iteration:2188 loss:1.9226258993148804 \n",
      "iteration:2189 loss:1.9166853427886963 \n",
      "iteration:2190 loss:1.8858377933502197 \n",
      "iteration:2191 loss:1.8896716833114624 \n",
      "iteration:2192 loss:1.8431074619293213 \n",
      "iteration:2193 loss:1.827643632888794 \n",
      "iteration:2194 loss:1.8789654970169067 \n",
      "iteration:2195 loss:1.8896653652191162 \n",
      "iteration:2196 loss:1.8845810890197754 \n",
      "iteration:2197 loss:1.8457692861557007 \n",
      "iteration:2198 loss:1.8444645404815674 \n",
      "iteration:2199 loss:1.8420027494430542 \n",
      "iteration:2200 loss:1.8075512647628784 \n",
      "iteration:2201 loss:1.886494755744934 \n",
      "iteration:2202 loss:1.8842344284057617 \n",
      "iteration:2203 loss:1.880407452583313 \n",
      "iteration:2204 loss:1.8848508596420288 \n",
      "iteration:2205 loss:1.8253403902053833 \n",
      "iteration:2206 loss:1.8274515867233276 \n",
      "iteration:2207 loss:1.806037425994873 \n",
      "iteration:2208 loss:1.8380143642425537 \n",
      "iteration:2209 loss:1.8855212926864624 \n",
      "iteration:2210 loss:1.9050315618515015 \n",
      "iteration:2211 loss:1.9145029783248901 \n",
      "iteration:2212 loss:1.8531073331832886 \n",
      "iteration:2213 loss:1.8518708944320679 \n",
      "iteration:2214 loss:1.9046791791915894 \n",
      "iteration:2215 loss:1.8465083837509155 \n",
      "iteration:2216 loss:1.8910406827926636 \n",
      "iteration:2217 loss:1.8807660341262817 \n",
      "iteration:2218 loss:1.8923563957214355 \n",
      "iteration:2219 loss:1.8233758211135864 \n",
      "iteration:2220 loss:1.8786702156066895 \n",
      "iteration:2221 loss:1.8533148765563965 \n",
      "iteration:2222 loss:1.827453851699829 \n",
      "iteration:2223 loss:1.8746758699417114 \n",
      "iteration:2224 loss:1.8930684328079224 \n",
      "iteration:2225 loss:1.8647509813308716 \n",
      "iteration:2226 loss:1.8137385845184326 \n",
      "iteration:2227 loss:1.8488657474517822 \n",
      "iteration:2228 loss:1.854677677154541 \n",
      "iteration:2229 loss:1.8448950052261353 \n",
      "iteration:2230 loss:1.8661348819732666 \n",
      "iteration:2231 loss:1.8504773378372192 \n",
      "iteration:2232 loss:1.8568944931030273 \n",
      "iteration:2233 loss:1.8278279304504395 \n",
      "iteration:2234 loss:1.8579593896865845 \n",
      "iteration:2235 loss:1.8730658292770386 \n",
      "iteration:2236 loss:1.8477213382720947 \n",
      "iteration:2237 loss:1.8334293365478516 \n",
      "iteration:2238 loss:1.834944248199463 \n",
      "iteration:2239 loss:1.8548048734664917 \n",
      "iteration:2240 loss:1.852050542831421 \n",
      "iteration:2241 loss:1.8683756589889526 \n",
      "iteration:2242 loss:1.7900972366333008 \n",
      "iteration:2243 loss:1.8569810390472412 \n",
      "iteration:2244 loss:1.8869836330413818 \n",
      "iteration:2245 loss:1.840213656425476 \n",
      "iteration:2246 loss:1.8391300439834595 \n",
      "iteration:2247 loss:1.8641000986099243 \n",
      "iteration:2248 loss:1.872273325920105 \n",
      "iteration:2249 loss:1.854640245437622 \n",
      "iteration:2250 loss:1.869221806526184 \n",
      "iteration:2251 loss:1.9060032367706299 \n",
      "iteration:2252 loss:1.833436131477356 \n",
      "iteration:2253 loss:1.8498846292495728 \n",
      "iteration:2254 loss:1.9001678228378296 \n",
      "iteration:2255 loss:1.8665246963500977 \n",
      "iteration:2256 loss:1.8155299425125122 \n",
      "iteration:2257 loss:1.8655277490615845 \n",
      "iteration:2258 loss:1.8988616466522217 \n",
      "iteration:2259 loss:1.909385323524475 \n",
      "iteration:2260 loss:1.8212984800338745 \n",
      "iteration:2261 loss:1.907709002494812 \n",
      "iteration:2262 loss:1.8348972797393799 \n",
      "iteration:2263 loss:1.8553900718688965 \n",
      "iteration:2264 loss:1.8738411664962769 \n",
      "iteration:2265 loss:1.8491780757904053 \n",
      "iteration:2266 loss:1.8363151550292969 \n",
      "iteration:2267 loss:1.8183562755584717 \n",
      "iteration:2268 loss:1.8249051570892334 \n",
      "iteration:2269 loss:1.8092470169067383 \n",
      "iteration:2270 loss:1.9204190969467163 \n",
      "iteration:2271 loss:1.8969712257385254 \n",
      "iteration:2272 loss:1.8822108507156372 \n",
      "iteration:2273 loss:1.8432419300079346 \n",
      "iteration:2274 loss:1.8646401166915894 \n",
      "iteration:2275 loss:1.8928087949752808 \n",
      "iteration:2276 loss:1.8562546968460083 \n",
      "iteration:2277 loss:1.8271976709365845 \n",
      "iteration:2278 loss:1.851301670074463 \n",
      "iteration:2279 loss:1.8636564016342163 \n",
      "iteration:2280 loss:1.852532982826233 \n",
      "iteration:2281 loss:1.838131070137024 \n",
      "iteration:2282 loss:1.7992417812347412 \n",
      "iteration:2283 loss:1.8680928945541382 \n",
      "iteration:2284 loss:1.9236787557601929 \n",
      "iteration:2285 loss:1.8348489999771118 \n",
      "iteration:2286 loss:1.8632723093032837 \n",
      "iteration:2287 loss:1.8547625541687012 \n",
      "iteration:2288 loss:1.813015341758728 \n",
      "iteration:2289 loss:1.88227117061615 \n",
      "iteration:2290 loss:1.9232944250106812 \n",
      "iteration:2291 loss:1.8640153408050537 \n",
      "iteration:2292 loss:1.8577296733856201 \n",
      "iteration:2293 loss:1.8170483112335205 \n",
      "iteration:2294 loss:1.8333662748336792 \n",
      "iteration:2295 loss:1.86078941822052 \n",
      "iteration:2296 loss:1.781460165977478 \n",
      "iteration:2297 loss:1.8526381254196167 \n",
      "iteration:2298 loss:1.900065541267395 \n",
      "iteration:2299 loss:1.842522382736206 \n",
      "iteration:2300 loss:1.896437168121338 \n",
      "iteration:2301 loss:1.8656069040298462 \n",
      "iteration:2302 loss:1.863783597946167 \n",
      "iteration:2303 loss:1.9036128520965576 \n",
      "iteration:2304 loss:1.876959204673767 \n",
      "iteration:2305 loss:1.8923211097717285 \n",
      "iteration:2306 loss:1.8763298988342285 \n",
      "iteration:2307 loss:1.8392181396484375 \n",
      "iteration:2308 loss:1.8707430362701416 \n",
      "iteration:2309 loss:1.8671226501464844 \n",
      "iteration:2310 loss:1.8515863418579102 \n",
      "iteration:2311 loss:1.8631951808929443 \n",
      "iteration:2312 loss:1.847562313079834 \n",
      "iteration:2313 loss:1.8231253623962402 \n",
      "iteration:2314 loss:1.8827399015426636 \n",
      "iteration:2315 loss:1.885968804359436 \n",
      "iteration:2316 loss:1.8878346681594849 \n",
      "iteration:2317 loss:1.8497118949890137 \n",
      "iteration:2318 loss:1.8866913318634033 \n",
      "iteration:2319 loss:2.004850387573242 \n",
      "Epoch-16 lr: 0.0007679133974894982\n",
      "iteration:2320 loss:1.8530269861221313 \n",
      "iteration:2321 loss:1.8167102336883545 \n",
      "iteration:2322 loss:1.8215293884277344 \n",
      "iteration:2323 loss:1.8549994230270386 \n",
      "iteration:2324 loss:1.838401436805725 \n",
      "iteration:2325 loss:1.8518190383911133 \n",
      "iteration:2326 loss:1.908781886100769 \n",
      "iteration:2327 loss:1.8745174407958984 \n",
      "iteration:2328 loss:1.8891839981079102 \n",
      "iteration:2329 loss:1.9072809219360352 \n",
      "iteration:2330 loss:1.8179974555969238 \n",
      "iteration:2331 loss:1.8385893106460571 \n",
      "iteration:2332 loss:1.8752846717834473 \n",
      "iteration:2333 loss:1.816389560699463 \n",
      "iteration:2334 loss:1.844113826751709 \n",
      "iteration:2335 loss:1.8245763778686523 \n",
      "iteration:2336 loss:1.883906602859497 \n",
      "iteration:2337 loss:1.8086525201797485 \n",
      "iteration:2338 loss:1.8696861267089844 \n",
      "iteration:2339 loss:1.8229879140853882 \n",
      "iteration:2340 loss:1.8569203615188599 \n",
      "iteration:2341 loss:1.901166558265686 \n",
      "iteration:2342 loss:1.8615086078643799 \n",
      "iteration:2343 loss:1.8512182235717773 \n",
      "iteration:2344 loss:1.8338053226470947 \n",
      "iteration:2345 loss:1.8307440280914307 \n",
      "iteration:2346 loss:1.9122185707092285 \n",
      "iteration:2347 loss:1.8495235443115234 \n",
      "iteration:2348 loss:1.8514225482940674 \n",
      "iteration:2349 loss:1.8484039306640625 \n",
      "iteration:2350 loss:1.842075228691101 \n",
      "iteration:2351 loss:1.8453731536865234 \n",
      "iteration:2352 loss:1.8306493759155273 \n",
      "iteration:2353 loss:1.873134970664978 \n",
      "iteration:2354 loss:1.833444595336914 \n",
      "iteration:2355 loss:1.8677685260772705 \n",
      "iteration:2356 loss:1.843166708946228 \n",
      "iteration:2357 loss:1.8286714553833008 \n",
      "iteration:2358 loss:1.825495958328247 \n",
      "iteration:2359 loss:1.8953324556350708 \n",
      "iteration:2360 loss:1.8329603672027588 \n",
      "iteration:2361 loss:1.8691446781158447 \n",
      "iteration:2362 loss:1.8366973400115967 \n",
      "iteration:2363 loss:1.8553218841552734 \n",
      "iteration:2364 loss:1.8922914266586304 \n",
      "iteration:2365 loss:1.838059902191162 \n",
      "iteration:2366 loss:1.8548097610473633 \n",
      "iteration:2367 loss:1.8612812757492065 \n",
      "iteration:2368 loss:1.8225886821746826 \n",
      "iteration:2369 loss:1.830857753753662 \n",
      "iteration:2370 loss:1.8423148393630981 \n",
      "iteration:2371 loss:1.8635038137435913 \n",
      "iteration:2372 loss:1.8490324020385742 \n",
      "iteration:2373 loss:1.8555018901824951 \n",
      "iteration:2374 loss:1.8274434804916382 \n",
      "iteration:2375 loss:1.8378493785858154 \n",
      "iteration:2376 loss:1.8838893175125122 \n",
      "iteration:2377 loss:1.8541017770767212 \n",
      "iteration:2378 loss:1.7884480953216553 \n",
      "iteration:2379 loss:1.8688496351242065 \n",
      "iteration:2380 loss:1.7937175035476685 \n",
      "iteration:2381 loss:1.8686285018920898 \n",
      "iteration:2382 loss:1.8409579992294312 \n",
      "iteration:2383 loss:1.8371803760528564 \n",
      "iteration:2384 loss:1.8119364976882935 \n",
      "iteration:2385 loss:1.8854937553405762 \n",
      "iteration:2386 loss:1.8268768787384033 \n",
      "iteration:2387 loss:1.8485904932022095 \n",
      "iteration:2388 loss:1.8524492979049683 \n",
      "iteration:2389 loss:1.8782806396484375 \n",
      "iteration:2390 loss:1.829185128211975 \n",
      "iteration:2391 loss:1.8499623537063599 \n",
      "iteration:2392 loss:1.8726509809494019 \n",
      "iteration:2393 loss:1.825290560722351 \n",
      "iteration:2394 loss:1.8366624116897583 \n",
      "iteration:2395 loss:1.8331612348556519 \n",
      "iteration:2396 loss:1.8489001989364624 \n",
      "iteration:2397 loss:1.847712755203247 \n",
      "iteration:2398 loss:1.8451385498046875 \n",
      "iteration:2399 loss:1.7939363718032837 \n",
      "iteration:2400 loss:1.8830595016479492 \n",
      "iteration:2401 loss:1.8770133256912231 \n",
      "iteration:2402 loss:1.83684504032135 \n",
      "iteration:2403 loss:1.8280987739562988 \n",
      "iteration:2404 loss:1.882719874382019 \n",
      "iteration:2405 loss:1.8532627820968628 \n",
      "iteration:2406 loss:1.8684834241867065 \n",
      "iteration:2407 loss:1.8417819738388062 \n",
      "iteration:2408 loss:1.899624228477478 \n",
      "iteration:2409 loss:1.8363816738128662 \n",
      "iteration:2410 loss:1.8546925783157349 \n",
      "iteration:2411 loss:1.8686637878417969 \n",
      "iteration:2412 loss:1.8656176328659058 \n",
      "iteration:2413 loss:1.8711447715759277 \n",
      "iteration:2414 loss:1.8698511123657227 \n",
      "iteration:2415 loss:1.8296672105789185 \n",
      "iteration:2416 loss:1.877048134803772 \n",
      "iteration:2417 loss:1.863131046295166 \n",
      "iteration:2418 loss:1.8252921104431152 \n",
      "iteration:2419 loss:1.8344306945800781 \n",
      "iteration:2420 loss:1.8734787702560425 \n",
      "iteration:2421 loss:1.8782920837402344 \n",
      "iteration:2422 loss:1.7981221675872803 \n",
      "iteration:2423 loss:1.7822394371032715 \n",
      "iteration:2424 loss:1.8397550582885742 \n",
      "iteration:2425 loss:1.8014544248580933 \n",
      "iteration:2426 loss:1.8213019371032715 \n",
      "iteration:2427 loss:1.841577410697937 \n",
      "iteration:2428 loss:1.8481227159500122 \n",
      "iteration:2429 loss:1.8542938232421875 \n",
      "iteration:2430 loss:1.8303191661834717 \n",
      "iteration:2431 loss:1.8206868171691895 \n",
      "iteration:2432 loss:1.8593257665634155 \n",
      "iteration:2433 loss:1.8593422174453735 \n",
      "iteration:2434 loss:1.8312331438064575 \n",
      "iteration:2435 loss:1.874966025352478 \n",
      "iteration:2436 loss:1.8757272958755493 \n",
      "iteration:2437 loss:1.9203851222991943 \n",
      "iteration:2438 loss:1.888719916343689 \n",
      "iteration:2439 loss:1.844712734222412 \n",
      "iteration:2440 loss:1.8765969276428223 \n",
      "iteration:2441 loss:1.8118082284927368 \n",
      "iteration:2442 loss:1.8673890829086304 \n",
      "iteration:2443 loss:1.8842004537582397 \n",
      "iteration:2444 loss:1.8389302492141724 \n",
      "iteration:2445 loss:1.8775309324264526 \n",
      "iteration:2446 loss:1.850774884223938 \n",
      "iteration:2447 loss:1.8357319831848145 \n",
      "iteration:2448 loss:1.8621066808700562 \n",
      "iteration:2449 loss:1.8443398475646973 \n",
      "iteration:2450 loss:1.8781602382659912 \n",
      "iteration:2451 loss:1.8032293319702148 \n",
      "iteration:2452 loss:1.8457163572311401 \n",
      "iteration:2453 loss:1.8500077724456787 \n",
      "iteration:2454 loss:1.866326928138733 \n",
      "iteration:2455 loss:1.8430029153823853 \n",
      "iteration:2456 loss:1.893852710723877 \n",
      "iteration:2457 loss:1.8393574953079224 \n",
      "iteration:2458 loss:1.8224133253097534 \n",
      "iteration:2459 loss:1.8002347946166992 \n",
      "iteration:2460 loss:1.8453826904296875 \n",
      "iteration:2461 loss:1.8082466125488281 \n",
      "iteration:2462 loss:1.8374418020248413 \n",
      "iteration:2463 loss:1.8470959663391113 \n",
      "iteration:2464 loss:1.9450892210006714 \n",
      "Epoch-17 lr: 0.0007408768370508576\n",
      "iteration:2465 loss:1.8833876848220825 \n",
      "iteration:2466 loss:1.8357990980148315 \n",
      "iteration:2467 loss:1.8223997354507446 \n",
      "iteration:2468 loss:1.8389556407928467 \n",
      "iteration:2469 loss:1.9079184532165527 \n",
      "iteration:2470 loss:1.8305938243865967 \n",
      "iteration:2471 loss:1.8527636528015137 \n",
      "iteration:2472 loss:1.8056387901306152 \n",
      "iteration:2473 loss:1.8802582025527954 \n",
      "iteration:2474 loss:1.8323177099227905 \n",
      "iteration:2475 loss:1.8604732751846313 \n",
      "iteration:2476 loss:1.8549859523773193 \n",
      "iteration:2477 loss:1.898075819015503 \n",
      "iteration:2478 loss:1.8448765277862549 \n",
      "iteration:2479 loss:1.879550576210022 \n",
      "iteration:2480 loss:1.867987871170044 \n",
      "iteration:2481 loss:1.873328447341919 \n",
      "iteration:2482 loss:1.7964595556259155 \n",
      "iteration:2483 loss:1.836451768875122 \n",
      "iteration:2484 loss:1.8494653701782227 \n",
      "iteration:2485 loss:1.8515331745147705 \n",
      "iteration:2486 loss:1.8433774709701538 \n",
      "iteration:2487 loss:1.8551857471466064 \n",
      "iteration:2488 loss:1.8406219482421875 \n",
      "iteration:2489 loss:1.8524971008300781 \n",
      "iteration:2490 loss:1.807104468345642 \n",
      "iteration:2491 loss:1.8228051662445068 \n",
      "iteration:2492 loss:1.8664838075637817 \n",
      "iteration:2493 loss:1.7843161821365356 \n",
      "iteration:2494 loss:1.8340373039245605 \n",
      "iteration:2495 loss:1.8133150339126587 \n",
      "iteration:2496 loss:1.892021894454956 \n",
      "iteration:2497 loss:1.8001387119293213 \n",
      "iteration:2498 loss:1.8663153648376465 \n",
      "iteration:2499 loss:1.8898392915725708 \n",
      "iteration:2500 loss:1.8098952770233154 \n",
      "iteration:2501 loss:1.8241827487945557 \n",
      "iteration:2502 loss:1.8487060070037842 \n",
      "iteration:2503 loss:1.834079623222351 \n",
      "iteration:2504 loss:1.8434576988220215 \n",
      "iteration:2505 loss:1.8359344005584717 \n",
      "iteration:2506 loss:1.875421166419983 \n",
      "iteration:2507 loss:1.902964472770691 \n",
      "iteration:2508 loss:1.8267728090286255 \n",
      "iteration:2509 loss:1.818660855293274 \n",
      "iteration:2510 loss:1.8307468891143799 \n",
      "iteration:2511 loss:1.8225961923599243 \n",
      "iteration:2512 loss:1.8789085149765015 \n",
      "iteration:2513 loss:1.8527066707611084 \n",
      "iteration:2514 loss:1.741861343383789 \n",
      "iteration:2515 loss:1.8924041986465454 \n",
      "iteration:2516 loss:1.8376855850219727 \n",
      "iteration:2517 loss:1.8157962560653687 \n",
      "iteration:2518 loss:1.8891512155532837 \n",
      "iteration:2519 loss:1.8978193998336792 \n",
      "iteration:2520 loss:1.9009393453598022 \n",
      "iteration:2521 loss:1.8813832998275757 \n",
      "iteration:2522 loss:1.7489123344421387 \n",
      "iteration:2523 loss:1.859127402305603 \n",
      "iteration:2524 loss:1.8509008884429932 \n",
      "iteration:2525 loss:1.8703680038452148 \n",
      "iteration:2526 loss:1.8427127599716187 \n",
      "iteration:2527 loss:1.8723238706588745 \n",
      "iteration:2528 loss:1.805443286895752 \n",
      "iteration:2529 loss:1.8511464595794678 \n",
      "iteration:2530 loss:1.8367081880569458 \n",
      "iteration:2531 loss:1.8658519983291626 \n",
      "iteration:2532 loss:1.8651031255722046 \n",
      "iteration:2533 loss:1.8458833694458008 \n",
      "iteration:2534 loss:1.8317612409591675 \n",
      "iteration:2535 loss:1.8191338777542114 \n",
      "iteration:2536 loss:1.8361223936080933 \n",
      "iteration:2537 loss:1.8436670303344727 \n",
      "iteration:2538 loss:1.849141001701355 \n",
      "iteration:2539 loss:1.8122884035110474 \n",
      "iteration:2540 loss:1.7878795862197876 \n",
      "iteration:2541 loss:1.8356132507324219 \n",
      "iteration:2542 loss:1.8336509466171265 \n",
      "iteration:2543 loss:1.8381420373916626 \n",
      "iteration:2544 loss:1.8675594329833984 \n",
      "iteration:2545 loss:1.8715726137161255 \n",
      "iteration:2546 loss:1.8101553916931152 \n",
      "iteration:2547 loss:1.84984290599823 \n",
      "iteration:2548 loss:1.8095680475234985 \n",
      "iteration:2549 loss:1.8624790906906128 \n",
      "iteration:2550 loss:1.8463252782821655 \n",
      "iteration:2551 loss:1.8017224073410034 \n",
      "iteration:2552 loss:1.8409409523010254 \n",
      "iteration:2553 loss:1.8008229732513428 \n",
      "iteration:2554 loss:1.8159308433532715 \n",
      "iteration:2555 loss:1.835242509841919 \n",
      "iteration:2556 loss:1.866800308227539 \n",
      "iteration:2557 loss:1.870157241821289 \n",
      "iteration:2558 loss:1.8573777675628662 \n",
      "iteration:2559 loss:1.8244833946228027 \n",
      "iteration:2560 loss:1.8364062309265137 \n",
      "iteration:2561 loss:1.8609635829925537 \n",
      "iteration:2562 loss:1.8217482566833496 \n",
      "iteration:2563 loss:1.8318381309509277 \n",
      "iteration:2564 loss:1.8848097324371338 \n",
      "iteration:2565 loss:1.8406846523284912 \n",
      "iteration:2566 loss:1.8350050449371338 \n",
      "iteration:2567 loss:1.8528389930725098 \n",
      "iteration:2568 loss:1.8594549894332886 \n",
      "iteration:2569 loss:1.8566125631332397 \n",
      "iteration:2570 loss:1.8462467193603516 \n",
      "iteration:2571 loss:1.8029779195785522 \n",
      "iteration:2572 loss:1.8084232807159424 \n",
      "iteration:2573 loss:1.864139437675476 \n",
      "iteration:2574 loss:1.8532027006149292 \n",
      "iteration:2575 loss:1.8245536088943481 \n",
      "iteration:2576 loss:1.8262758255004883 \n",
      "iteration:2577 loss:1.8283129930496216 \n",
      "iteration:2578 loss:1.8955802917480469 \n",
      "iteration:2579 loss:1.800279974937439 \n",
      "iteration:2580 loss:1.802872896194458 \n",
      "iteration:2581 loss:1.8151109218597412 \n",
      "iteration:2582 loss:1.842065691947937 \n",
      "iteration:2583 loss:1.8208587169647217 \n",
      "iteration:2584 loss:1.8753180503845215 \n",
      "iteration:2585 loss:1.8714426755905151 \n",
      "iteration:2586 loss:1.8546942472457886 \n",
      "iteration:2587 loss:1.8885618448257446 \n",
      "iteration:2588 loss:1.8103456497192383 \n",
      "iteration:2589 loss:1.8434088230133057 \n",
      "iteration:2590 loss:1.8376531600952148 \n",
      "iteration:2591 loss:1.8869960308074951 \n",
      "iteration:2592 loss:1.839026927947998 \n",
      "iteration:2593 loss:1.8752493858337402 \n",
      "iteration:2594 loss:1.8685798645019531 \n",
      "iteration:2595 loss:1.845765233039856 \n",
      "iteration:2596 loss:1.8426427841186523 \n",
      "iteration:2597 loss:1.8782234191894531 \n",
      "iteration:2598 loss:1.8587076663970947 \n",
      "iteration:2599 loss:1.9009002447128296 \n",
      "iteration:2600 loss:1.8191174268722534 \n",
      "iteration:2601 loss:1.8542897701263428 \n",
      "iteration:2602 loss:1.8300527334213257 \n",
      "iteration:2603 loss:1.8299745321273804 \n",
      "iteration:2604 loss:1.8154003620147705 \n",
      "iteration:2605 loss:1.8407552242279053 \n",
      "iteration:2606 loss:1.8515530824661255 \n",
      "iteration:2607 loss:1.8360973596572876 \n",
      "iteration:2608 loss:1.8531978130340576 \n",
      "iteration:2609 loss:1.9932634830474854 \n",
      "Epoch-18 lr: 0.0007128896457825362\n",
      "iteration:2610 loss:1.846574306488037 \n",
      "iteration:2611 loss:1.8597469329833984 \n",
      "iteration:2612 loss:1.8488245010375977 \n",
      "iteration:2613 loss:1.8155174255371094 \n",
      "iteration:2614 loss:1.8261899948120117 \n",
      "iteration:2615 loss:1.7979698181152344 \n",
      "iteration:2616 loss:1.8123632669448853 \n",
      "iteration:2617 loss:1.8479925394058228 \n",
      "iteration:2618 loss:1.8831913471221924 \n",
      "iteration:2619 loss:1.8126723766326904 \n",
      "iteration:2620 loss:1.826469898223877 \n",
      "iteration:2621 loss:1.8236080408096313 \n",
      "iteration:2622 loss:1.8374103307724 \n",
      "iteration:2623 loss:1.853610634803772 \n",
      "iteration:2624 loss:1.876800298690796 \n",
      "iteration:2625 loss:1.8248329162597656 \n",
      "iteration:2626 loss:1.8826428651809692 \n",
      "iteration:2627 loss:1.8109393119812012 \n",
      "iteration:2628 loss:1.7910144329071045 \n",
      "iteration:2629 loss:1.876501441001892 \n",
      "iteration:2630 loss:1.8876063823699951 \n",
      "iteration:2631 loss:1.8945072889328003 \n",
      "iteration:2632 loss:1.8661171197891235 \n",
      "iteration:2633 loss:1.8467508554458618 \n",
      "iteration:2634 loss:1.815674066543579 \n",
      "iteration:2635 loss:1.883992314338684 \n",
      "iteration:2636 loss:1.8017431497573853 \n",
      "iteration:2637 loss:1.8115743398666382 \n",
      "iteration:2638 loss:1.8935011625289917 \n",
      "iteration:2639 loss:1.9109468460083008 \n",
      "iteration:2640 loss:1.9103903770446777 \n",
      "iteration:2641 loss:1.8264565467834473 \n",
      "iteration:2642 loss:1.8485971689224243 \n",
      "iteration:2643 loss:1.8479769229888916 \n",
      "iteration:2644 loss:1.8817427158355713 \n",
      "iteration:2645 loss:1.8279818296432495 \n",
      "iteration:2646 loss:1.8512187004089355 \n",
      "iteration:2647 loss:1.822771430015564 \n",
      "iteration:2648 loss:1.804795742034912 \n",
      "iteration:2649 loss:1.8289693593978882 \n",
      "iteration:2650 loss:1.8596433401107788 \n",
      "iteration:2651 loss:1.827110767364502 \n",
      "iteration:2652 loss:1.7841339111328125 \n",
      "iteration:2653 loss:1.805077314376831 \n",
      "iteration:2654 loss:1.8444792032241821 \n",
      "iteration:2655 loss:1.8219740390777588 \n",
      "iteration:2656 loss:1.8513425588607788 \n",
      "iteration:2657 loss:1.8027254343032837 \n",
      "iteration:2658 loss:1.8494518995285034 \n",
      "iteration:2659 loss:1.8138850927352905 \n",
      "iteration:2660 loss:1.8524590730667114 \n",
      "iteration:2661 loss:1.8634021282196045 \n",
      "iteration:2662 loss:1.8812379837036133 \n",
      "iteration:2663 loss:1.8595350980758667 \n",
      "iteration:2664 loss:1.7824894189834595 \n",
      "iteration:2665 loss:1.8901007175445557 \n",
      "iteration:2666 loss:1.856886386871338 \n",
      "iteration:2667 loss:1.8404319286346436 \n",
      "iteration:2668 loss:1.8132836818695068 \n",
      "iteration:2669 loss:1.8563756942749023 \n",
      "iteration:2670 loss:1.8521897792816162 \n",
      "iteration:2671 loss:1.84821355342865 \n",
      "iteration:2672 loss:1.8044978380203247 \n",
      "iteration:2673 loss:1.8423389196395874 \n",
      "iteration:2674 loss:1.8194352388381958 \n",
      "iteration:2675 loss:1.8191477060317993 \n",
      "iteration:2676 loss:1.8164676427841187 \n",
      "iteration:2677 loss:1.866547703742981 \n",
      "iteration:2678 loss:1.8545629978179932 \n",
      "iteration:2679 loss:1.823459506034851 \n",
      "iteration:2680 loss:1.8536019325256348 \n",
      "iteration:2681 loss:1.8149678707122803 \n",
      "iteration:2682 loss:1.8226310014724731 \n",
      "iteration:2683 loss:1.867458701133728 \n",
      "iteration:2684 loss:1.829898476600647 \n",
      "iteration:2685 loss:1.8475713729858398 \n",
      "iteration:2686 loss:1.8587983846664429 \n",
      "iteration:2687 loss:1.8741904497146606 \n",
      "iteration:2688 loss:1.8551523685455322 \n",
      "iteration:2689 loss:1.8589272499084473 \n",
      "iteration:2690 loss:1.815816044807434 \n",
      "iteration:2691 loss:1.8156651258468628 \n",
      "iteration:2692 loss:1.8472424745559692 \n",
      "iteration:2693 loss:1.8659650087356567 \n",
      "iteration:2694 loss:1.8472096920013428 \n",
      "iteration:2695 loss:1.843519926071167 \n",
      "iteration:2696 loss:1.842983603477478 \n",
      "iteration:2697 loss:1.827947974205017 \n",
      "iteration:2698 loss:1.8215020895004272 \n",
      "iteration:2699 loss:1.8204057216644287 \n",
      "iteration:2700 loss:1.804021954536438 \n",
      "iteration:2701 loss:1.8426433801651 \n",
      "iteration:2702 loss:1.798632025718689 \n",
      "iteration:2703 loss:1.8094922304153442 \n",
      "iteration:2704 loss:1.823398232460022 \n",
      "iteration:2705 loss:1.8529738187789917 \n",
      "iteration:2706 loss:1.814195990562439 \n",
      "iteration:2707 loss:1.7770025730133057 \n",
      "iteration:2708 loss:1.8798269033432007 \n",
      "iteration:2709 loss:1.8454670906066895 \n",
      "iteration:2710 loss:1.8598359823226929 \n",
      "iteration:2711 loss:1.8336403369903564 \n",
      "iteration:2712 loss:1.7993825674057007 \n",
      "iteration:2713 loss:1.8497965335845947 \n",
      "iteration:2714 loss:1.7888209819793701 \n",
      "iteration:2715 loss:1.8604068756103516 \n",
      "iteration:2716 loss:1.827650785446167 \n",
      "iteration:2717 loss:1.7854076623916626 \n",
      "iteration:2718 loss:1.8591501712799072 \n",
      "iteration:2719 loss:1.8726369142532349 \n",
      "iteration:2720 loss:1.8907614946365356 \n",
      "iteration:2721 loss:1.861200213432312 \n",
      "iteration:2722 loss:1.8308857679367065 \n",
      "iteration:2723 loss:1.7940559387207031 \n",
      "iteration:2724 loss:1.837683081626892 \n",
      "iteration:2725 loss:1.832892894744873 \n",
      "iteration:2726 loss:1.7679616212844849 \n",
      "iteration:2727 loss:1.8702791929244995 \n",
      "iteration:2728 loss:1.848323941230774 \n",
      "iteration:2729 loss:1.8145251274108887 \n",
      "iteration:2730 loss:1.8088010549545288 \n",
      "iteration:2731 loss:1.8295812606811523 \n",
      "iteration:2732 loss:1.818463921546936 \n",
      "iteration:2733 loss:1.811740756034851 \n",
      "iteration:2734 loss:1.8042579889297485 \n",
      "iteration:2735 loss:1.8500386476516724 \n",
      "iteration:2736 loss:1.8499926328659058 \n",
      "iteration:2737 loss:1.866881251335144 \n",
      "iteration:2738 loss:1.8165804147720337 \n",
      "iteration:2739 loss:1.7894278764724731 \n",
      "iteration:2740 loss:1.8131331205368042 \n",
      "iteration:2741 loss:1.8487640619277954 \n",
      "iteration:2742 loss:1.8121861219406128 \n",
      "iteration:2743 loss:1.7717653512954712 \n",
      "iteration:2744 loss:1.8099051713943481 \n",
      "iteration:2745 loss:1.8495923280715942 \n",
      "iteration:2746 loss:1.7950948476791382 \n",
      "iteration:2747 loss:1.8156813383102417 \n",
      "iteration:2748 loss:1.85399329662323 \n",
      "iteration:2749 loss:1.8347383737564087 \n",
      "iteration:2750 loss:1.8481128215789795 \n",
      "iteration:2751 loss:1.8057607412338257 \n",
      "iteration:2752 loss:1.8236562013626099 \n",
      "iteration:2753 loss:1.8130171298980713 \n",
      "iteration:2754 loss:1.8713425397872925 \n",
      "Epoch-19 lr: 0.0006840622763423389\n",
      "iteration:2755 loss:1.879744291305542 \n",
      "iteration:2756 loss:1.884552240371704 \n",
      "iteration:2757 loss:1.74248468875885 \n",
      "iteration:2758 loss:1.840753436088562 \n",
      "iteration:2759 loss:1.8323897123336792 \n",
      "iteration:2760 loss:1.8197084665298462 \n",
      "iteration:2761 loss:1.8034484386444092 \n",
      "iteration:2762 loss:1.8382468223571777 \n",
      "iteration:2763 loss:1.8534669876098633 \n",
      "iteration:2764 loss:1.807030439376831 \n",
      "iteration:2765 loss:1.8430869579315186 \n",
      "iteration:2766 loss:1.8072395324707031 \n",
      "iteration:2767 loss:1.7840458154678345 \n",
      "iteration:2768 loss:1.8576115369796753 \n",
      "iteration:2769 loss:1.782667875289917 \n",
      "iteration:2770 loss:1.7859375476837158 \n",
      "iteration:2771 loss:1.8668023347854614 \n",
      "iteration:2772 loss:1.8401377201080322 \n",
      "iteration:2773 loss:1.809183120727539 \n",
      "iteration:2774 loss:1.8098093271255493 \n",
      "iteration:2775 loss:1.8130625486373901 \n",
      "iteration:2776 loss:1.7561976909637451 \n",
      "iteration:2777 loss:1.8108400106430054 \n",
      "iteration:2778 loss:1.8343926668167114 \n",
      "iteration:2779 loss:1.799470067024231 \n",
      "iteration:2780 loss:1.870765209197998 \n",
      "iteration:2781 loss:1.8032361268997192 \n",
      "iteration:2782 loss:1.8375595808029175 \n",
      "iteration:2783 loss:1.8215744495391846 \n",
      "iteration:2784 loss:1.8329951763153076 \n",
      "iteration:2785 loss:1.8168432712554932 \n",
      "iteration:2786 loss:1.8190940618515015 \n",
      "iteration:2787 loss:1.8257486820220947 \n",
      "iteration:2788 loss:1.7877413034439087 \n",
      "iteration:2789 loss:1.8015490770339966 \n",
      "iteration:2790 loss:1.7986009120941162 \n",
      "iteration:2791 loss:1.7933276891708374 \n",
      "iteration:2792 loss:1.8359979391098022 \n",
      "iteration:2793 loss:1.7951518297195435 \n",
      "iteration:2794 loss:1.7955708503723145 \n",
      "iteration:2795 loss:1.8354225158691406 \n",
      "iteration:2796 loss:1.8183070421218872 \n",
      "iteration:2797 loss:1.8353118896484375 \n",
      "iteration:2798 loss:1.7594726085662842 \n",
      "iteration:2799 loss:1.854777216911316 \n",
      "iteration:2800 loss:1.8267425298690796 \n",
      "iteration:2801 loss:1.8160667419433594 \n",
      "iteration:2802 loss:1.8080490827560425 \n",
      "iteration:2803 loss:1.7946606874465942 \n",
      "iteration:2804 loss:1.8223283290863037 \n",
      "iteration:2805 loss:1.8769943714141846 \n",
      "iteration:2806 loss:1.8230761289596558 \n",
      "iteration:2807 loss:1.8418172597885132 \n",
      "iteration:2808 loss:1.829409122467041 \n",
      "iteration:2809 loss:1.8302968740463257 \n",
      "iteration:2810 loss:1.8755841255187988 \n",
      "iteration:2811 loss:1.8341668844223022 \n",
      "iteration:2812 loss:1.8323335647583008 \n",
      "iteration:2813 loss:1.8373355865478516 \n",
      "iteration:2814 loss:1.8313848972320557 \n",
      "iteration:2815 loss:1.8205105066299438 \n",
      "iteration:2816 loss:1.8130302429199219 \n",
      "iteration:2817 loss:1.8050755262374878 \n",
      "iteration:2818 loss:1.829617977142334 \n",
      "iteration:2819 loss:1.7895749807357788 \n",
      "iteration:2820 loss:1.867147445678711 \n",
      "iteration:2821 loss:1.8340595960617065 \n",
      "iteration:2822 loss:1.8621793985366821 \n",
      "iteration:2823 loss:1.837446689605713 \n",
      "iteration:2824 loss:1.8311171531677246 \n",
      "iteration:2825 loss:1.8288482427597046 \n",
      "iteration:2826 loss:1.8273972272872925 \n",
      "iteration:2827 loss:1.784898042678833 \n",
      "iteration:2828 loss:1.8005625009536743 \n",
      "iteration:2829 loss:1.8326393365859985 \n",
      "iteration:2830 loss:1.8354750871658325 \n",
      "iteration:2831 loss:1.818220853805542 \n",
      "iteration:2832 loss:1.859491229057312 \n",
      "iteration:2833 loss:1.8015413284301758 \n",
      "iteration:2834 loss:1.817837119102478 \n",
      "iteration:2835 loss:1.8519502878189087 \n",
      "iteration:2836 loss:1.8234707117080688 \n",
      "iteration:2837 loss:1.8310978412628174 \n",
      "iteration:2838 loss:1.8337223529815674 \n",
      "iteration:2839 loss:1.8187140226364136 \n",
      "iteration:2840 loss:1.8020528554916382 \n",
      "iteration:2841 loss:1.837589144706726 \n",
      "iteration:2842 loss:1.8351985216140747 \n",
      "iteration:2843 loss:1.824427843093872 \n",
      "iteration:2844 loss:1.7801265716552734 \n",
      "iteration:2845 loss:1.8145173788070679 \n",
      "iteration:2846 loss:1.826636791229248 \n",
      "iteration:2847 loss:1.8251415491104126 \n",
      "iteration:2848 loss:1.890367031097412 \n",
      "iteration:2849 loss:1.7539341449737549 \n",
      "iteration:2850 loss:1.8124219179153442 \n",
      "iteration:2851 loss:1.829338550567627 \n",
      "iteration:2852 loss:1.8443448543548584 \n",
      "iteration:2853 loss:1.8220882415771484 \n",
      "iteration:2854 loss:1.8076940774917603 \n",
      "iteration:2855 loss:1.8700426816940308 \n",
      "iteration:2856 loss:1.8543318510055542 \n",
      "iteration:2857 loss:1.8071985244750977 \n",
      "iteration:2858 loss:1.7699066400527954 \n",
      "iteration:2859 loss:1.8228843212127686 \n",
      "iteration:2860 loss:1.8076307773590088 \n",
      "iteration:2861 loss:1.8185505867004395 \n",
      "iteration:2862 loss:1.8113987445831299 \n",
      "iteration:2863 loss:1.8713514804840088 \n",
      "iteration:2864 loss:1.7874788045883179 \n",
      "iteration:2865 loss:1.8036384582519531 \n",
      "iteration:2866 loss:1.8041479587554932 \n",
      "iteration:2867 loss:1.844696044921875 \n",
      "iteration:2868 loss:1.8197556734085083 \n",
      "iteration:2869 loss:1.8600091934204102 \n",
      "iteration:2870 loss:1.785730242729187 \n",
      "iteration:2871 loss:1.820613145828247 \n",
      "iteration:2872 loss:1.846360683441162 \n",
      "iteration:2873 loss:1.8051763772964478 \n",
      "iteration:2874 loss:1.800716519355774 \n",
      "iteration:2875 loss:1.8240402936935425 \n",
      "iteration:2876 loss:1.8056782484054565 \n",
      "iteration:2877 loss:1.8362243175506592 \n",
      "iteration:2878 loss:1.8184889554977417 \n",
      "iteration:2879 loss:1.8211324214935303 \n",
      "iteration:2880 loss:1.8433383703231812 \n",
      "iteration:2881 loss:1.809606671333313 \n",
      "iteration:2882 loss:1.7977380752563477 \n",
      "iteration:2883 loss:1.83600914478302 \n",
      "iteration:2884 loss:1.8201332092285156 \n",
      "iteration:2885 loss:1.8464620113372803 \n",
      "iteration:2886 loss:1.7589330673217773 \n",
      "iteration:2887 loss:1.83795166015625 \n",
      "iteration:2888 loss:1.8085280656814575 \n",
      "iteration:2889 loss:1.790601372718811 \n",
      "iteration:2890 loss:1.847171425819397 \n",
      "iteration:2891 loss:1.840254545211792 \n",
      "iteration:2892 loss:1.8006844520568848 \n",
      "iteration:2893 loss:1.865278720855713 \n",
      "iteration:2894 loss:1.764577031135559 \n",
      "iteration:2895 loss:1.8275811672210693 \n",
      "iteration:2896 loss:1.8047611713409424 \n",
      "iteration:2897 loss:1.7962802648544312 \n",
      "iteration:2898 loss:1.8178887367248535 \n",
      "iteration:2899 loss:1.941727876663208 \n",
      "Epoch-20 lr: 0.0006545084971874735\n",
      "iteration:2900 loss:1.8479061126708984 \n",
      "iteration:2901 loss:1.7770814895629883 \n",
      "iteration:2902 loss:1.774845838546753 \n",
      "iteration:2903 loss:1.8315974473953247 \n",
      "iteration:2904 loss:1.8453176021575928 \n",
      "iteration:2905 loss:1.7410343885421753 \n",
      "iteration:2906 loss:1.8121620416641235 \n",
      "iteration:2907 loss:1.8208829164505005 \n",
      "iteration:2908 loss:1.7839139699935913 \n",
      "iteration:2909 loss:1.8442022800445557 \n",
      "iteration:2910 loss:1.7726008892059326 \n",
      "iteration:2911 loss:1.8373050689697266 \n",
      "iteration:2912 loss:1.8590128421783447 \n",
      "iteration:2913 loss:1.7999480962753296 \n",
      "iteration:2914 loss:1.7695749998092651 \n",
      "iteration:2915 loss:1.8080418109893799 \n",
      "iteration:2916 loss:1.800803780555725 \n",
      "iteration:2917 loss:1.7895591259002686 \n",
      "iteration:2918 loss:1.8650065660476685 \n",
      "iteration:2919 loss:1.7992600202560425 \n",
      "iteration:2920 loss:1.8366597890853882 \n",
      "iteration:2921 loss:1.8303041458129883 \n",
      "iteration:2922 loss:1.811693549156189 \n",
      "iteration:2923 loss:1.814245343208313 \n",
      "iteration:2924 loss:1.7878645658493042 \n",
      "iteration:2925 loss:1.8666085004806519 \n",
      "iteration:2926 loss:1.8268640041351318 \n",
      "iteration:2927 loss:1.8208496570587158 \n",
      "iteration:2928 loss:1.8095000982284546 \n",
      "iteration:2929 loss:1.818657398223877 \n",
      "iteration:2930 loss:1.7774473428726196 \n",
      "iteration:2931 loss:1.7952665090560913 \n",
      "iteration:2932 loss:1.7548505067825317 \n",
      "iteration:2933 loss:1.8341144323349 \n",
      "iteration:2934 loss:1.847510576248169 \n",
      "iteration:2935 loss:1.8053885698318481 \n",
      "iteration:2936 loss:1.7789274454116821 \n",
      "iteration:2937 loss:1.8038427829742432 \n",
      "iteration:2938 loss:1.827697992324829 \n",
      "iteration:2939 loss:1.8058679103851318 \n",
      "iteration:2940 loss:1.846280574798584 \n",
      "iteration:2941 loss:1.8344767093658447 \n",
      "iteration:2942 loss:1.8095078468322754 \n",
      "iteration:2943 loss:1.8064109086990356 \n",
      "iteration:2944 loss:1.8150784969329834 \n",
      "iteration:2945 loss:1.8277884721755981 \n",
      "iteration:2946 loss:1.8026759624481201 \n",
      "iteration:2947 loss:1.840941309928894 \n",
      "iteration:2948 loss:1.8558992147445679 \n",
      "iteration:2949 loss:1.830689549446106 \n",
      "iteration:2950 loss:1.8194591999053955 \n",
      "iteration:2951 loss:1.791804552078247 \n",
      "iteration:2952 loss:1.8015632629394531 \n",
      "iteration:2953 loss:1.7992181777954102 \n",
      "iteration:2954 loss:1.8162217140197754 \n",
      "iteration:2955 loss:1.7759790420532227 \n",
      "iteration:2956 loss:1.8247594833374023 \n",
      "iteration:2957 loss:1.7990795373916626 \n",
      "iteration:2958 loss:1.7912101745605469 \n",
      "iteration:2959 loss:1.8583040237426758 \n",
      "iteration:2960 loss:1.7938261032104492 \n",
      "iteration:2961 loss:1.7889554500579834 \n",
      "iteration:2962 loss:1.8396047353744507 \n",
      "iteration:2963 loss:1.8272091150283813 \n",
      "iteration:2964 loss:1.8175705671310425 \n",
      "iteration:2965 loss:1.8305212259292603 \n",
      "iteration:2966 loss:1.7819453477859497 \n",
      "iteration:2967 loss:1.791778802871704 \n",
      "iteration:2968 loss:1.8223036527633667 \n",
      "iteration:2969 loss:1.7816740274429321 \n",
      "iteration:2970 loss:1.816564917564392 \n",
      "iteration:2971 loss:1.8249865770339966 \n",
      "iteration:2972 loss:1.8101409673690796 \n",
      "iteration:2973 loss:1.8017338514328003 \n",
      "iteration:2974 loss:1.8406158685684204 \n",
      "iteration:2975 loss:1.8208603858947754 \n",
      "iteration:2976 loss:1.8546409606933594 \n",
      "iteration:2977 loss:1.7959085702896118 \n",
      "iteration:2978 loss:1.7704282999038696 \n",
      "iteration:2979 loss:1.8293548822402954 \n",
      "iteration:2980 loss:1.8062965869903564 \n",
      "iteration:2981 loss:1.8106567859649658 \n",
      "iteration:2982 loss:1.8366835117340088 \n",
      "iteration:2983 loss:1.8125805854797363 \n",
      "iteration:2984 loss:1.8347455263137817 \n",
      "iteration:2985 loss:1.8219490051269531 \n",
      "iteration:2986 loss:1.8268084526062012 \n",
      "iteration:2987 loss:1.8263204097747803 \n",
      "iteration:2988 loss:1.7878981828689575 \n",
      "iteration:2989 loss:1.8548051118850708 \n",
      "iteration:2990 loss:1.810275912284851 \n",
      "iteration:2991 loss:1.784816861152649 \n",
      "iteration:2992 loss:1.8070729970932007 \n",
      "iteration:2993 loss:1.7978214025497437 \n",
      "iteration:2994 loss:1.78600013256073 \n",
      "iteration:2995 loss:1.8224087953567505 \n",
      "iteration:2996 loss:1.8025535345077515 \n",
      "iteration:2997 loss:1.8116613626480103 \n",
      "iteration:2998 loss:1.7972089052200317 \n",
      "iteration:2999 loss:1.8282753229141235 \n",
      "iteration:3000 loss:1.8191674947738647 \n",
      "iteration:3001 loss:1.8228741884231567 \n",
      "iteration:3002 loss:1.7866593599319458 \n",
      "iteration:3003 loss:1.8296681642532349 \n",
      "iteration:3004 loss:1.8388315439224243 \n",
      "iteration:3005 loss:1.788793683052063 \n",
      "iteration:3006 loss:1.769711971282959 \n",
      "iteration:3007 loss:1.7869682312011719 \n",
      "iteration:3008 loss:1.77545166015625 \n",
      "iteration:3009 loss:1.8053064346313477 \n",
      "iteration:3010 loss:1.8298966884613037 \n",
      "iteration:3011 loss:1.7832610607147217 \n",
      "iteration:3012 loss:1.8119025230407715 \n",
      "iteration:3013 loss:1.8030041456222534 \n",
      "iteration:3014 loss:1.8402231931686401 \n",
      "iteration:3015 loss:1.789063572883606 \n",
      "iteration:3016 loss:1.807214379310608 \n",
      "iteration:3017 loss:1.8287758827209473 \n",
      "iteration:3018 loss:1.828821063041687 \n",
      "iteration:3019 loss:1.831068515777588 \n",
      "iteration:3020 loss:1.795446515083313 \n",
      "iteration:3021 loss:1.7914679050445557 \n",
      "iteration:3022 loss:1.8354558944702148 \n",
      "iteration:3023 loss:1.7993483543395996 \n",
      "iteration:3024 loss:1.7998191118240356 \n",
      "iteration:3025 loss:1.8489774465560913 \n",
      "iteration:3026 loss:1.8491711616516113 \n",
      "iteration:3027 loss:1.7964627742767334 \n",
      "iteration:3028 loss:1.7841275930404663 \n",
      "iteration:3029 loss:1.821860909461975 \n",
      "iteration:3030 loss:1.807613492012024 \n",
      "iteration:3031 loss:1.8003122806549072 \n",
      "iteration:3032 loss:1.8630180358886719 \n",
      "iteration:3033 loss:1.8004437685012817 \n",
      "iteration:3034 loss:1.7945741415023804 \n",
      "iteration:3035 loss:1.7973684072494507 \n",
      "iteration:3036 loss:1.825138807296753 \n",
      "iteration:3037 loss:1.810912847518921 \n",
      "iteration:3038 loss:1.8053096532821655 \n",
      "iteration:3039 loss:1.8457465171813965 \n",
      "iteration:3040 loss:1.8234249353408813 \n",
      "iteration:3041 loss:1.8041871786117554 \n",
      "iteration:3042 loss:1.7924177646636963 \n",
      "iteration:3043 loss:1.7892922163009644 \n",
      "iteration:3044 loss:2.0117461681365967 \n",
      "Epoch-21 lr: 0.0006243449435824271\n",
      "iteration:3045 loss:1.7797465324401855 \n",
      "iteration:3046 loss:1.786981463432312 \n",
      "iteration:3047 loss:1.875702977180481 \n",
      "iteration:3048 loss:1.849940538406372 \n",
      "iteration:3049 loss:1.8266245126724243 \n",
      "iteration:3050 loss:1.8437837362289429 \n",
      "iteration:3051 loss:1.846848964691162 \n",
      "iteration:3052 loss:1.826691746711731 \n",
      "iteration:3053 loss:1.837043285369873 \n",
      "iteration:3054 loss:1.8189032077789307 \n",
      "iteration:3055 loss:1.8249361515045166 \n",
      "iteration:3056 loss:1.8125032186508179 \n",
      "iteration:3057 loss:1.779236078262329 \n",
      "iteration:3058 loss:1.8585233688354492 \n",
      "iteration:3059 loss:1.8335556983947754 \n",
      "iteration:3060 loss:1.8165982961654663 \n",
      "iteration:3061 loss:1.8276522159576416 \n",
      "iteration:3062 loss:1.8718528747558594 \n",
      "iteration:3063 loss:1.8183268308639526 \n",
      "iteration:3064 loss:1.8044946193695068 \n",
      "iteration:3065 loss:1.8370695114135742 \n",
      "iteration:3066 loss:1.816457748413086 \n",
      "iteration:3067 loss:1.8099684715270996 \n",
      "iteration:3068 loss:1.783006191253662 \n",
      "iteration:3069 loss:1.8231534957885742 \n",
      "iteration:3070 loss:1.873571753501892 \n",
      "iteration:3071 loss:1.8070638179779053 \n",
      "iteration:3072 loss:1.788794994354248 \n",
      "iteration:3073 loss:1.825292706489563 \n",
      "iteration:3074 loss:1.8348970413208008 \n",
      "iteration:3075 loss:1.8548678159713745 \n",
      "iteration:3076 loss:1.8012927770614624 \n",
      "iteration:3077 loss:1.8079525232315063 \n",
      "iteration:3078 loss:1.8362300395965576 \n",
      "iteration:3079 loss:1.8709478378295898 \n",
      "iteration:3080 loss:1.8004634380340576 \n",
      "iteration:3081 loss:1.8493198156356812 \n",
      "iteration:3082 loss:1.7866251468658447 \n",
      "iteration:3083 loss:1.8369218111038208 \n",
      "iteration:3084 loss:1.8810442686080933 \n",
      "iteration:3085 loss:1.7581793069839478 \n",
      "iteration:3086 loss:1.8213608264923096 \n",
      "iteration:3087 loss:1.8164935111999512 \n",
      "iteration:3088 loss:1.8310863971710205 \n",
      "iteration:3089 loss:1.760833501815796 \n",
      "iteration:3090 loss:1.783829927444458 \n",
      "iteration:3091 loss:1.8586950302124023 \n",
      "iteration:3092 loss:1.801414966583252 \n",
      "iteration:3093 loss:1.7873647212982178 \n",
      "iteration:3094 loss:1.8056424856185913 \n",
      "iteration:3095 loss:1.8022749423980713 \n",
      "iteration:3096 loss:1.8205076456069946 \n",
      "iteration:3097 loss:1.7837700843811035 \n",
      "iteration:3098 loss:1.8112131357192993 \n",
      "iteration:3099 loss:1.8321890830993652 \n",
      "iteration:3100 loss:1.8297147750854492 \n",
      "iteration:3101 loss:1.8240500688552856 \n",
      "iteration:3102 loss:1.775071620941162 \n",
      "iteration:3103 loss:1.8041330575942993 \n",
      "iteration:3104 loss:1.800892949104309 \n",
      "iteration:3105 loss:1.7736021280288696 \n",
      "iteration:3106 loss:1.8135206699371338 \n",
      "iteration:3107 loss:1.8278650045394897 \n",
      "iteration:3108 loss:1.8101941347122192 \n",
      "iteration:3109 loss:1.7542065382003784 \n",
      "iteration:3110 loss:1.8517539501190186 \n",
      "iteration:3111 loss:1.825265884399414 \n",
      "iteration:3112 loss:1.8120853900909424 \n",
      "iteration:3113 loss:1.8542413711547852 \n",
      "iteration:3114 loss:1.8320436477661133 \n",
      "iteration:3115 loss:1.7563236951828003 \n",
      "iteration:3116 loss:1.7880216836929321 \n",
      "iteration:3117 loss:1.8026739358901978 \n",
      "iteration:3118 loss:1.7450768947601318 \n",
      "iteration:3119 loss:1.792653203010559 \n",
      "iteration:3120 loss:1.8462210893630981 \n",
      "iteration:3121 loss:1.8123770952224731 \n",
      "iteration:3122 loss:1.842051386833191 \n",
      "iteration:3123 loss:1.8217535018920898 \n",
      "iteration:3124 loss:1.7950537204742432 \n",
      "iteration:3125 loss:1.846035361289978 \n",
      "iteration:3126 loss:1.7655696868896484 \n",
      "iteration:3127 loss:1.8013403415679932 \n",
      "iteration:3128 loss:1.7450892925262451 \n",
      "iteration:3129 loss:1.7565380334854126 \n",
      "iteration:3130 loss:1.7843778133392334 \n",
      "iteration:3131 loss:1.777860164642334 \n",
      "iteration:3132 loss:1.800086498260498 \n",
      "iteration:3133 loss:1.7715929746627808 \n",
      "iteration:3134 loss:1.8257602453231812 \n",
      "iteration:3135 loss:1.789247989654541 \n",
      "iteration:3136 loss:1.7821643352508545 \n",
      "iteration:3137 loss:1.8032658100128174 \n",
      "iteration:3138 loss:1.7669252157211304 \n",
      "iteration:3139 loss:1.8302431106567383 \n",
      "iteration:3140 loss:1.8272515535354614 \n",
      "iteration:3141 loss:1.813845157623291 \n",
      "iteration:3142 loss:1.784657597541809 \n",
      "iteration:3143 loss:1.7945504188537598 \n",
      "iteration:3144 loss:1.8117992877960205 \n",
      "iteration:3145 loss:1.8121695518493652 \n",
      "iteration:3146 loss:1.7923051118850708 \n",
      "iteration:3147 loss:1.808769941329956 \n",
      "iteration:3148 loss:1.7841227054595947 \n",
      "iteration:3149 loss:1.7826924324035645 \n",
      "iteration:3150 loss:1.78998863697052 \n",
      "iteration:3151 loss:1.8008532524108887 \n",
      "iteration:3152 loss:1.823354721069336 \n",
      "iteration:3153 loss:1.848393201828003 \n",
      "iteration:3154 loss:1.8131990432739258 \n",
      "iteration:3155 loss:1.8224650621414185 \n",
      "iteration:3156 loss:1.8520013093948364 \n",
      "iteration:3157 loss:1.843190312385559 \n",
      "iteration:3158 loss:1.7942354679107666 \n",
      "iteration:3159 loss:1.7979174852371216 \n",
      "iteration:3160 loss:1.7991762161254883 \n",
      "iteration:3161 loss:1.796319603919983 \n",
      "iteration:3162 loss:1.8265460729599 \n",
      "iteration:3163 loss:1.8693413734436035 \n",
      "iteration:3164 loss:1.8117997646331787 \n",
      "iteration:3165 loss:1.764862298965454 \n",
      "iteration:3166 loss:1.7831296920776367 \n",
      "iteration:3167 loss:1.8493727445602417 \n",
      "iteration:3168 loss:1.8093013763427734 \n",
      "iteration:3169 loss:1.8190563917160034 \n",
      "iteration:3170 loss:1.857163667678833 \n",
      "iteration:3171 loss:1.828829288482666 \n",
      "iteration:3172 loss:1.821405053138733 \n",
      "iteration:3173 loss:1.8226128816604614 \n",
      "iteration:3174 loss:1.8284053802490234 \n",
      "iteration:3175 loss:1.820265293121338 \n",
      "iteration:3176 loss:1.8046754598617554 \n",
      "iteration:3177 loss:1.8428226709365845 \n",
      "iteration:3178 loss:1.780029535293579 \n",
      "iteration:3179 loss:1.8017736673355103 \n",
      "iteration:3180 loss:1.8212164640426636 \n",
      "iteration:3181 loss:1.7946394681930542 \n",
      "iteration:3182 loss:1.7666913270950317 \n",
      "iteration:3183 loss:1.8149727582931519 \n",
      "iteration:3184 loss:1.8341326713562012 \n",
      "iteration:3185 loss:1.7876880168914795 \n",
      "iteration:3186 loss:1.8004581928253174 \n",
      "iteration:3187 loss:1.7977867126464844 \n",
      "iteration:3188 loss:1.772010087966919 \n",
      "iteration:3189 loss:1.8423571586608887 \n",
      "Epoch-22 lr: 0.0005936906572928622\n",
      "iteration:3190 loss:1.8141191005706787 \n",
      "iteration:3191 loss:1.8023868799209595 \n",
      "iteration:3192 loss:1.7328755855560303 \n",
      "iteration:3193 loss:1.809403896331787 \n",
      "iteration:3194 loss:1.7992689609527588 \n",
      "iteration:3195 loss:1.7474218606948853 \n",
      "iteration:3196 loss:1.8312493562698364 \n",
      "iteration:3197 loss:1.8076422214508057 \n",
      "iteration:3198 loss:1.847360610961914 \n",
      "iteration:3199 loss:1.8022432327270508 \n",
      "iteration:3200 loss:1.8531652688980103 \n",
      "iteration:3201 loss:1.7837570905685425 \n",
      "iteration:3202 loss:1.8521944284439087 \n",
      "iteration:3203 loss:1.8236604928970337 \n",
      "iteration:3204 loss:1.7956727743148804 \n",
      "iteration:3205 loss:1.7842882871627808 \n",
      "iteration:3206 loss:1.8191332817077637 \n",
      "iteration:3207 loss:1.7798852920532227 \n",
      "iteration:3208 loss:1.8330518007278442 \n",
      "iteration:3209 loss:1.8062504529953003 \n",
      "iteration:3210 loss:1.7632462978363037 \n",
      "iteration:3211 loss:1.7894879579544067 \n",
      "iteration:3212 loss:1.784142017364502 \n",
      "iteration:3213 loss:1.8123899698257446 \n",
      "iteration:3214 loss:1.8041408061981201 \n",
      "iteration:3215 loss:1.759761929512024 \n",
      "iteration:3216 loss:1.812501072883606 \n",
      "iteration:3217 loss:1.8070898056030273 \n",
      "iteration:3218 loss:1.749979853630066 \n",
      "iteration:3219 loss:1.825993299484253 \n",
      "iteration:3220 loss:1.7956249713897705 \n",
      "iteration:3221 loss:1.8048415184020996 \n",
      "iteration:3222 loss:1.7882696390151978 \n",
      "iteration:3223 loss:1.8026295900344849 \n",
      "iteration:3224 loss:1.7814161777496338 \n",
      "iteration:3225 loss:1.8706339597702026 \n",
      "iteration:3226 loss:1.81804621219635 \n",
      "iteration:3227 loss:1.7369848489761353 \n",
      "iteration:3228 loss:1.7660878896713257 \n",
      "iteration:3229 loss:1.7919156551361084 \n",
      "iteration:3230 loss:1.7850488424301147 \n",
      "iteration:3231 loss:1.735184907913208 \n",
      "iteration:3232 loss:1.8193172216415405 \n",
      "iteration:3233 loss:1.732344388961792 \n",
      "iteration:3234 loss:1.8538357019424438 \n",
      "iteration:3235 loss:1.7903995513916016 \n",
      "iteration:3236 loss:1.7720210552215576 \n",
      "iteration:3237 loss:1.7912532091140747 \n",
      "iteration:3238 loss:1.8057688474655151 \n",
      "iteration:3239 loss:1.8247119188308716 \n",
      "iteration:3240 loss:1.8478479385375977 \n",
      "iteration:3241 loss:1.8063311576843262 \n",
      "iteration:3242 loss:1.8044030666351318 \n",
      "iteration:3243 loss:1.7871626615524292 \n",
      "iteration:3244 loss:1.8562101125717163 \n",
      "iteration:3245 loss:1.795436978340149 \n",
      "iteration:3246 loss:1.8053778409957886 \n",
      "iteration:3247 loss:1.7882094383239746 \n",
      "iteration:3248 loss:1.8236572742462158 \n",
      "iteration:3249 loss:1.7892619371414185 \n",
      "iteration:3250 loss:1.7824585437774658 \n",
      "iteration:3251 loss:1.8545581102371216 \n",
      "iteration:3252 loss:1.7660366296768188 \n",
      "iteration:3253 loss:1.8316679000854492 \n",
      "iteration:3254 loss:1.832685947418213 \n",
      "iteration:3255 loss:1.7853468656539917 \n",
      "iteration:3256 loss:1.8167445659637451 \n",
      "iteration:3257 loss:1.8132646083831787 \n",
      "iteration:3258 loss:1.8144264221191406 \n",
      "iteration:3259 loss:1.7936344146728516 \n",
      "iteration:3260 loss:1.7985076904296875 \n",
      "iteration:3261 loss:1.8027361631393433 \n",
      "iteration:3262 loss:1.788295865058899 \n",
      "iteration:3263 loss:1.7838019132614136 \n",
      "iteration:3264 loss:1.7811684608459473 \n",
      "iteration:3265 loss:1.812470555305481 \n",
      "iteration:3266 loss:1.8167223930358887 \n",
      "iteration:3267 loss:1.7449562549591064 \n",
      "iteration:3268 loss:1.8357270956039429 \n",
      "iteration:3269 loss:1.750335454940796 \n",
      "iteration:3270 loss:1.8081889152526855 \n",
      "iteration:3271 loss:1.8308039903640747 \n",
      "iteration:3272 loss:1.7906748056411743 \n",
      "iteration:3273 loss:1.8303049802780151 \n",
      "iteration:3274 loss:1.8794103860855103 \n",
      "iteration:3275 loss:1.7581194639205933 \n",
      "iteration:3276 loss:1.7997900247573853 \n",
      "iteration:3277 loss:1.8537365198135376 \n",
      "iteration:3278 loss:1.8076025247573853 \n",
      "iteration:3279 loss:1.796535611152649 \n",
      "iteration:3280 loss:1.80876886844635 \n",
      "iteration:3281 loss:1.8251489400863647 \n",
      "iteration:3282 loss:1.7934950590133667 \n",
      "iteration:3283 loss:1.7755104303359985 \n",
      "iteration:3284 loss:1.8136029243469238 \n",
      "iteration:3285 loss:1.7993803024291992 \n",
      "iteration:3286 loss:1.7588419914245605 \n",
      "iteration:3287 loss:1.8021036386489868 \n",
      "iteration:3288 loss:1.8386287689208984 \n",
      "iteration:3289 loss:1.818461537361145 \n",
      "iteration:3290 loss:1.7995479106903076 \n",
      "iteration:3291 loss:1.7888975143432617 \n",
      "iteration:3292 loss:1.7952561378479004 \n",
      "iteration:3293 loss:1.804893136024475 \n",
      "iteration:3294 loss:1.8135573863983154 \n",
      "iteration:3295 loss:1.7984999418258667 \n",
      "iteration:3296 loss:1.8475335836410522 \n",
      "iteration:3297 loss:1.7887691259384155 \n",
      "iteration:3298 loss:1.7776926755905151 \n",
      "iteration:3299 loss:1.7763015031814575 \n",
      "iteration:3300 loss:1.796365737915039 \n",
      "iteration:3301 loss:1.8025081157684326 \n",
      "iteration:3302 loss:1.7657641172409058 \n",
      "iteration:3303 loss:1.733373761177063 \n",
      "iteration:3304 loss:1.8007309436798096 \n",
      "iteration:3305 loss:1.8057222366333008 \n",
      "iteration:3306 loss:1.8078837394714355 \n",
      "iteration:3307 loss:1.7527192831039429 \n",
      "iteration:3308 loss:1.782456874847412 \n",
      "iteration:3309 loss:1.8157293796539307 \n",
      "iteration:3310 loss:1.7916306257247925 \n",
      "iteration:3311 loss:1.7940642833709717 \n",
      "iteration:3312 loss:1.8124536275863647 \n",
      "iteration:3313 loss:1.7992101907730103 \n",
      "iteration:3314 loss:1.8067822456359863 \n",
      "iteration:3315 loss:1.8018418550491333 \n",
      "iteration:3316 loss:1.8222947120666504 \n",
      "iteration:3317 loss:1.8196152448654175 \n",
      "iteration:3318 loss:1.810425877571106 \n",
      "iteration:3319 loss:1.783659815788269 \n",
      "iteration:3320 loss:1.8869819641113281 \n",
      "iteration:3321 loss:1.7633278369903564 \n",
      "iteration:3322 loss:1.7958391904830933 \n",
      "iteration:3323 loss:1.829728364944458 \n",
      "iteration:3324 loss:1.7774019241333008 \n",
      "iteration:3325 loss:1.823462963104248 \n",
      "iteration:3326 loss:1.7764230966567993 \n",
      "iteration:3327 loss:1.7789760828018188 \n",
      "iteration:3328 loss:1.816094994544983 \n",
      "iteration:3329 loss:1.7893904447555542 \n",
      "iteration:3330 loss:1.814087152481079 \n",
      "iteration:3331 loss:1.7951552867889404 \n",
      "iteration:3332 loss:1.7702549695968628 \n",
      "iteration:3333 loss:1.766026258468628 \n",
      "iteration:3334 loss:1.7303802967071533 \n",
      "Epoch-23 lr: 0.000562666616782152\n",
      "iteration:3335 loss:1.7929491996765137 \n",
      "iteration:3336 loss:1.7754337787628174 \n",
      "iteration:3337 loss:1.8009623289108276 \n",
      "iteration:3338 loss:1.8163893222808838 \n",
      "iteration:3339 loss:1.821860909461975 \n",
      "iteration:3340 loss:1.7682956457138062 \n",
      "iteration:3341 loss:1.8074918985366821 \n",
      "iteration:3342 loss:1.7682678699493408 \n",
      "iteration:3343 loss:1.7816803455352783 \n",
      "iteration:3344 loss:1.7943742275238037 \n",
      "iteration:3345 loss:1.7974752187728882 \n",
      "iteration:3346 loss:1.8047949075698853 \n",
      "iteration:3347 loss:1.7771384716033936 \n",
      "iteration:3348 loss:1.793860912322998 \n",
      "iteration:3349 loss:1.7662410736083984 \n",
      "iteration:3350 loss:1.7612398862838745 \n",
      "iteration:3351 loss:1.7928720712661743 \n",
      "iteration:3352 loss:1.8142549991607666 \n",
      "iteration:3353 loss:1.7790690660476685 \n",
      "iteration:3354 loss:1.7577508687973022 \n",
      "iteration:3355 loss:1.7836997509002686 \n",
      "iteration:3356 loss:1.7563356161117554 \n",
      "iteration:3357 loss:1.7849324941635132 \n",
      "iteration:3358 loss:1.8111802339553833 \n",
      "iteration:3359 loss:1.7771186828613281 \n",
      "iteration:3360 loss:1.812590479850769 \n",
      "iteration:3361 loss:1.7599703073501587 \n",
      "iteration:3362 loss:1.7587214708328247 \n",
      "iteration:3363 loss:1.8051749467849731 \n",
      "iteration:3364 loss:1.7636464834213257 \n",
      "iteration:3365 loss:1.7735337018966675 \n",
      "iteration:3366 loss:1.7834279537200928 \n",
      "iteration:3367 loss:1.795846939086914 \n",
      "iteration:3368 loss:1.7664670944213867 \n",
      "iteration:3369 loss:1.753074288368225 \n",
      "iteration:3370 loss:1.7696948051452637 \n",
      "iteration:3371 loss:1.7763628959655762 \n",
      "iteration:3372 loss:1.8432297706604004 \n",
      "iteration:3373 loss:1.736687183380127 \n",
      "iteration:3374 loss:1.78947913646698 \n",
      "iteration:3375 loss:1.778158187866211 \n",
      "iteration:3376 loss:1.863000512123108 \n",
      "iteration:3377 loss:1.8069225549697876 \n",
      "iteration:3378 loss:1.7832471132278442 \n",
      "iteration:3379 loss:1.7606031894683838 \n",
      "iteration:3380 loss:1.8191064596176147 \n",
      "iteration:3381 loss:1.8163682222366333 \n",
      "iteration:3382 loss:1.7570971250534058 \n",
      "iteration:3383 loss:1.76691472530365 \n",
      "iteration:3384 loss:1.8148443698883057 \n",
      "iteration:3385 loss:1.8097044229507446 \n",
      "iteration:3386 loss:1.8246879577636719 \n",
      "iteration:3387 loss:1.8126108646392822 \n",
      "iteration:3388 loss:1.7839668989181519 \n",
      "iteration:3389 loss:1.7906094789505005 \n",
      "iteration:3390 loss:1.7949109077453613 \n",
      "iteration:3391 loss:1.8067034482955933 \n",
      "iteration:3392 loss:1.7835336923599243 \n",
      "iteration:3393 loss:1.799046277999878 \n",
      "iteration:3394 loss:1.7585910558700562 \n",
      "iteration:3395 loss:1.7934170961380005 \n",
      "iteration:3396 loss:1.8089463710784912 \n",
      "iteration:3397 loss:1.8509351015090942 \n",
      "iteration:3398 loss:1.794676661491394 \n",
      "iteration:3399 loss:1.7995412349700928 \n",
      "iteration:3400 loss:1.8199869394302368 \n",
      "iteration:3401 loss:1.7641526460647583 \n",
      "iteration:3402 loss:1.793215274810791 \n",
      "iteration:3403 loss:1.788224458694458 \n",
      "iteration:3404 loss:1.7946596145629883 \n",
      "iteration:3405 loss:1.8240875005722046 \n",
      "iteration:3406 loss:1.812899112701416 \n",
      "iteration:3407 loss:1.8146693706512451 \n",
      "iteration:3408 loss:1.8222930431365967 \n",
      "iteration:3409 loss:1.8090057373046875 \n",
      "iteration:3410 loss:1.8467597961425781 \n",
      "iteration:3411 loss:1.7919158935546875 \n",
      "iteration:3412 loss:1.7398016452789307 \n",
      "iteration:3413 loss:1.7913824319839478 \n",
      "iteration:3414 loss:1.7778055667877197 \n",
      "iteration:3415 loss:1.8119926452636719 \n",
      "iteration:3416 loss:1.755124568939209 \n",
      "iteration:3417 loss:1.8009169101715088 \n",
      "iteration:3418 loss:1.7787256240844727 \n",
      "iteration:3419 loss:1.7803548574447632 \n",
      "iteration:3420 loss:1.7718021869659424 \n",
      "iteration:3421 loss:1.7666436433792114 \n",
      "iteration:3422 loss:1.8538835048675537 \n",
      "iteration:3423 loss:1.7318564653396606 \n",
      "iteration:3424 loss:1.7495723962783813 \n",
      "iteration:3425 loss:1.810081958770752 \n",
      "iteration:3426 loss:1.797635555267334 \n",
      "iteration:3427 loss:1.8076064586639404 \n",
      "iteration:3428 loss:1.7999436855316162 \n",
      "iteration:3429 loss:1.7945533990859985 \n",
      "iteration:3430 loss:1.7759402990341187 \n",
      "iteration:3431 loss:1.7743406295776367 \n",
      "iteration:3432 loss:1.7598785161972046 \n",
      "iteration:3433 loss:1.752416968345642 \n",
      "iteration:3434 loss:1.8559311628341675 \n",
      "iteration:3435 loss:1.8101047277450562 \n",
      "iteration:3436 loss:1.7546063661575317 \n",
      "iteration:3437 loss:1.8097589015960693 \n",
      "iteration:3438 loss:1.825898289680481 \n",
      "iteration:3439 loss:1.7779937982559204 \n",
      "iteration:3440 loss:1.775519847869873 \n",
      "iteration:3441 loss:1.7571213245391846 \n",
      "iteration:3442 loss:1.7434505224227905 \n",
      "iteration:3443 loss:1.7905783653259277 \n",
      "iteration:3444 loss:1.7913093566894531 \n",
      "iteration:3445 loss:1.8091524839401245 \n",
      "iteration:3446 loss:1.853549838066101 \n",
      "iteration:3447 loss:1.7750428915023804 \n",
      "iteration:3448 loss:1.7787014245986938 \n",
      "iteration:3449 loss:1.8271960020065308 \n",
      "iteration:3450 loss:1.8032057285308838 \n",
      "iteration:3451 loss:1.8557045459747314 \n",
      "iteration:3452 loss:1.7600451707839966 \n",
      "iteration:3453 loss:1.7789217233657837 \n",
      "iteration:3454 loss:1.7565462589263916 \n",
      "iteration:3455 loss:1.7609612941741943 \n",
      "iteration:3456 loss:1.759162425994873 \n",
      "iteration:3457 loss:1.8096439838409424 \n",
      "iteration:3458 loss:1.790916919708252 \n",
      "iteration:3459 loss:1.786462426185608 \n",
      "iteration:3460 loss:1.807031273841858 \n",
      "iteration:3461 loss:1.7986645698547363 \n",
      "iteration:3462 loss:1.8285819292068481 \n",
      "iteration:3463 loss:1.7954264879226685 \n",
      "iteration:3464 loss:1.8327758312225342 \n",
      "iteration:3465 loss:1.7630470991134644 \n",
      "iteration:3466 loss:1.7822158336639404 \n",
      "iteration:3467 loss:1.766473412513733 \n",
      "iteration:3468 loss:1.8262276649475098 \n",
      "iteration:3469 loss:1.8378431797027588 \n",
      "iteration:3470 loss:1.7919944524765015 \n",
      "iteration:3471 loss:1.7848063707351685 \n",
      "iteration:3472 loss:1.8062801361083984 \n",
      "iteration:3473 loss:1.7887901067733765 \n",
      "iteration:3474 loss:1.801629662513733 \n",
      "iteration:3475 loss:1.7345662117004395 \n",
      "iteration:3476 loss:1.7699902057647705 \n",
      "iteration:3477 loss:1.7826417684555054 \n",
      "iteration:3478 loss:1.7754305601119995 \n",
      "iteration:3479 loss:1.9572848081588745 \n",
      "Epoch-24 lr: 0.0005313952597646566\n",
      "iteration:3480 loss:1.7434096336364746 \n",
      "iteration:3481 loss:1.7635788917541504 \n",
      "iteration:3482 loss:1.7701566219329834 \n",
      "iteration:3483 loss:1.7885408401489258 \n",
      "iteration:3484 loss:1.7884114980697632 \n",
      "iteration:3485 loss:1.813433051109314 \n",
      "iteration:3486 loss:1.7918802499771118 \n",
      "iteration:3487 loss:1.76654052734375 \n",
      "iteration:3488 loss:1.765085220336914 \n",
      "iteration:3489 loss:1.7654712200164795 \n",
      "iteration:3490 loss:1.7593786716461182 \n",
      "iteration:3491 loss:1.8060797452926636 \n",
      "iteration:3492 loss:1.8074121475219727 \n",
      "iteration:3493 loss:1.739645004272461 \n",
      "iteration:3494 loss:1.7909960746765137 \n",
      "iteration:3495 loss:1.7708913087844849 \n",
      "iteration:3496 loss:1.7274469137191772 \n",
      "iteration:3497 loss:1.8143926858901978 \n",
      "iteration:3498 loss:1.7512593269348145 \n",
      "iteration:3499 loss:1.7289530038833618 \n",
      "iteration:3500 loss:1.792687177658081 \n",
      "iteration:3501 loss:1.8129398822784424 \n",
      "iteration:3502 loss:1.7451716661453247 \n",
      "iteration:3503 loss:1.7942841053009033 \n",
      "iteration:3504 loss:1.786976933479309 \n",
      "iteration:3505 loss:1.7937209606170654 \n",
      "iteration:3506 loss:1.7594921588897705 \n",
      "iteration:3507 loss:1.7733923196792603 \n",
      "iteration:3508 loss:1.8182499408721924 \n",
      "iteration:3509 loss:1.7697629928588867 \n",
      "iteration:3510 loss:1.798302173614502 \n",
      "iteration:3511 loss:1.7972285747528076 \n",
      "iteration:3512 loss:1.7566107511520386 \n",
      "iteration:3513 loss:1.8286633491516113 \n",
      "iteration:3514 loss:1.775119662284851 \n",
      "iteration:3515 loss:1.760575771331787 \n",
      "iteration:3516 loss:1.8218892812728882 \n",
      "iteration:3517 loss:1.7617924213409424 \n",
      "iteration:3518 loss:1.7877185344696045 \n",
      "iteration:3519 loss:1.8243030309677124 \n",
      "iteration:3520 loss:1.7674534320831299 \n",
      "iteration:3521 loss:1.8007104396820068 \n",
      "iteration:3522 loss:1.795074701309204 \n",
      "iteration:3523 loss:1.8544161319732666 \n",
      "iteration:3524 loss:1.7991076707839966 \n",
      "iteration:3525 loss:1.7499597072601318 \n",
      "iteration:3526 loss:1.7913087606430054 \n",
      "iteration:3527 loss:1.775580883026123 \n",
      "iteration:3528 loss:1.7930582761764526 \n",
      "iteration:3529 loss:1.7383610010147095 \n",
      "iteration:3530 loss:1.7911443710327148 \n",
      "iteration:3531 loss:1.7797198295593262 \n",
      "iteration:3532 loss:1.7651885747909546 \n",
      "iteration:3533 loss:1.732893705368042 \n",
      "iteration:3534 loss:1.7805616855621338 \n",
      "iteration:3535 loss:1.7713134288787842 \n",
      "iteration:3536 loss:1.7666658163070679 \n",
      "iteration:3537 loss:1.79608154296875 \n",
      "iteration:3538 loss:1.7428231239318848 \n",
      "iteration:3539 loss:1.8070160150527954 \n",
      "iteration:3540 loss:1.7194404602050781 \n",
      "iteration:3541 loss:1.826335072517395 \n",
      "iteration:3542 loss:1.7940067052841187 \n",
      "iteration:3543 loss:1.754180669784546 \n",
      "iteration:3544 loss:1.8051379919052124 \n",
      "iteration:3545 loss:1.7953251600265503 \n",
      "iteration:3546 loss:1.817962408065796 \n",
      "iteration:3547 loss:1.7760194540023804 \n",
      "iteration:3548 loss:1.7893160581588745 \n",
      "iteration:3549 loss:1.7527892589569092 \n",
      "iteration:3550 loss:1.8153988122940063 \n",
      "iteration:3551 loss:1.7332289218902588 \n",
      "iteration:3552 loss:1.7671834230422974 \n",
      "iteration:3553 loss:1.7738192081451416 \n",
      "iteration:3554 loss:1.749451756477356 \n",
      "iteration:3555 loss:1.7575950622558594 \n",
      "iteration:3556 loss:1.7933933734893799 \n",
      "iteration:3557 loss:1.737922191619873 \n",
      "iteration:3558 loss:1.7920290231704712 \n",
      "iteration:3559 loss:1.802469253540039 \n",
      "iteration:3560 loss:1.8285027742385864 \n",
      "iteration:3561 loss:1.8063565492630005 \n",
      "iteration:3562 loss:1.7541874647140503 \n",
      "iteration:3563 loss:1.7470005750656128 \n",
      "iteration:3564 loss:1.776330590248108 \n",
      "iteration:3565 loss:1.8060495853424072 \n",
      "iteration:3566 loss:1.8176954984664917 \n",
      "iteration:3567 loss:1.778306484222412 \n",
      "iteration:3568 loss:1.7700663805007935 \n",
      "iteration:3569 loss:1.773580551147461 \n",
      "iteration:3570 loss:1.7828047275543213 \n",
      "iteration:3571 loss:1.8513644933700562 \n",
      "iteration:3572 loss:1.8180752992630005 \n",
      "iteration:3573 loss:1.7707853317260742 \n",
      "iteration:3574 loss:1.7770129442214966 \n",
      "iteration:3575 loss:1.7952420711517334 \n",
      "iteration:3576 loss:1.7889513969421387 \n",
      "iteration:3577 loss:1.7963964939117432 \n",
      "iteration:3578 loss:1.8260282278060913 \n",
      "iteration:3579 loss:1.7697174549102783 \n",
      "iteration:3580 loss:1.8084523677825928 \n",
      "iteration:3581 loss:1.7682526111602783 \n",
      "iteration:3582 loss:1.75963294506073 \n",
      "iteration:3583 loss:1.788508653640747 \n",
      "iteration:3584 loss:1.7935309410095215 \n",
      "iteration:3585 loss:1.7462817430496216 \n",
      "iteration:3586 loss:1.7665460109710693 \n",
      "iteration:3587 loss:1.762593150138855 \n",
      "iteration:3588 loss:1.7581686973571777 \n",
      "iteration:3589 loss:1.7647984027862549 \n",
      "iteration:3590 loss:1.7835779190063477 \n",
      "iteration:3591 loss:1.754662275314331 \n",
      "iteration:3592 loss:1.8097434043884277 \n",
      "iteration:3593 loss:1.7674204111099243 \n",
      "iteration:3594 loss:1.8045672178268433 \n",
      "iteration:3595 loss:1.7788975238800049 \n",
      "iteration:3596 loss:1.7251112461090088 \n",
      "iteration:3597 loss:1.7933036088943481 \n",
      "iteration:3598 loss:1.804107904434204 \n",
      "iteration:3599 loss:1.7566217184066772 \n",
      "iteration:3600 loss:1.7986605167388916 \n",
      "iteration:3601 loss:1.805104374885559 \n",
      "iteration:3602 loss:1.8157650232315063 \n",
      "iteration:3603 loss:1.817720890045166 \n",
      "iteration:3604 loss:1.783964991569519 \n",
      "iteration:3605 loss:1.7588484287261963 \n",
      "iteration:3606 loss:1.7745634317398071 \n",
      "iteration:3607 loss:1.745009183883667 \n",
      "iteration:3608 loss:1.7795182466506958 \n",
      "iteration:3609 loss:1.7972853183746338 \n",
      "iteration:3610 loss:1.7459518909454346 \n",
      "iteration:3611 loss:1.8102742433547974 \n",
      "iteration:3612 loss:1.802873134613037 \n",
      "iteration:3613 loss:1.781609296798706 \n",
      "iteration:3614 loss:1.8203599452972412 \n",
      "iteration:3615 loss:1.7474263906478882 \n",
      "iteration:3616 loss:1.8177913427352905 \n",
      "iteration:3617 loss:1.814965844154358 \n",
      "iteration:3618 loss:1.7473280429840088 \n",
      "iteration:3619 loss:1.7608280181884766 \n",
      "iteration:3620 loss:1.7821928262710571 \n",
      "iteration:3621 loss:1.809790015220642 \n",
      "iteration:3622 loss:1.7735673189163208 \n",
      "iteration:3623 loss:1.775099277496338 \n",
      "iteration:3624 loss:1.7740765810012817 \n",
      "Epoch-25 lr: 0.0004999999999999998\n",
      "iteration:3625 loss:1.7630350589752197 \n",
      "iteration:3626 loss:1.7909799814224243 \n",
      "iteration:3627 loss:1.745910882949829 \n",
      "iteration:3628 loss:1.7508141994476318 \n",
      "iteration:3629 loss:1.7829759120941162 \n",
      "iteration:3630 loss:1.7997076511383057 \n",
      "iteration:3631 loss:1.7414116859436035 \n",
      "iteration:3632 loss:1.8828248977661133 \n",
      "iteration:3633 loss:1.7727179527282715 \n",
      "iteration:3634 loss:1.7947869300842285 \n",
      "iteration:3635 loss:1.8054325580596924 \n",
      "iteration:3636 loss:1.7422910928726196 \n",
      "iteration:3637 loss:1.7587743997573853 \n",
      "iteration:3638 loss:1.7774031162261963 \n",
      "iteration:3639 loss:1.8125221729278564 \n",
      "iteration:3640 loss:1.8284969329833984 \n",
      "iteration:3641 loss:1.7873342037200928 \n",
      "iteration:3642 loss:1.7728314399719238 \n",
      "iteration:3643 loss:1.770625352859497 \n",
      "iteration:3644 loss:1.7987825870513916 \n",
      "iteration:3645 loss:1.7532912492752075 \n",
      "iteration:3646 loss:1.748979926109314 \n",
      "iteration:3647 loss:1.7539114952087402 \n",
      "iteration:3648 loss:1.7927522659301758 \n",
      "iteration:3649 loss:1.7571152448654175 \n",
      "iteration:3650 loss:1.764736294746399 \n",
      "iteration:3651 loss:1.7846378087997437 \n",
      "iteration:3652 loss:1.7571221590042114 \n",
      "iteration:3653 loss:1.7746349573135376 \n",
      "iteration:3654 loss:1.771416425704956 \n",
      "iteration:3655 loss:1.7409995794296265 \n",
      "iteration:3656 loss:1.7881380319595337 \n",
      "iteration:3657 loss:1.7940189838409424 \n",
      "iteration:3658 loss:1.7924107313156128 \n",
      "iteration:3659 loss:1.7556437253952026 \n",
      "iteration:3660 loss:1.8157498836517334 \n",
      "iteration:3661 loss:1.7882474660873413 \n",
      "iteration:3662 loss:1.7539184093475342 \n",
      "iteration:3663 loss:1.7611008882522583 \n",
      "iteration:3664 loss:1.7996069192886353 \n",
      "iteration:3665 loss:1.8190442323684692 \n",
      "iteration:3666 loss:1.7951802015304565 \n",
      "iteration:3667 loss:1.7549564838409424 \n",
      "iteration:3668 loss:1.7705777883529663 \n",
      "iteration:3669 loss:1.7474583387374878 \n",
      "iteration:3670 loss:1.7758979797363281 \n",
      "iteration:3671 loss:1.7622536420822144 \n",
      "iteration:3672 loss:1.7547633647918701 \n",
      "iteration:3673 loss:1.7873425483703613 \n",
      "iteration:3674 loss:1.8365055322647095 \n",
      "iteration:3675 loss:1.8197660446166992 \n",
      "iteration:3676 loss:1.824919581413269 \n",
      "iteration:3677 loss:1.7825274467468262 \n",
      "iteration:3678 loss:1.7813225984573364 \n",
      "iteration:3679 loss:1.8319675922393799 \n",
      "iteration:3680 loss:1.751123309135437 \n",
      "iteration:3681 loss:1.7709664106369019 \n",
      "iteration:3682 loss:1.7651646137237549 \n",
      "iteration:3683 loss:1.761637806892395 \n",
      "iteration:3684 loss:1.779571533203125 \n",
      "iteration:3685 loss:1.7336887121200562 \n",
      "iteration:3686 loss:1.796979546546936 \n",
      "iteration:3687 loss:1.7929644584655762 \n",
      "iteration:3688 loss:1.7722649574279785 \n",
      "iteration:3689 loss:1.7724186182022095 \n",
      "iteration:3690 loss:1.784511685371399 \n",
      "iteration:3691 loss:1.797196865081787 \n",
      "iteration:3692 loss:1.8411545753479004 \n",
      "iteration:3693 loss:1.7408630847930908 \n",
      "iteration:3694 loss:1.754724144935608 \n",
      "iteration:3695 loss:1.776188611984253 \n",
      "iteration:3696 loss:1.7862597703933716 \n",
      "iteration:3697 loss:1.7780065536499023 \n",
      "iteration:3698 loss:1.770107388496399 \n",
      "iteration:3699 loss:1.7713332176208496 \n",
      "iteration:3700 loss:1.761711835861206 \n",
      "iteration:3701 loss:1.7968465089797974 \n",
      "iteration:3702 loss:1.754252552986145 \n",
      "iteration:3703 loss:1.7938154935836792 \n",
      "iteration:3704 loss:1.7547783851623535 \n",
      "iteration:3705 loss:1.7433489561080933 \n",
      "iteration:3706 loss:1.7318270206451416 \n",
      "iteration:3707 loss:1.773586630821228 \n",
      "iteration:3708 loss:1.7753633260726929 \n",
      "iteration:3709 loss:1.759718656539917 \n",
      "iteration:3710 loss:1.7680094242095947 \n",
      "iteration:3711 loss:1.7814618349075317 \n",
      "iteration:3712 loss:1.7218611240386963 \n",
      "iteration:3713 loss:1.8020269870758057 \n",
      "iteration:3714 loss:1.8038076162338257 \n",
      "iteration:3715 loss:1.7782137393951416 \n",
      "iteration:3716 loss:1.7790205478668213 \n",
      "iteration:3717 loss:1.7271183729171753 \n",
      "iteration:3718 loss:1.8074564933776855 \n",
      "iteration:3719 loss:1.8033881187438965 \n",
      "iteration:3720 loss:1.7511004209518433 \n",
      "iteration:3721 loss:1.7788242101669312 \n",
      "iteration:3722 loss:1.8043071031570435 \n",
      "iteration:3723 loss:1.791110634803772 \n",
      "iteration:3724 loss:1.8131417036056519 \n",
      "iteration:3725 loss:1.7737990617752075 \n",
      "iteration:3726 loss:1.7991257905960083 \n",
      "iteration:3727 loss:1.781882882118225 \n",
      "iteration:3728 loss:1.7970436811447144 \n",
      "iteration:3729 loss:1.7551945447921753 \n",
      "iteration:3730 loss:1.7568875551223755 \n",
      "iteration:3731 loss:1.757965326309204 \n",
      "iteration:3732 loss:1.818278193473816 \n",
      "iteration:3733 loss:1.7947001457214355 \n",
      "iteration:3734 loss:1.7966707944869995 \n",
      "iteration:3735 loss:1.8547049760818481 \n",
      "iteration:3736 loss:1.7831881046295166 \n",
      "iteration:3737 loss:1.8026713132858276 \n",
      "iteration:3738 loss:1.7351891994476318 \n",
      "iteration:3739 loss:1.7897272109985352 \n",
      "iteration:3740 loss:1.7901263236999512 \n",
      "iteration:3741 loss:1.7929834127426147 \n",
      "iteration:3742 loss:1.7613513469696045 \n",
      "iteration:3743 loss:1.7576290369033813 \n",
      "iteration:3744 loss:1.8038281202316284 \n",
      "iteration:3745 loss:1.7930859327316284 \n",
      "iteration:3746 loss:1.7626690864562988 \n",
      "iteration:3747 loss:1.8563863039016724 \n",
      "iteration:3748 loss:1.7597135305404663 \n",
      "iteration:3749 loss:1.7588779926300049 \n",
      "iteration:3750 loss:1.753201961517334 \n",
      "iteration:3751 loss:1.7776843309402466 \n",
      "iteration:3752 loss:1.759379267692566 \n",
      "iteration:3753 loss:1.832085132598877 \n",
      "iteration:3754 loss:1.7737332582473755 \n",
      "iteration:3755 loss:1.7789931297302246 \n",
      "iteration:3756 loss:1.7887144088745117 \n",
      "iteration:3757 loss:1.7832053899765015 \n",
      "iteration:3758 loss:1.7399548292160034 \n",
      "iteration:3759 loss:1.7744719982147217 \n",
      "iteration:3760 loss:1.8000056743621826 \n",
      "iteration:3761 loss:1.783877968788147 \n",
      "iteration:3762 loss:1.780565619468689 \n",
      "iteration:3763 loss:1.8106868267059326 \n",
      "iteration:3764 loss:1.7851766347885132 \n",
      "iteration:3765 loss:1.7712339162826538 \n",
      "iteration:3766 loss:1.7904034852981567 \n",
      "iteration:3767 loss:1.7574682235717773 \n",
      "iteration:3768 loss:1.739359974861145 \n",
      "iteration:3769 loss:1.8824923038482666 \n",
      "Epoch-26 lr: 0.00046860474023534314\n",
      "iteration:3770 loss:1.7960666418075562 \n",
      "iteration:3771 loss:1.7972660064697266 \n",
      "iteration:3772 loss:1.778102159500122 \n",
      "iteration:3773 loss:1.766402006149292 \n",
      "iteration:3774 loss:1.775766372680664 \n",
      "iteration:3775 loss:1.782911777496338 \n",
      "iteration:3776 loss:1.8236864805221558 \n",
      "iteration:3777 loss:1.7029550075531006 \n",
      "iteration:3778 loss:1.7640637159347534 \n",
      "iteration:3779 loss:1.8124306201934814 \n",
      "iteration:3780 loss:1.7599588632583618 \n",
      "iteration:3781 loss:1.734701156616211 \n",
      "iteration:3782 loss:1.7897047996520996 \n",
      "iteration:3783 loss:1.789082646369934 \n",
      "iteration:3784 loss:1.768960952758789 \n",
      "iteration:3785 loss:1.7579823732376099 \n",
      "iteration:3786 loss:1.7338343858718872 \n",
      "iteration:3787 loss:1.7927361726760864 \n",
      "iteration:3788 loss:1.7483022212982178 \n",
      "iteration:3789 loss:1.789753794670105 \n",
      "iteration:3790 loss:1.8134737014770508 \n",
      "iteration:3791 loss:1.765438437461853 \n",
      "iteration:3792 loss:1.7330149412155151 \n",
      "iteration:3793 loss:1.786302924156189 \n",
      "iteration:3794 loss:1.7860270738601685 \n",
      "iteration:3795 loss:1.8109533786773682 \n",
      "iteration:3796 loss:1.7338887453079224 \n",
      "iteration:3797 loss:1.8067848682403564 \n",
      "iteration:3798 loss:1.7356215715408325 \n",
      "iteration:3799 loss:1.7946549654006958 \n",
      "iteration:3800 loss:1.7547727823257446 \n",
      "iteration:3801 loss:1.795230746269226 \n",
      "iteration:3802 loss:1.7200812101364136 \n",
      "iteration:3803 loss:1.7438653707504272 \n",
      "iteration:3804 loss:1.7443093061447144 \n",
      "iteration:3805 loss:1.7639811038970947 \n",
      "iteration:3806 loss:1.7963244915008545 \n",
      "iteration:3807 loss:1.756832242012024 \n",
      "iteration:3808 loss:1.8012490272521973 \n",
      "iteration:3809 loss:1.711709976196289 \n",
      "iteration:3810 loss:1.7839418649673462 \n",
      "iteration:3811 loss:1.7728573083877563 \n",
      "iteration:3812 loss:1.74138605594635 \n",
      "iteration:3813 loss:1.7713924646377563 \n",
      "iteration:3814 loss:1.7823256254196167 \n",
      "iteration:3815 loss:1.810686469078064 \n",
      "iteration:3816 loss:1.761138677597046 \n",
      "iteration:3817 loss:1.7492845058441162 \n",
      "iteration:3818 loss:1.7708172798156738 \n",
      "iteration:3819 loss:1.7367818355560303 \n",
      "iteration:3820 loss:1.7796757221221924 \n",
      "iteration:3821 loss:1.7309226989746094 \n",
      "iteration:3822 loss:1.7998063564300537 \n",
      "iteration:3823 loss:1.7452590465545654 \n",
      "iteration:3824 loss:1.804694652557373 \n",
      "iteration:3825 loss:1.746296763420105 \n",
      "iteration:3826 loss:1.7437628507614136 \n",
      "iteration:3827 loss:1.778290033340454 \n",
      "iteration:3828 loss:1.748582124710083 \n",
      "iteration:3829 loss:1.759249210357666 \n",
      "iteration:3830 loss:1.7374440431594849 \n",
      "iteration:3831 loss:1.8118689060211182 \n",
      "iteration:3832 loss:1.7506279945373535 \n",
      "iteration:3833 loss:1.78887939453125 \n",
      "iteration:3834 loss:1.7361112833023071 \n",
      "iteration:3835 loss:1.773521065711975 \n",
      "iteration:3836 loss:1.7579126358032227 \n",
      "iteration:3837 loss:1.7881596088409424 \n",
      "iteration:3838 loss:1.7517356872558594 \n",
      "iteration:3839 loss:1.7613039016723633 \n",
      "iteration:3840 loss:1.7151345014572144 \n",
      "iteration:3841 loss:1.8093425035476685 \n",
      "iteration:3842 loss:1.7657792568206787 \n",
      "iteration:3843 loss:1.749250054359436 \n",
      "iteration:3844 loss:1.7788405418395996 \n",
      "iteration:3845 loss:1.7951167821884155 \n",
      "iteration:3846 loss:1.7851876020431519 \n",
      "iteration:3847 loss:1.8062831163406372 \n",
      "iteration:3848 loss:1.7746317386627197 \n",
      "iteration:3849 loss:1.7874153852462769 \n",
      "iteration:3850 loss:1.7905172109603882 \n",
      "iteration:3851 loss:1.7535181045532227 \n",
      "iteration:3852 loss:1.7679274082183838 \n",
      "iteration:3853 loss:1.7727736234664917 \n",
      "iteration:3854 loss:1.791407823562622 \n",
      "iteration:3855 loss:1.798054814338684 \n",
      "iteration:3856 loss:1.7883579730987549 \n",
      "iteration:3857 loss:1.77759850025177 \n",
      "iteration:3858 loss:1.8046596050262451 \n",
      "iteration:3859 loss:1.738726019859314 \n",
      "iteration:3860 loss:1.7828330993652344 \n",
      "iteration:3861 loss:1.7508060932159424 \n",
      "iteration:3862 loss:1.774468183517456 \n",
      "iteration:3863 loss:1.7006502151489258 \n",
      "iteration:3864 loss:1.7547965049743652 \n",
      "iteration:3865 loss:1.8326027393341064 \n",
      "iteration:3866 loss:1.7381482124328613 \n",
      "iteration:3867 loss:1.7383750677108765 \n",
      "iteration:3868 loss:1.8049230575561523 \n",
      "iteration:3869 loss:1.7960898876190186 \n",
      "iteration:3870 loss:1.7511299848556519 \n",
      "iteration:3871 loss:1.7390148639678955 \n",
      "iteration:3872 loss:1.8052996397018433 \n",
      "iteration:3873 loss:1.799615740776062 \n",
      "iteration:3874 loss:1.8222180604934692 \n",
      "iteration:3875 loss:1.7245612144470215 \n",
      "iteration:3876 loss:1.7900195121765137 \n",
      "iteration:3877 loss:1.767279028892517 \n",
      "iteration:3878 loss:1.7712788581848145 \n",
      "iteration:3879 loss:1.7233058214187622 \n",
      "iteration:3880 loss:1.807586908340454 \n",
      "iteration:3881 loss:1.7615708112716675 \n",
      "iteration:3882 loss:1.769279956817627 \n",
      "iteration:3883 loss:1.7413277626037598 \n",
      "iteration:3884 loss:1.791733980178833 \n",
      "iteration:3885 loss:1.7833220958709717 \n",
      "iteration:3886 loss:1.7786177396774292 \n",
      "iteration:3887 loss:1.8028403520584106 \n",
      "iteration:3888 loss:1.7641937732696533 \n",
      "iteration:3889 loss:1.7437962293624878 \n",
      "iteration:3890 loss:1.804967999458313 \n",
      "iteration:3891 loss:1.730658769607544 \n",
      "iteration:3892 loss:1.7849658727645874 \n",
      "iteration:3893 loss:1.7454231977462769 \n",
      "iteration:3894 loss:1.7184853553771973 \n",
      "iteration:3895 loss:1.798071265220642 \n",
      "iteration:3896 loss:1.7734832763671875 \n",
      "iteration:3897 loss:1.7700423002243042 \n",
      "iteration:3898 loss:1.7301971912384033 \n",
      "iteration:3899 loss:1.7220804691314697 \n",
      "iteration:3900 loss:1.7418239116668701 \n",
      "iteration:3901 loss:1.7458369731903076 \n",
      "iteration:3902 loss:1.7822654247283936 \n",
      "iteration:3903 loss:1.8026896715164185 \n",
      "iteration:3904 loss:1.8486144542694092 \n",
      "iteration:3905 loss:1.7748173475265503 \n",
      "iteration:3906 loss:1.7591131925582886 \n",
      "iteration:3907 loss:1.7209676504135132 \n",
      "iteration:3908 loss:1.7016706466674805 \n",
      "iteration:3909 loss:1.7636948823928833 \n",
      "iteration:3910 loss:1.8081406354904175 \n",
      "iteration:3911 loss:1.7132737636566162 \n",
      "iteration:3912 loss:1.753193974494934 \n",
      "iteration:3913 loss:1.79072105884552 \n",
      "iteration:3914 loss:2.086329936981201 \n",
      "Epoch-27 lr: 0.00043733338321784774\n",
      "iteration:3915 loss:1.7974398136138916 \n",
      "iteration:3916 loss:1.7471376657485962 \n",
      "iteration:3917 loss:1.7922208309173584 \n",
      "iteration:3918 loss:1.7345612049102783 \n",
      "iteration:3919 loss:1.7626582384109497 \n",
      "iteration:3920 loss:1.7111623287200928 \n",
      "iteration:3921 loss:1.7587096691131592 \n",
      "iteration:3922 loss:1.7627921104431152 \n",
      "iteration:3923 loss:1.8058209419250488 \n",
      "iteration:3924 loss:1.775614857673645 \n",
      "iteration:3925 loss:1.807596206665039 \n",
      "iteration:3926 loss:1.7950209379196167 \n",
      "iteration:3927 loss:1.7285892963409424 \n",
      "iteration:3928 loss:1.7787187099456787 \n",
      "iteration:3929 loss:1.700356125831604 \n",
      "iteration:3930 loss:1.7454462051391602 \n",
      "iteration:3931 loss:1.7395981550216675 \n",
      "iteration:3932 loss:1.7766239643096924 \n",
      "iteration:3933 loss:1.7874047756195068 \n",
      "iteration:3934 loss:1.740501880645752 \n",
      "iteration:3935 loss:1.8172733783721924 \n",
      "iteration:3936 loss:1.7771157026290894 \n",
      "iteration:3937 loss:1.758582353591919 \n",
      "iteration:3938 loss:1.8080856800079346 \n",
      "iteration:3939 loss:1.732262372970581 \n",
      "iteration:3940 loss:1.7896677255630493 \n",
      "iteration:3941 loss:1.7867268323898315 \n",
      "iteration:3942 loss:1.7790518999099731 \n",
      "iteration:3943 loss:1.7760637998580933 \n",
      "iteration:3944 loss:1.7365221977233887 \n",
      "iteration:3945 loss:1.7815805673599243 \n",
      "iteration:3946 loss:1.767007827758789 \n",
      "iteration:3947 loss:1.7964739799499512 \n",
      "iteration:3948 loss:1.8505100011825562 \n",
      "iteration:3949 loss:1.7411231994628906 \n",
      "iteration:3950 loss:1.7302494049072266 \n",
      "iteration:3951 loss:1.7808681726455688 \n",
      "iteration:3952 loss:1.7111626863479614 \n",
      "iteration:3953 loss:1.76002037525177 \n",
      "iteration:3954 loss:1.7758504152297974 \n",
      "iteration:3955 loss:1.7855942249298096 \n",
      "iteration:3956 loss:1.7938950061798096 \n",
      "iteration:3957 loss:1.8152929544448853 \n",
      "iteration:3958 loss:1.7699685096740723 \n",
      "iteration:3959 loss:1.7736645936965942 \n",
      "iteration:3960 loss:1.7845450639724731 \n",
      "iteration:3961 loss:1.7708035707473755 \n",
      "iteration:3962 loss:1.8194706439971924 \n",
      "iteration:3963 loss:1.8121856451034546 \n",
      "iteration:3964 loss:1.7874330282211304 \n",
      "iteration:3965 loss:1.728308081626892 \n",
      "iteration:3966 loss:1.771634578704834 \n",
      "iteration:3967 loss:1.7892478704452515 \n",
      "iteration:3968 loss:1.8095389604568481 \n",
      "iteration:3969 loss:1.752732515335083 \n",
      "iteration:3970 loss:1.7555323839187622 \n",
      "iteration:3971 loss:1.7846043109893799 \n",
      "iteration:3972 loss:1.7449157238006592 \n",
      "iteration:3973 loss:1.8072112798690796 \n",
      "iteration:3974 loss:1.7501932382583618 \n",
      "iteration:3975 loss:1.7307243347167969 \n",
      "iteration:3976 loss:1.7614535093307495 \n",
      "iteration:3977 loss:1.7667065858840942 \n",
      "iteration:3978 loss:1.7708691358566284 \n",
      "iteration:3979 loss:1.7746895551681519 \n",
      "iteration:3980 loss:1.7467807531356812 \n",
      "iteration:3981 loss:1.7584614753723145 \n",
      "iteration:3982 loss:1.7380163669586182 \n",
      "iteration:3983 loss:1.7796075344085693 \n",
      "iteration:3984 loss:1.783629298210144 \n",
      "iteration:3985 loss:1.7971069812774658 \n",
      "iteration:3986 loss:1.7422617673873901 \n",
      "iteration:3987 loss:1.737647294998169 \n",
      "iteration:3988 loss:1.807023525238037 \n",
      "iteration:3989 loss:1.7556475400924683 \n",
      "iteration:3990 loss:1.769313931465149 \n",
      "iteration:3991 loss:1.7474188804626465 \n",
      "iteration:3992 loss:1.7311820983886719 \n",
      "iteration:3993 loss:1.768349051475525 \n",
      "iteration:3994 loss:1.7381240129470825 \n",
      "iteration:3995 loss:1.8209364414215088 \n",
      "iteration:3996 loss:1.7384556531906128 \n",
      "iteration:3997 loss:1.775333285331726 \n",
      "iteration:3998 loss:1.734650731086731 \n",
      "iteration:3999 loss:1.7050014734268188 \n",
      "iteration:4000 loss:1.7813993692398071 \n",
      "iteration:4001 loss:1.756767749786377 \n",
      "iteration:4002 loss:1.7549479007720947 \n",
      "iteration:4003 loss:1.7747831344604492 \n",
      "iteration:4004 loss:1.747216820716858 \n",
      "iteration:4005 loss:1.7876523733139038 \n",
      "iteration:4006 loss:1.7699999809265137 \n",
      "iteration:4007 loss:1.7832123041152954 \n",
      "iteration:4008 loss:1.7577691078186035 \n",
      "iteration:4009 loss:1.7310863733291626 \n",
      "iteration:4010 loss:1.7670406103134155 \n",
      "iteration:4011 loss:1.7696397304534912 \n",
      "iteration:4012 loss:1.762631893157959 \n",
      "iteration:4013 loss:1.7611613273620605 \n",
      "iteration:4014 loss:1.7739851474761963 \n",
      "iteration:4015 loss:1.7799497842788696 \n",
      "iteration:4016 loss:1.7793188095092773 \n",
      "iteration:4017 loss:1.7886954545974731 \n",
      "iteration:4018 loss:1.8057204484939575 \n",
      "iteration:4019 loss:1.8207504749298096 \n",
      "iteration:4020 loss:1.7238084077835083 \n",
      "iteration:4021 loss:1.7666101455688477 \n",
      "iteration:4022 loss:1.7682912349700928 \n",
      "iteration:4023 loss:1.7593398094177246 \n",
      "iteration:4024 loss:1.777549386024475 \n",
      "iteration:4025 loss:1.7700122594833374 \n",
      "iteration:4026 loss:1.7718477249145508 \n",
      "iteration:4027 loss:1.8044980764389038 \n",
      "iteration:4028 loss:1.7353469133377075 \n",
      "iteration:4029 loss:1.759453535079956 \n",
      "iteration:4030 loss:1.7295036315917969 \n",
      "iteration:4031 loss:1.7900968790054321 \n",
      "iteration:4032 loss:1.771044135093689 \n",
      "iteration:4033 loss:1.7894644737243652 \n",
      "iteration:4034 loss:1.72773015499115 \n",
      "iteration:4035 loss:1.8301299810409546 \n",
      "iteration:4036 loss:1.7645641565322876 \n",
      "iteration:4037 loss:1.7176231145858765 \n",
      "iteration:4038 loss:1.7319124937057495 \n",
      "iteration:4039 loss:1.784578800201416 \n",
      "iteration:4040 loss:1.7506217956542969 \n",
      "iteration:4041 loss:1.8256319761276245 \n",
      "iteration:4042 loss:1.7966749668121338 \n",
      "iteration:4043 loss:1.7696236371994019 \n",
      "iteration:4044 loss:1.754072904586792 \n",
      "iteration:4045 loss:1.779280424118042 \n",
      "iteration:4046 loss:1.799623966217041 \n",
      "iteration:4047 loss:1.7587262392044067 \n",
      "iteration:4048 loss:1.729223608970642 \n",
      "iteration:4049 loss:1.7410316467285156 \n",
      "iteration:4050 loss:1.7579138278961182 \n",
      "iteration:4051 loss:1.753824234008789 \n",
      "iteration:4052 loss:1.7449568510055542 \n",
      "iteration:4053 loss:1.756595253944397 \n",
      "iteration:4054 loss:1.7524194717407227 \n",
      "iteration:4055 loss:1.7485313415527344 \n",
      "iteration:4056 loss:1.7818100452423096 \n",
      "iteration:4057 loss:1.7524402141571045 \n",
      "iteration:4058 loss:1.7653560638427734 \n",
      "iteration:4059 loss:1.8794554471969604 \n",
      "Epoch-28 lr: 0.00040630934270713756\n",
      "iteration:4060 loss:1.796643853187561 \n",
      "iteration:4061 loss:1.7496793270111084 \n",
      "iteration:4062 loss:1.7455511093139648 \n",
      "iteration:4063 loss:1.7751131057739258 \n",
      "iteration:4064 loss:1.765904188156128 \n",
      "iteration:4065 loss:1.7952302694320679 \n",
      "iteration:4066 loss:1.7361444234848022 \n",
      "iteration:4067 loss:1.7133941650390625 \n",
      "iteration:4068 loss:1.6882195472717285 \n",
      "iteration:4069 loss:1.738148808479309 \n",
      "iteration:4070 loss:1.7170052528381348 \n",
      "iteration:4071 loss:1.7338879108428955 \n",
      "iteration:4072 loss:1.7723060846328735 \n",
      "iteration:4073 loss:1.791879653930664 \n",
      "iteration:4074 loss:1.7992061376571655 \n",
      "iteration:4075 loss:1.740815281867981 \n",
      "iteration:4076 loss:1.7486013174057007 \n",
      "iteration:4077 loss:1.7608087062835693 \n",
      "iteration:4078 loss:1.7470452785491943 \n",
      "iteration:4079 loss:1.7383918762207031 \n",
      "iteration:4080 loss:1.777246117591858 \n",
      "iteration:4081 loss:1.720666527748108 \n",
      "iteration:4082 loss:1.753271222114563 \n",
      "iteration:4083 loss:1.7667107582092285 \n",
      "iteration:4084 loss:1.7812288999557495 \n",
      "iteration:4085 loss:1.7753468751907349 \n",
      "iteration:4086 loss:1.7628470659255981 \n",
      "iteration:4087 loss:1.7603273391723633 \n",
      "iteration:4088 loss:1.7254642248153687 \n",
      "iteration:4089 loss:1.783864974975586 \n",
      "iteration:4090 loss:1.812099814414978 \n",
      "iteration:4091 loss:1.7798813581466675 \n",
      "iteration:4092 loss:1.7336761951446533 \n",
      "iteration:4093 loss:1.755669355392456 \n",
      "iteration:4094 loss:1.758717656135559 \n",
      "iteration:4095 loss:1.7686264514923096 \n",
      "iteration:4096 loss:1.7356343269348145 \n",
      "iteration:4097 loss:1.750144600868225 \n",
      "iteration:4098 loss:1.7510056495666504 \n",
      "iteration:4099 loss:1.7714048624038696 \n",
      "iteration:4100 loss:1.781288504600525 \n",
      "iteration:4101 loss:1.7650022506713867 \n",
      "iteration:4102 loss:1.7797367572784424 \n",
      "iteration:4103 loss:1.7896207571029663 \n",
      "iteration:4104 loss:1.7651689052581787 \n",
      "iteration:4105 loss:1.7725738286972046 \n",
      "iteration:4106 loss:1.782671332359314 \n",
      "iteration:4107 loss:1.7332676649093628 \n",
      "iteration:4108 loss:1.743124008178711 \n",
      "iteration:4109 loss:1.8001219034194946 \n",
      "iteration:4110 loss:1.797672152519226 \n",
      "iteration:4111 loss:1.6781203746795654 \n",
      "iteration:4112 loss:1.724137544631958 \n",
      "iteration:4113 loss:1.7734858989715576 \n",
      "iteration:4114 loss:1.7896161079406738 \n",
      "iteration:4115 loss:1.6912368535995483 \n",
      "iteration:4116 loss:1.816991925239563 \n",
      "iteration:4117 loss:1.7626373767852783 \n",
      "iteration:4118 loss:1.7666019201278687 \n",
      "iteration:4119 loss:1.8158652782440186 \n",
      "iteration:4120 loss:1.7795580625534058 \n",
      "iteration:4121 loss:1.791075587272644 \n",
      "iteration:4122 loss:1.7427982091903687 \n",
      "iteration:4123 loss:1.771392822265625 \n",
      "iteration:4124 loss:1.7509098052978516 \n",
      "iteration:4125 loss:1.7630301713943481 \n",
      "iteration:4126 loss:1.7498825788497925 \n",
      "iteration:4127 loss:1.7353227138519287 \n",
      "iteration:4128 loss:1.7495660781860352 \n",
      "iteration:4129 loss:1.7876543998718262 \n",
      "iteration:4130 loss:1.751060128211975 \n",
      "iteration:4131 loss:1.7063775062561035 \n",
      "iteration:4132 loss:1.7394063472747803 \n",
      "iteration:4133 loss:1.7223325967788696 \n",
      "iteration:4134 loss:1.7500333786010742 \n",
      "iteration:4135 loss:1.7203210592269897 \n",
      "iteration:4136 loss:1.7707234621047974 \n",
      "iteration:4137 loss:1.7394976615905762 \n",
      "iteration:4138 loss:1.7679160833358765 \n",
      "iteration:4139 loss:1.7652181386947632 \n",
      "iteration:4140 loss:1.7261407375335693 \n",
      "iteration:4141 loss:1.806550145149231 \n",
      "iteration:4142 loss:1.7663401365280151 \n",
      "iteration:4143 loss:1.716274619102478 \n",
      "iteration:4144 loss:1.7599698305130005 \n",
      "iteration:4145 loss:1.757143497467041 \n",
      "iteration:4146 loss:1.7615138292312622 \n",
      "iteration:4147 loss:1.7628642320632935 \n",
      "iteration:4148 loss:1.7518362998962402 \n",
      "iteration:4149 loss:1.7512298822402954 \n",
      "iteration:4150 loss:1.756460189819336 \n",
      "iteration:4151 loss:1.7760089635849 \n",
      "iteration:4152 loss:1.772708535194397 \n",
      "iteration:4153 loss:1.7563287019729614 \n",
      "iteration:4154 loss:1.787632703781128 \n",
      "iteration:4155 loss:1.7808811664581299 \n",
      "iteration:4156 loss:1.7271913290023804 \n",
      "iteration:4157 loss:1.7138162851333618 \n",
      "iteration:4158 loss:1.7643665075302124 \n",
      "iteration:4159 loss:1.7395641803741455 \n",
      "iteration:4160 loss:1.8306561708450317 \n",
      "iteration:4161 loss:1.7766772508621216 \n",
      "iteration:4162 loss:1.7563989162445068 \n",
      "iteration:4163 loss:1.7423179149627686 \n",
      "iteration:4164 loss:1.780131459236145 \n",
      "iteration:4165 loss:1.7627209424972534 \n",
      "iteration:4166 loss:1.7528715133666992 \n",
      "iteration:4167 loss:1.7287768125534058 \n",
      "iteration:4168 loss:1.7212907075881958 \n",
      "iteration:4169 loss:1.81714928150177 \n",
      "iteration:4170 loss:1.7581121921539307 \n",
      "iteration:4171 loss:1.7284784317016602 \n",
      "iteration:4172 loss:1.751861810684204 \n",
      "iteration:4173 loss:1.748813271522522 \n",
      "iteration:4174 loss:1.7425144910812378 \n",
      "iteration:4175 loss:1.7959779500961304 \n",
      "iteration:4176 loss:1.7250057458877563 \n",
      "iteration:4177 loss:1.705202579498291 \n",
      "iteration:4178 loss:1.7794102430343628 \n",
      "iteration:4179 loss:1.7391092777252197 \n",
      "iteration:4180 loss:1.74278724193573 \n",
      "iteration:4181 loss:1.7712010145187378 \n",
      "iteration:4182 loss:1.7878546714782715 \n",
      "iteration:4183 loss:1.7326643466949463 \n",
      "iteration:4184 loss:1.7701761722564697 \n",
      "iteration:4185 loss:1.764790654182434 \n",
      "iteration:4186 loss:1.8000720739364624 \n",
      "iteration:4187 loss:1.7562575340270996 \n",
      "iteration:4188 loss:1.7838423252105713 \n",
      "iteration:4189 loss:1.7496176958084106 \n",
      "iteration:4190 loss:1.745707631111145 \n",
      "iteration:4191 loss:1.7338893413543701 \n",
      "iteration:4192 loss:1.744717001914978 \n",
      "iteration:4193 loss:1.778994083404541 \n",
      "iteration:4194 loss:1.7520397901535034 \n",
      "iteration:4195 loss:1.769657015800476 \n",
      "iteration:4196 loss:1.7136175632476807 \n",
      "iteration:4197 loss:1.7806209325790405 \n",
      "iteration:4198 loss:1.7312475442886353 \n",
      "iteration:4199 loss:1.7212274074554443 \n",
      "iteration:4200 loss:1.772987723350525 \n",
      "iteration:4201 loss:1.7755886316299438 \n",
      "iteration:4202 loss:1.744899868965149 \n",
      "iteration:4203 loss:1.7433327436447144 \n",
      "iteration:4204 loss:1.8404284715652466 \n",
      "Epoch-29 lr: 0.00037565505641757246\n",
      "iteration:4205 loss:1.7509909868240356 \n",
      "iteration:4206 loss:1.7490127086639404 \n",
      "iteration:4207 loss:1.7174198627471924 \n",
      "iteration:4208 loss:1.7412374019622803 \n",
      "iteration:4209 loss:1.738341212272644 \n",
      "iteration:4210 loss:1.713708519935608 \n",
      "iteration:4211 loss:1.7453564405441284 \n",
      "iteration:4212 loss:1.7525315284729004 \n",
      "iteration:4213 loss:1.8192564249038696 \n",
      "iteration:4214 loss:1.7972092628479004 \n",
      "iteration:4215 loss:1.7567130327224731 \n",
      "iteration:4216 loss:1.711557388305664 \n",
      "iteration:4217 loss:1.6993862390518188 \n",
      "iteration:4218 loss:1.731605887413025 \n",
      "iteration:4219 loss:1.776011347770691 \n",
      "iteration:4220 loss:1.740221381187439 \n",
      "iteration:4221 loss:1.8020849227905273 \n",
      "iteration:4222 loss:1.7059826850891113 \n",
      "iteration:4223 loss:1.7492932081222534 \n",
      "iteration:4224 loss:1.7610200643539429 \n",
      "iteration:4225 loss:1.7685503959655762 \n",
      "iteration:4226 loss:1.721825361251831 \n",
      "iteration:4227 loss:1.776129961013794 \n",
      "iteration:4228 loss:1.7703382968902588 \n",
      "iteration:4229 loss:1.7549296617507935 \n",
      "iteration:4230 loss:1.7432725429534912 \n",
      "iteration:4231 loss:1.725151538848877 \n",
      "iteration:4232 loss:1.755279541015625 \n",
      "iteration:4233 loss:1.799904227256775 \n",
      "iteration:4234 loss:1.7646781206130981 \n",
      "iteration:4235 loss:1.7943086624145508 \n",
      "iteration:4236 loss:1.7196991443634033 \n",
      "iteration:4237 loss:1.7151492834091187 \n",
      "iteration:4238 loss:1.745459794998169 \n",
      "iteration:4239 loss:1.734049677848816 \n",
      "iteration:4240 loss:1.7663058042526245 \n",
      "iteration:4241 loss:1.7212636470794678 \n",
      "iteration:4242 loss:1.743333101272583 \n",
      "iteration:4243 loss:1.737017035484314 \n",
      "iteration:4244 loss:1.7360652685165405 \n",
      "iteration:4245 loss:1.7543652057647705 \n",
      "iteration:4246 loss:1.7648776769638062 \n",
      "iteration:4247 loss:1.713674545288086 \n",
      "iteration:4248 loss:1.7271040678024292 \n",
      "iteration:4249 loss:1.7294530868530273 \n",
      "iteration:4250 loss:1.7626674175262451 \n",
      "iteration:4251 loss:1.712077021598816 \n",
      "iteration:4252 loss:1.771544337272644 \n",
      "iteration:4253 loss:1.7553894519805908 \n",
      "iteration:4254 loss:1.7997678518295288 \n",
      "iteration:4255 loss:1.730659008026123 \n",
      "iteration:4256 loss:1.7365537881851196 \n",
      "iteration:4257 loss:1.764142632484436 \n",
      "iteration:4258 loss:1.7550017833709717 \n",
      "iteration:4259 loss:1.7680660486221313 \n",
      "iteration:4260 loss:1.732652187347412 \n",
      "iteration:4261 loss:1.7374204397201538 \n",
      "iteration:4262 loss:1.7605204582214355 \n",
      "iteration:4263 loss:1.7534397840499878 \n",
      "iteration:4264 loss:1.7304232120513916 \n",
      "iteration:4265 loss:1.7647583484649658 \n",
      "iteration:4266 loss:1.7424529790878296 \n",
      "iteration:4267 loss:1.7465095520019531 \n",
      "iteration:4268 loss:1.7161084413528442 \n",
      "iteration:4269 loss:1.7400535345077515 \n",
      "iteration:4270 loss:1.7238924503326416 \n",
      "iteration:4271 loss:1.7382783889770508 \n",
      "iteration:4272 loss:1.7528102397918701 \n",
      "iteration:4273 loss:1.7545980215072632 \n",
      "iteration:4274 loss:1.7564581632614136 \n",
      "iteration:4275 loss:1.7290749549865723 \n",
      "iteration:4276 loss:1.7688586711883545 \n",
      "iteration:4277 loss:1.7631937265396118 \n",
      "iteration:4278 loss:1.706345796585083 \n",
      "iteration:4279 loss:1.779752254486084 \n",
      "iteration:4280 loss:1.803179144859314 \n",
      "iteration:4281 loss:1.6985145807266235 \n",
      "iteration:4282 loss:1.7372701168060303 \n",
      "iteration:4283 loss:1.7740403413772583 \n",
      "iteration:4284 loss:1.7630022764205933 \n",
      "iteration:4285 loss:1.7511905431747437 \n",
      "iteration:4286 loss:1.827246069908142 \n",
      "iteration:4287 loss:1.7918888330459595 \n",
      "iteration:4288 loss:1.779668927192688 \n",
      "iteration:4289 loss:1.7725821733474731 \n",
      "iteration:4290 loss:1.728827714920044 \n",
      "iteration:4291 loss:1.7067992687225342 \n",
      "iteration:4292 loss:1.7457232475280762 \n",
      "iteration:4293 loss:1.7378145456314087 \n",
      "iteration:4294 loss:1.759598970413208 \n",
      "iteration:4295 loss:1.7651349306106567 \n",
      "iteration:4296 loss:1.7410566806793213 \n",
      "iteration:4297 loss:1.7639241218566895 \n",
      "iteration:4298 loss:1.7144196033477783 \n",
      "iteration:4299 loss:1.6930936574935913 \n",
      "iteration:4300 loss:1.7527066469192505 \n",
      "iteration:4301 loss:1.7682340145111084 \n",
      "iteration:4302 loss:1.7595080137252808 \n",
      "iteration:4303 loss:1.7771459817886353 \n",
      "iteration:4304 loss:1.7850526571273804 \n",
      "iteration:4305 loss:1.720424771308899 \n",
      "iteration:4306 loss:1.7863928079605103 \n",
      "iteration:4307 loss:1.710724949836731 \n",
      "iteration:4308 loss:1.768032431602478 \n",
      "iteration:4309 loss:1.739580512046814 \n",
      "iteration:4310 loss:1.7734694480895996 \n",
      "iteration:4311 loss:1.793318271636963 \n",
      "iteration:4312 loss:1.7592684030532837 \n",
      "iteration:4313 loss:1.785220980644226 \n",
      "iteration:4314 loss:1.722264051437378 \n",
      "iteration:4315 loss:1.7704561948776245 \n",
      "iteration:4316 loss:1.7738598585128784 \n",
      "iteration:4317 loss:1.7214677333831787 \n",
      "iteration:4318 loss:1.7450000047683716 \n",
      "iteration:4319 loss:1.7191780805587769 \n",
      "iteration:4320 loss:1.725555181503296 \n",
      "iteration:4321 loss:1.7610349655151367 \n",
      "iteration:4322 loss:1.7698240280151367 \n",
      "iteration:4323 loss:1.7243772745132446 \n",
      "iteration:4324 loss:1.7693359851837158 \n",
      "iteration:4325 loss:1.789259433746338 \n",
      "iteration:4326 loss:1.7322747707366943 \n",
      "iteration:4327 loss:1.713169813156128 \n",
      "iteration:4328 loss:1.7497825622558594 \n",
      "iteration:4329 loss:1.7480312585830688 \n",
      "iteration:4330 loss:1.7748386859893799 \n",
      "iteration:4331 loss:1.7571011781692505 \n",
      "iteration:4332 loss:1.7531945705413818 \n",
      "iteration:4333 loss:1.7460612058639526 \n",
      "iteration:4334 loss:1.7268584966659546 \n",
      "iteration:4335 loss:1.7659800052642822 \n",
      "iteration:4336 loss:1.7440522909164429 \n",
      "iteration:4337 loss:1.7714860439300537 \n",
      "iteration:4338 loss:1.7552504539489746 \n",
      "iteration:4339 loss:1.7584389448165894 \n",
      "iteration:4340 loss:1.7443623542785645 \n",
      "iteration:4341 loss:1.7140485048294067 \n",
      "iteration:4342 loss:1.771897554397583 \n",
      "iteration:4343 loss:1.7833914756774902 \n",
      "iteration:4344 loss:1.7394452095031738 \n",
      "iteration:4345 loss:1.7953473329544067 \n",
      "iteration:4346 loss:1.7796248197555542 \n",
      "iteration:4347 loss:1.7827376127243042 \n",
      "iteration:4348 loss:1.7801960706710815 \n",
      "iteration:4349 loss:1.8671793937683105 \n",
      "Epoch-30 lr: 0.00034549150281252633\n",
      "iteration:4350 loss:1.7714380025863647 \n",
      "iteration:4351 loss:1.7734934091567993 \n",
      "iteration:4352 loss:1.7491981983184814 \n",
      "iteration:4353 loss:1.7194677591323853 \n",
      "iteration:4354 loss:1.7377426624298096 \n",
      "iteration:4355 loss:1.74250066280365 \n",
      "iteration:4356 loss:1.7242388725280762 \n",
      "iteration:4357 loss:1.7889760732650757 \n",
      "iteration:4358 loss:1.760040044784546 \n",
      "iteration:4359 loss:1.7521390914916992 \n",
      "iteration:4360 loss:1.6978693008422852 \n",
      "iteration:4361 loss:1.7466894388198853 \n",
      "iteration:4362 loss:1.7897379398345947 \n",
      "iteration:4363 loss:1.7536191940307617 \n",
      "iteration:4364 loss:1.743858814239502 \n",
      "iteration:4365 loss:1.7395460605621338 \n",
      "iteration:4366 loss:1.733279824256897 \n",
      "iteration:4367 loss:1.744832992553711 \n",
      "iteration:4368 loss:1.742895483970642 \n",
      "iteration:4369 loss:1.7332857847213745 \n",
      "iteration:4370 loss:1.7459391355514526 \n",
      "iteration:4371 loss:1.7394415140151978 \n",
      "iteration:4372 loss:1.733900547027588 \n",
      "iteration:4373 loss:1.7477344274520874 \n",
      "iteration:4374 loss:1.7670546770095825 \n",
      "iteration:4375 loss:1.6934679746627808 \n",
      "iteration:4376 loss:1.7526739835739136 \n",
      "iteration:4377 loss:1.8131394386291504 \n",
      "iteration:4378 loss:1.7176114320755005 \n",
      "iteration:4379 loss:1.7375582456588745 \n",
      "iteration:4380 loss:1.7237600088119507 \n",
      "iteration:4381 loss:1.6953850984573364 \n",
      "iteration:4382 loss:1.7222529649734497 \n",
      "iteration:4383 loss:1.7256115674972534 \n",
      "iteration:4384 loss:1.7431288957595825 \n",
      "iteration:4385 loss:1.7681770324707031 \n",
      "iteration:4386 loss:1.7340211868286133 \n",
      "iteration:4387 loss:1.779147744178772 \n",
      "iteration:4388 loss:1.7322676181793213 \n",
      "iteration:4389 loss:1.7576326131820679 \n",
      "iteration:4390 loss:1.7362135648727417 \n",
      "iteration:4391 loss:1.7216929197311401 \n",
      "iteration:4392 loss:1.748409390449524 \n",
      "iteration:4393 loss:1.7933052778244019 \n",
      "iteration:4394 loss:1.7597792148590088 \n",
      "iteration:4395 loss:1.7502663135528564 \n",
      "iteration:4396 loss:1.7608412504196167 \n",
      "iteration:4397 loss:1.7388495206832886 \n",
      "iteration:4398 loss:1.7197725772857666 \n",
      "iteration:4399 loss:1.745707631111145 \n",
      "iteration:4400 loss:1.7242414951324463 \n",
      "iteration:4401 loss:1.7923710346221924 \n",
      "iteration:4402 loss:1.762253999710083 \n",
      "iteration:4403 loss:1.7489532232284546 \n",
      "iteration:4404 loss:1.74250066280365 \n",
      "iteration:4405 loss:1.763147234916687 \n",
      "iteration:4406 loss:1.7566068172454834 \n",
      "iteration:4407 loss:1.7336270809173584 \n",
      "iteration:4408 loss:1.7063544988632202 \n",
      "iteration:4409 loss:1.7740044593811035 \n",
      "iteration:4410 loss:1.7361637353897095 \n",
      "iteration:4411 loss:1.7634505033493042 \n",
      "iteration:4412 loss:1.7598553895950317 \n",
      "iteration:4413 loss:1.7525887489318848 \n",
      "iteration:4414 loss:1.7355812788009644 \n",
      "iteration:4415 loss:1.725318193435669 \n",
      "iteration:4416 loss:1.7142488956451416 \n",
      "iteration:4417 loss:1.7526448965072632 \n",
      "iteration:4418 loss:1.7526723146438599 \n",
      "iteration:4419 loss:1.733188271522522 \n",
      "iteration:4420 loss:1.6853762865066528 \n",
      "iteration:4421 loss:1.7490063905715942 \n",
      "iteration:4422 loss:1.7665915489196777 \n",
      "iteration:4423 loss:1.7341971397399902 \n",
      "iteration:4424 loss:1.735959529876709 \n",
      "iteration:4425 loss:1.7231978178024292 \n",
      "iteration:4426 loss:1.7482643127441406 \n",
      "iteration:4427 loss:1.7273112535476685 \n",
      "iteration:4428 loss:1.7858471870422363 \n",
      "iteration:4429 loss:1.7231088876724243 \n",
      "iteration:4430 loss:1.7425235509872437 \n",
      "iteration:4431 loss:1.762033462524414 \n",
      "iteration:4432 loss:1.7187355756759644 \n",
      "iteration:4433 loss:1.7211016416549683 \n",
      "iteration:4434 loss:1.7988696098327637 \n",
      "iteration:4435 loss:1.7086023092269897 \n",
      "iteration:4436 loss:1.8036930561065674 \n",
      "iteration:4437 loss:1.7344529628753662 \n",
      "iteration:4438 loss:1.6873459815979004 \n",
      "iteration:4439 loss:1.754590630531311 \n",
      "iteration:4440 loss:1.7493104934692383 \n",
      "iteration:4441 loss:1.7452610731124878 \n",
      "iteration:4442 loss:1.7868361473083496 \n",
      "iteration:4443 loss:1.7640893459320068 \n",
      "iteration:4444 loss:1.7381608486175537 \n",
      "iteration:4445 loss:1.735538125038147 \n",
      "iteration:4446 loss:1.7146929502487183 \n",
      "iteration:4447 loss:1.8110734224319458 \n",
      "iteration:4448 loss:1.760850429534912 \n",
      "iteration:4449 loss:1.7982761859893799 \n",
      "iteration:4450 loss:1.7696654796600342 \n",
      "iteration:4451 loss:1.7459616661071777 \n",
      "iteration:4452 loss:1.7723599672317505 \n",
      "iteration:4453 loss:1.7208484411239624 \n",
      "iteration:4454 loss:1.7709580659866333 \n",
      "iteration:4455 loss:1.7648491859436035 \n",
      "iteration:4456 loss:1.7548274993896484 \n",
      "iteration:4457 loss:1.7872207164764404 \n",
      "iteration:4458 loss:1.7452032566070557 \n",
      "iteration:4459 loss:1.7488501071929932 \n",
      "iteration:4460 loss:1.674781322479248 \n",
      "iteration:4461 loss:1.7713326215744019 \n",
      "iteration:4462 loss:1.7578558921813965 \n",
      "iteration:4463 loss:1.746443510055542 \n",
      "iteration:4464 loss:1.7298656702041626 \n",
      "iteration:4465 loss:1.7216237783432007 \n",
      "iteration:4466 loss:1.738813042640686 \n",
      "iteration:4467 loss:1.7618424892425537 \n",
      "iteration:4468 loss:1.7804787158966064 \n",
      "iteration:4469 loss:1.7242279052734375 \n",
      "iteration:4470 loss:1.8073850870132446 \n",
      "iteration:4471 loss:1.7003934383392334 \n",
      "iteration:4472 loss:1.7541227340698242 \n",
      "iteration:4473 loss:1.7629390954971313 \n",
      "iteration:4474 loss:1.7780438661575317 \n",
      "iteration:4475 loss:1.734812617301941 \n",
      "iteration:4476 loss:1.748785376548767 \n",
      "iteration:4477 loss:1.7547051906585693 \n",
      "iteration:4478 loss:1.7658205032348633 \n",
      "iteration:4479 loss:1.7316365242004395 \n",
      "iteration:4480 loss:1.7313425540924072 \n",
      "iteration:4481 loss:1.749894618988037 \n",
      "iteration:4482 loss:1.7508904933929443 \n",
      "iteration:4483 loss:1.7420997619628906 \n",
      "iteration:4484 loss:1.7755014896392822 \n",
      "iteration:4485 loss:1.756347417831421 \n",
      "iteration:4486 loss:1.7309544086456299 \n",
      "iteration:4487 loss:1.7203352451324463 \n",
      "iteration:4488 loss:1.7563128471374512 \n",
      "iteration:4489 loss:1.7767748832702637 \n",
      "iteration:4490 loss:1.7156620025634766 \n",
      "iteration:4491 loss:1.7435870170593262 \n",
      "iteration:4492 loss:1.748497724533081 \n",
      "iteration:4493 loss:1.7473878860473633 \n",
      "iteration:4494 loss:1.9706931114196777 \n",
      "Epoch-31 lr: 0.00031593772365766105\n",
      "iteration:4495 loss:1.7341365814208984 \n",
      "iteration:4496 loss:1.7443461418151855 \n",
      "iteration:4497 loss:1.7326387166976929 \n",
      "iteration:4498 loss:1.7781994342803955 \n",
      "iteration:4499 loss:1.7482714653015137 \n",
      "iteration:4500 loss:1.751366376876831 \n",
      "iteration:4501 loss:1.7476381063461304 \n",
      "iteration:4502 loss:1.715067982673645 \n",
      "iteration:4503 loss:1.7301017045974731 \n",
      "iteration:4504 loss:1.7765529155731201 \n",
      "iteration:4505 loss:1.7893017530441284 \n",
      "iteration:4506 loss:1.7460390329360962 \n",
      "iteration:4507 loss:1.7663600444793701 \n",
      "iteration:4508 loss:1.7965962886810303 \n",
      "iteration:4509 loss:1.7688710689544678 \n",
      "iteration:4510 loss:1.7447054386138916 \n",
      "iteration:4511 loss:1.7421952486038208 \n",
      "iteration:4512 loss:1.7199835777282715 \n",
      "iteration:4513 loss:1.7711621522903442 \n",
      "iteration:4514 loss:1.7488240003585815 \n",
      "iteration:4515 loss:1.7296473979949951 \n",
      "iteration:4516 loss:1.7141982316970825 \n",
      "iteration:4517 loss:1.7472468614578247 \n",
      "iteration:4518 loss:1.7567297220230103 \n",
      "iteration:4519 loss:1.756486415863037 \n",
      "iteration:4520 loss:1.7100417613983154 \n",
      "iteration:4521 loss:1.748172640800476 \n",
      "iteration:4522 loss:1.7228137254714966 \n",
      "iteration:4523 loss:1.774088978767395 \n",
      "iteration:4524 loss:1.7176623344421387 \n",
      "iteration:4525 loss:1.736138939857483 \n",
      "iteration:4526 loss:1.8162058591842651 \n",
      "iteration:4527 loss:1.7484079599380493 \n",
      "iteration:4528 loss:1.7740246057510376 \n",
      "iteration:4529 loss:1.7315685749053955 \n",
      "iteration:4530 loss:1.766017198562622 \n",
      "iteration:4531 loss:1.7263633012771606 \n",
      "iteration:4532 loss:1.7389512062072754 \n",
      "iteration:4533 loss:1.7381421327590942 \n",
      "iteration:4534 loss:1.6932231187820435 \n",
      "iteration:4535 loss:1.7660974264144897 \n",
      "iteration:4536 loss:1.6939448118209839 \n",
      "iteration:4537 loss:1.7617322206497192 \n",
      "iteration:4538 loss:1.7745001316070557 \n",
      "iteration:4539 loss:1.7422983646392822 \n",
      "iteration:4540 loss:1.7651634216308594 \n",
      "iteration:4541 loss:1.7290818691253662 \n",
      "iteration:4542 loss:1.7743970155715942 \n",
      "iteration:4543 loss:1.741637945175171 \n",
      "iteration:4544 loss:1.7392492294311523 \n",
      "iteration:4545 loss:1.7089143991470337 \n",
      "iteration:4546 loss:1.729356050491333 \n",
      "iteration:4547 loss:1.733012080192566 \n",
      "iteration:4548 loss:1.7474528551101685 \n",
      "iteration:4549 loss:1.7138280868530273 \n",
      "iteration:4550 loss:1.746537446975708 \n",
      "iteration:4551 loss:1.7500463724136353 \n",
      "iteration:4552 loss:1.7423760890960693 \n",
      "iteration:4553 loss:1.7289811372756958 \n",
      "iteration:4554 loss:1.703584909439087 \n",
      "iteration:4555 loss:1.7311755418777466 \n",
      "iteration:4556 loss:1.7472692728042603 \n",
      "iteration:4557 loss:1.762515902519226 \n",
      "iteration:4558 loss:1.7031841278076172 \n",
      "iteration:4559 loss:1.7439030408859253 \n",
      "iteration:4560 loss:1.7374924421310425 \n",
      "iteration:4561 loss:1.7319563627243042 \n",
      "iteration:4562 loss:1.715010404586792 \n",
      "iteration:4563 loss:1.728678584098816 \n",
      "iteration:4564 loss:1.7199496030807495 \n",
      "iteration:4565 loss:1.7412229776382446 \n",
      "iteration:4566 loss:1.7043342590332031 \n",
      "iteration:4567 loss:1.7343908548355103 \n",
      "iteration:4568 loss:1.756883144378662 \n",
      "iteration:4569 loss:1.7087485790252686 \n",
      "iteration:4570 loss:1.7325358390808105 \n",
      "iteration:4571 loss:1.7541589736938477 \n",
      "iteration:4572 loss:1.7741351127624512 \n",
      "iteration:4573 loss:1.705214023590088 \n",
      "iteration:4574 loss:1.7593882083892822 \n",
      "iteration:4575 loss:1.7634426355361938 \n",
      "iteration:4576 loss:1.7280515432357788 \n",
      "iteration:4577 loss:1.765350341796875 \n",
      "iteration:4578 loss:1.7538769245147705 \n",
      "iteration:4579 loss:1.742191195487976 \n",
      "iteration:4580 loss:1.764329195022583 \n",
      "iteration:4581 loss:1.7406703233718872 \n",
      "iteration:4582 loss:1.7141001224517822 \n",
      "iteration:4583 loss:1.7722135782241821 \n",
      "iteration:4584 loss:1.7264262437820435 \n",
      "iteration:4585 loss:1.7669661045074463 \n",
      "iteration:4586 loss:1.7393670082092285 \n",
      "iteration:4587 loss:1.7547733783721924 \n",
      "iteration:4588 loss:1.7334085702896118 \n",
      "iteration:4589 loss:1.7221187353134155 \n",
      "iteration:4590 loss:1.7458839416503906 \n",
      "iteration:4591 loss:1.7489107847213745 \n",
      "iteration:4592 loss:1.7538295984268188 \n",
      "iteration:4593 loss:1.7264426946640015 \n",
      "iteration:4594 loss:1.6915855407714844 \n",
      "iteration:4595 loss:1.762119174003601 \n",
      "iteration:4596 loss:1.7379369735717773 \n",
      "iteration:4597 loss:1.7481309175491333 \n",
      "iteration:4598 loss:1.7298321723937988 \n",
      "iteration:4599 loss:1.7424895763397217 \n",
      "iteration:4600 loss:1.7058145999908447 \n",
      "iteration:4601 loss:1.734694004058838 \n",
      "iteration:4602 loss:1.768977403640747 \n",
      "iteration:4603 loss:1.733919620513916 \n",
      "iteration:4604 loss:1.75194251537323 \n",
      "iteration:4605 loss:1.7230383157730103 \n",
      "iteration:4606 loss:1.7693049907684326 \n",
      "iteration:4607 loss:1.7346194982528687 \n",
      "iteration:4608 loss:1.712233066558838 \n",
      "iteration:4609 loss:1.6907343864440918 \n",
      "iteration:4610 loss:1.7754170894622803 \n",
      "iteration:4611 loss:1.7980139255523682 \n",
      "iteration:4612 loss:1.758246660232544 \n",
      "iteration:4613 loss:1.7469482421875 \n",
      "iteration:4614 loss:1.7710188627243042 \n",
      "iteration:4615 loss:1.739640712738037 \n",
      "iteration:4616 loss:1.7419425249099731 \n",
      "iteration:4617 loss:1.7448736429214478 \n",
      "iteration:4618 loss:1.713399887084961 \n",
      "iteration:4619 loss:1.739068865776062 \n",
      "iteration:4620 loss:1.7195757627487183 \n",
      "iteration:4621 loss:1.7587502002716064 \n",
      "iteration:4622 loss:1.7313610315322876 \n",
      "iteration:4623 loss:1.7897251844406128 \n",
      "iteration:4624 loss:1.7586462497711182 \n",
      "iteration:4625 loss:1.7200528383255005 \n",
      "iteration:4626 loss:1.7474172115325928 \n",
      "iteration:4627 loss:1.7401295900344849 \n",
      "iteration:4628 loss:1.713991641998291 \n",
      "iteration:4629 loss:1.794453740119934 \n",
      "iteration:4630 loss:1.698643684387207 \n",
      "iteration:4631 loss:1.7216529846191406 \n",
      "iteration:4632 loss:1.7073237895965576 \n",
      "iteration:4633 loss:1.7983810901641846 \n",
      "iteration:4634 loss:1.7241058349609375 \n",
      "iteration:4635 loss:1.7236812114715576 \n",
      "iteration:4636 loss:1.7423700094223022 \n",
      "iteration:4637 loss:1.732262134552002 \n",
      "iteration:4638 loss:1.7771323919296265 \n",
      "iteration:4639 loss:1.9057399034500122 \n",
      "Epoch-32 lr: 0.00028711035421746355\n",
      "iteration:4640 loss:1.7431697845458984 \n",
      "iteration:4641 loss:1.710862398147583 \n",
      "iteration:4642 loss:1.7278358936309814 \n",
      "iteration:4643 loss:1.7124990224838257 \n",
      "iteration:4644 loss:1.7528266906738281 \n",
      "iteration:4645 loss:1.75957453250885 \n",
      "iteration:4646 loss:1.7710412740707397 \n",
      "iteration:4647 loss:1.7664777040481567 \n",
      "iteration:4648 loss:1.6779710054397583 \n",
      "iteration:4649 loss:1.705246925354004 \n",
      "iteration:4650 loss:1.727789282798767 \n",
      "iteration:4651 loss:1.7011131048202515 \n",
      "iteration:4652 loss:1.7240309715270996 \n",
      "iteration:4653 loss:1.7030280828475952 \n",
      "iteration:4654 loss:1.7245750427246094 \n",
      "iteration:4655 loss:1.698875069618225 \n",
      "iteration:4656 loss:1.737779974937439 \n",
      "iteration:4657 loss:1.7628898620605469 \n",
      "iteration:4658 loss:1.7444325685501099 \n",
      "iteration:4659 loss:1.7326855659484863 \n",
      "iteration:4660 loss:1.738786220550537 \n",
      "iteration:4661 loss:1.7326700687408447 \n",
      "iteration:4662 loss:1.7307385206222534 \n",
      "iteration:4663 loss:1.7304630279541016 \n",
      "iteration:4664 loss:1.7296136617660522 \n",
      "iteration:4665 loss:1.7347334623336792 \n",
      "iteration:4666 loss:1.772553563117981 \n",
      "iteration:4667 loss:1.6985511779785156 \n",
      "iteration:4668 loss:1.7627805471420288 \n",
      "iteration:4669 loss:1.76410710811615 \n",
      "iteration:4670 loss:1.7148022651672363 \n",
      "iteration:4671 loss:1.6901048421859741 \n",
      "iteration:4672 loss:1.7773417234420776 \n",
      "iteration:4673 loss:1.731713056564331 \n",
      "iteration:4674 loss:1.713036060333252 \n",
      "iteration:4675 loss:1.7506537437438965 \n",
      "iteration:4676 loss:1.7226271629333496 \n",
      "iteration:4677 loss:1.717612862586975 \n",
      "iteration:4678 loss:1.7322371006011963 \n",
      "iteration:4679 loss:1.7368229627609253 \n",
      "iteration:4680 loss:1.7302992343902588 \n",
      "iteration:4681 loss:1.724462866783142 \n",
      "iteration:4682 loss:1.7155643701553345 \n",
      "iteration:4683 loss:1.7624082565307617 \n",
      "iteration:4684 loss:1.782742977142334 \n",
      "iteration:4685 loss:1.7432550191879272 \n",
      "iteration:4686 loss:1.6844213008880615 \n",
      "iteration:4687 loss:1.7177152633666992 \n",
      "iteration:4688 loss:1.7327005863189697 \n",
      "iteration:4689 loss:1.7037492990493774 \n",
      "iteration:4690 loss:1.7611452341079712 \n",
      "iteration:4691 loss:1.7354851961135864 \n",
      "iteration:4692 loss:1.7288821935653687 \n",
      "iteration:4693 loss:1.757049798965454 \n",
      "iteration:4694 loss:1.7108036279678345 \n",
      "iteration:4695 loss:1.767852783203125 \n",
      "iteration:4696 loss:1.7922178506851196 \n",
      "iteration:4697 loss:1.7297636270523071 \n",
      "iteration:4698 loss:1.7397961616516113 \n",
      "iteration:4699 loss:1.7838008403778076 \n",
      "iteration:4700 loss:1.7346433401107788 \n",
      "iteration:4701 loss:1.727476954460144 \n",
      "iteration:4702 loss:1.7225555181503296 \n",
      "iteration:4703 loss:1.732539415359497 \n",
      "iteration:4704 loss:1.7172080278396606 \n",
      "iteration:4705 loss:1.7499867677688599 \n",
      "iteration:4706 loss:1.739465355873108 \n",
      "iteration:4707 loss:1.7396553754806519 \n",
      "iteration:4708 loss:1.7369502782821655 \n",
      "iteration:4709 loss:1.695568323135376 \n",
      "iteration:4710 loss:1.7375136613845825 \n",
      "iteration:4711 loss:1.7630165815353394 \n",
      "iteration:4712 loss:1.7148544788360596 \n",
      "iteration:4713 loss:1.7046501636505127 \n",
      "iteration:4714 loss:1.7262386083602905 \n",
      "iteration:4715 loss:1.7748427391052246 \n",
      "iteration:4716 loss:1.7267193794250488 \n",
      "iteration:4717 loss:1.7150222063064575 \n",
      "iteration:4718 loss:1.691723346710205 \n",
      "iteration:4719 loss:1.7144838571548462 \n",
      "iteration:4720 loss:1.7401494979858398 \n",
      "iteration:4721 loss:1.7175050973892212 \n",
      "iteration:4722 loss:1.723913550376892 \n",
      "iteration:4723 loss:1.7176856994628906 \n",
      "iteration:4724 loss:1.762628197669983 \n",
      "iteration:4725 loss:1.7687416076660156 \n",
      "iteration:4726 loss:1.741756796836853 \n",
      "iteration:4727 loss:1.7397501468658447 \n",
      "iteration:4728 loss:1.7720884084701538 \n",
      "iteration:4729 loss:1.7252016067504883 \n",
      "iteration:4730 loss:1.7388274669647217 \n",
      "iteration:4731 loss:1.7305914163589478 \n",
      "iteration:4732 loss:1.7393672466278076 \n",
      "iteration:4733 loss:1.7265506982803345 \n",
      "iteration:4734 loss:1.731278896331787 \n",
      "iteration:4735 loss:1.7239630222320557 \n",
      "iteration:4736 loss:1.7557533979415894 \n",
      "iteration:4737 loss:1.7751588821411133 \n",
      "iteration:4738 loss:1.7238171100616455 \n",
      "iteration:4739 loss:1.7559692859649658 \n",
      "iteration:4740 loss:1.780225396156311 \n",
      "iteration:4741 loss:1.7247185707092285 \n",
      "iteration:4742 loss:1.7403100728988647 \n",
      "iteration:4743 loss:1.747253656387329 \n",
      "iteration:4744 loss:1.733952283859253 \n",
      "iteration:4745 loss:1.707641839981079 \n",
      "iteration:4746 loss:1.7984627485275269 \n",
      "iteration:4747 loss:1.7368417978286743 \n",
      "iteration:4748 loss:1.7319560050964355 \n",
      "iteration:4749 loss:1.745201826095581 \n",
      "iteration:4750 loss:1.7848659753799438 \n",
      "iteration:4751 loss:1.7942633628845215 \n",
      "iteration:4752 loss:1.757694125175476 \n",
      "iteration:4753 loss:1.7092468738555908 \n",
      "iteration:4754 loss:1.7340433597564697 \n",
      "iteration:4755 loss:1.7121639251708984 \n",
      "iteration:4756 loss:1.7731971740722656 \n",
      "iteration:4757 loss:1.7083766460418701 \n",
      "iteration:4758 loss:1.7676739692687988 \n",
      "iteration:4759 loss:1.728037714958191 \n",
      "iteration:4760 loss:1.7521036863327026 \n",
      "iteration:4761 loss:1.744236707687378 \n",
      "iteration:4762 loss:1.7375956773757935 \n",
      "iteration:4763 loss:1.7861586809158325 \n",
      "iteration:4764 loss:1.774911880493164 \n",
      "iteration:4765 loss:1.7493878602981567 \n",
      "iteration:4766 loss:1.773407220840454 \n",
      "iteration:4767 loss:1.7327752113342285 \n",
      "iteration:4768 loss:1.7728354930877686 \n",
      "iteration:4769 loss:1.7336376905441284 \n",
      "iteration:4770 loss:1.7951312065124512 \n",
      "iteration:4771 loss:1.7275851964950562 \n",
      "iteration:4772 loss:1.7115637063980103 \n",
      "iteration:4773 loss:1.746982216835022 \n",
      "iteration:4774 loss:1.6884119510650635 \n",
      "iteration:4775 loss:1.7552905082702637 \n",
      "iteration:4776 loss:1.798386812210083 \n",
      "iteration:4777 loss:1.7399609088897705 \n",
      "iteration:4778 loss:1.7123736143112183 \n",
      "iteration:4779 loss:1.728124976158142 \n",
      "iteration:4780 loss:1.7068204879760742 \n",
      "iteration:4781 loss:1.7559785842895508 \n",
      "iteration:4782 loss:1.7738667726516724 \n",
      "iteration:4783 loss:1.7660280466079712 \n",
      "iteration:4784 loss:1.8013746738433838 \n",
      "Epoch-33 lr: 0.0002591231629491422\n",
      "iteration:4785 loss:1.7088686227798462 \n",
      "iteration:4786 loss:1.7782281637191772 \n",
      "iteration:4787 loss:1.7580456733703613 \n",
      "iteration:4788 loss:1.7421101331710815 \n",
      "iteration:4789 loss:1.750034213066101 \n",
      "iteration:4790 loss:1.7405861616134644 \n",
      "iteration:4791 loss:1.7491233348846436 \n",
      "iteration:4792 loss:1.7357637882232666 \n",
      "iteration:4793 loss:1.705556035041809 \n",
      "iteration:4794 loss:1.7299383878707886 \n",
      "iteration:4795 loss:1.7317852973937988 \n",
      "iteration:4796 loss:1.7757295370101929 \n",
      "iteration:4797 loss:1.7149332761764526 \n",
      "iteration:4798 loss:1.7278839349746704 \n",
      "iteration:4799 loss:1.7085752487182617 \n",
      "iteration:4800 loss:1.7838637828826904 \n",
      "iteration:4801 loss:1.7239906787872314 \n",
      "iteration:4802 loss:1.7235193252563477 \n",
      "iteration:4803 loss:1.7869616746902466 \n",
      "iteration:4804 loss:1.7168642282485962 \n",
      "iteration:4805 loss:1.7203657627105713 \n",
      "iteration:4806 loss:1.7234816551208496 \n",
      "iteration:4807 loss:1.6915446519851685 \n",
      "iteration:4808 loss:1.6864047050476074 \n",
      "iteration:4809 loss:1.7150455713272095 \n",
      "iteration:4810 loss:1.7481119632720947 \n",
      "iteration:4811 loss:1.8142277002334595 \n",
      "iteration:4812 loss:1.7615609169006348 \n",
      "iteration:4813 loss:1.6986639499664307 \n",
      "iteration:4814 loss:1.7690236568450928 \n",
      "iteration:4815 loss:1.7435014247894287 \n",
      "iteration:4816 loss:1.7327553033828735 \n",
      "iteration:4817 loss:1.7025632858276367 \n",
      "iteration:4818 loss:1.7614319324493408 \n",
      "iteration:4819 loss:1.7051050662994385 \n",
      "iteration:4820 loss:1.7180157899856567 \n",
      "iteration:4821 loss:1.6928521394729614 \n",
      "iteration:4822 loss:1.685642957687378 \n",
      "iteration:4823 loss:1.7689038515090942 \n",
      "iteration:4824 loss:1.7367384433746338 \n",
      "iteration:4825 loss:1.7154256105422974 \n",
      "iteration:4826 loss:1.7074322700500488 \n",
      "iteration:4827 loss:1.7064300775527954 \n",
      "iteration:4828 loss:1.7346338033676147 \n",
      "iteration:4829 loss:1.797187089920044 \n",
      "iteration:4830 loss:1.6804805994033813 \n",
      "iteration:4831 loss:1.7651946544647217 \n",
      "iteration:4832 loss:1.691252589225769 \n",
      "iteration:4833 loss:1.7220577001571655 \n",
      "iteration:4834 loss:1.7202634811401367 \n",
      "iteration:4835 loss:1.7509719133377075 \n",
      "iteration:4836 loss:1.739399790763855 \n",
      "iteration:4837 loss:1.6868326663970947 \n",
      "iteration:4838 loss:1.7502363920211792 \n",
      "iteration:4839 loss:1.7229524850845337 \n",
      "iteration:4840 loss:1.7521733045578003 \n",
      "iteration:4841 loss:1.7203150987625122 \n",
      "iteration:4842 loss:1.7721604108810425 \n",
      "iteration:4843 loss:1.734818696975708 \n",
      "iteration:4844 loss:1.7926361560821533 \n",
      "iteration:4845 loss:1.7224135398864746 \n",
      "iteration:4846 loss:1.7540521621704102 \n",
      "iteration:4847 loss:1.720255970954895 \n",
      "iteration:4848 loss:1.7261275053024292 \n",
      "iteration:4849 loss:1.720682978630066 \n",
      "iteration:4850 loss:1.7462742328643799 \n",
      "iteration:4851 loss:1.745124340057373 \n",
      "iteration:4852 loss:1.7418397665023804 \n",
      "iteration:4853 loss:1.7043557167053223 \n",
      "iteration:4854 loss:1.702915906906128 \n",
      "iteration:4855 loss:1.7272981405258179 \n",
      "iteration:4856 loss:1.7209919691085815 \n",
      "iteration:4857 loss:1.733588457107544 \n",
      "iteration:4858 loss:1.7354344129562378 \n",
      "iteration:4859 loss:1.7102850675582886 \n",
      "iteration:4860 loss:1.7410707473754883 \n",
      "iteration:4861 loss:1.7178155183792114 \n",
      "iteration:4862 loss:1.732689380645752 \n",
      "iteration:4863 loss:1.742946982383728 \n",
      "iteration:4864 loss:1.734156847000122 \n",
      "iteration:4865 loss:1.7054728269577026 \n",
      "iteration:4866 loss:1.7142518758773804 \n",
      "iteration:4867 loss:1.7165489196777344 \n",
      "iteration:4868 loss:1.7181341648101807 \n",
      "iteration:4869 loss:1.7316606044769287 \n",
      "iteration:4870 loss:1.7282179594039917 \n",
      "iteration:4871 loss:1.6715445518493652 \n",
      "iteration:4872 loss:1.762302041053772 \n",
      "iteration:4873 loss:1.7703888416290283 \n",
      "iteration:4874 loss:1.7323046922683716 \n",
      "iteration:4875 loss:1.6926814317703247 \n",
      "iteration:4876 loss:1.7142425775527954 \n",
      "iteration:4877 loss:1.7446842193603516 \n",
      "iteration:4878 loss:1.7081589698791504 \n",
      "iteration:4879 loss:1.7113938331604004 \n",
      "iteration:4880 loss:1.6957956552505493 \n",
      "iteration:4881 loss:1.7154738903045654 \n",
      "iteration:4882 loss:1.7214027643203735 \n",
      "iteration:4883 loss:1.7080490589141846 \n",
      "iteration:4884 loss:1.8051127195358276 \n",
      "iteration:4885 loss:1.7238023281097412 \n",
      "iteration:4886 loss:1.7325960397720337 \n",
      "iteration:4887 loss:1.7267885208129883 \n",
      "iteration:4888 loss:1.7446695566177368 \n",
      "iteration:4889 loss:1.7377175092697144 \n",
      "iteration:4890 loss:1.7271918058395386 \n",
      "iteration:4891 loss:1.7076133489608765 \n",
      "iteration:4892 loss:1.7276780605316162 \n",
      "iteration:4893 loss:1.748878836631775 \n",
      "iteration:4894 loss:1.6994506120681763 \n",
      "iteration:4895 loss:1.7075468301773071 \n",
      "iteration:4896 loss:1.7539379596710205 \n",
      "iteration:4897 loss:1.7210713624954224 \n",
      "iteration:4898 loss:1.749243140220642 \n",
      "iteration:4899 loss:1.7583330869674683 \n",
      "iteration:4900 loss:1.738189458847046 \n",
      "iteration:4901 loss:1.7705509662628174 \n",
      "iteration:4902 loss:1.7444671392440796 \n",
      "iteration:4903 loss:1.774909257888794 \n",
      "iteration:4904 loss:1.7246545553207397 \n",
      "iteration:4905 loss:1.7130578756332397 \n",
      "iteration:4906 loss:1.7114850282669067 \n",
      "iteration:4907 loss:1.749793529510498 \n",
      "iteration:4908 loss:1.6771858930587769 \n",
      "iteration:4909 loss:1.718000888824463 \n",
      "iteration:4910 loss:1.7674843072891235 \n",
      "iteration:4911 loss:1.7467904090881348 \n",
      "iteration:4912 loss:1.7709275484085083 \n",
      "iteration:4913 loss:1.7532873153686523 \n",
      "iteration:4914 loss:1.7814668416976929 \n",
      "iteration:4915 loss:1.7400981187820435 \n",
      "iteration:4916 loss:1.701837420463562 \n",
      "iteration:4917 loss:1.746335506439209 \n",
      "iteration:4918 loss:1.748362421989441 \n",
      "iteration:4919 loss:1.7278560400009155 \n",
      "iteration:4920 loss:1.7075073719024658 \n",
      "iteration:4921 loss:1.7412147521972656 \n",
      "iteration:4922 loss:1.6906894445419312 \n",
      "iteration:4923 loss:1.7113468647003174 \n",
      "iteration:4924 loss:1.7059073448181152 \n",
      "iteration:4925 loss:1.7100346088409424 \n",
      "iteration:4926 loss:1.7103829383850098 \n",
      "iteration:4927 loss:1.70126473903656 \n",
      "iteration:4928 loss:1.7538803815841675 \n",
      "iteration:4929 loss:1.7899092435836792 \n",
      "Epoch-34 lr: 0.00023208660251050145\n",
      "iteration:4930 loss:1.7339457273483276 \n",
      "iteration:4931 loss:1.759853720664978 \n",
      "iteration:4932 loss:1.7092357873916626 \n",
      "iteration:4933 loss:1.6758522987365723 \n",
      "iteration:4934 loss:1.7143828868865967 \n",
      "iteration:4935 loss:1.7483339309692383 \n",
      "iteration:4936 loss:1.7100040912628174 \n",
      "iteration:4937 loss:1.7297027111053467 \n",
      "iteration:4938 loss:1.7569818496704102 \n",
      "iteration:4939 loss:1.694044828414917 \n",
      "iteration:4940 loss:1.7022582292556763 \n",
      "iteration:4941 loss:1.7692983150482178 \n",
      "iteration:4942 loss:1.7364643812179565 \n",
      "iteration:4943 loss:1.693543791770935 \n",
      "iteration:4944 loss:1.7081522941589355 \n",
      "iteration:4945 loss:1.6863036155700684 \n",
      "iteration:4946 loss:1.7100014686584473 \n",
      "iteration:4947 loss:1.7018529176712036 \n",
      "iteration:4948 loss:1.7516355514526367 \n",
      "iteration:4949 loss:1.7484585046768188 \n",
      "iteration:4950 loss:1.7149180173873901 \n",
      "iteration:4951 loss:1.7744636535644531 \n",
      "iteration:4952 loss:1.7490979433059692 \n",
      "iteration:4953 loss:1.726470947265625 \n",
      "iteration:4954 loss:1.7566837072372437 \n",
      "iteration:4955 loss:1.747740626335144 \n",
      "iteration:4956 loss:1.7296864986419678 \n",
      "iteration:4957 loss:1.7020143270492554 \n",
      "iteration:4958 loss:1.7356858253479004 \n",
      "iteration:4959 loss:1.7049640417099 \n",
      "iteration:4960 loss:1.7723504304885864 \n",
      "iteration:4961 loss:1.7545404434204102 \n",
      "iteration:4962 loss:1.686669111251831 \n",
      "iteration:4963 loss:1.753628134727478 \n",
      "iteration:4964 loss:1.776646375656128 \n",
      "iteration:4965 loss:1.7569637298583984 \n",
      "iteration:4966 loss:1.6857377290725708 \n",
      "iteration:4967 loss:1.743414282798767 \n",
      "iteration:4968 loss:1.7182856798171997 \n",
      "iteration:4969 loss:1.76727294921875 \n",
      "iteration:4970 loss:1.7431070804595947 \n",
      "iteration:4971 loss:1.7180564403533936 \n",
      "iteration:4972 loss:1.7494357824325562 \n",
      "iteration:4973 loss:1.7261114120483398 \n",
      "iteration:4974 loss:1.7006756067276 \n",
      "iteration:4975 loss:1.690201997756958 \n",
      "iteration:4976 loss:1.7193355560302734 \n",
      "iteration:4977 loss:1.7292615175247192 \n",
      "iteration:4978 loss:1.7812258005142212 \n",
      "iteration:4979 loss:1.6853467226028442 \n",
      "iteration:4980 loss:1.7209365367889404 \n",
      "iteration:4981 loss:1.716596245765686 \n",
      "iteration:4982 loss:1.661480188369751 \n",
      "iteration:4983 loss:1.7346073389053345 \n",
      "iteration:4984 loss:1.7242368459701538 \n",
      "iteration:4985 loss:1.7621554136276245 \n",
      "iteration:4986 loss:1.7713879346847534 \n",
      "iteration:4987 loss:1.671324610710144 \n",
      "iteration:4988 loss:1.7156602144241333 \n",
      "iteration:4989 loss:1.7029310464859009 \n",
      "iteration:4990 loss:1.7430572509765625 \n",
      "iteration:4991 loss:1.759097695350647 \n",
      "iteration:4992 loss:1.7356363534927368 \n",
      "iteration:4993 loss:1.7443740367889404 \n",
      "iteration:4994 loss:1.7093021869659424 \n",
      "iteration:4995 loss:1.7241452932357788 \n",
      "iteration:4996 loss:1.702386498451233 \n",
      "iteration:4997 loss:1.750471830368042 \n",
      "iteration:4998 loss:1.7721059322357178 \n",
      "iteration:4999 loss:1.7243247032165527 \n",
      "iteration:5000 loss:1.6984237432479858 \n",
      "iteration:5001 loss:1.7205703258514404 \n",
      "iteration:5002 loss:1.6859612464904785 \n",
      "iteration:5003 loss:1.7371901273727417 \n",
      "iteration:5004 loss:1.7234612703323364 \n",
      "iteration:5005 loss:1.713918685913086 \n",
      "iteration:5006 loss:1.7620769739151 \n",
      "iteration:5007 loss:1.6895945072174072 \n",
      "iteration:5008 loss:1.6875402927398682 \n",
      "iteration:5009 loss:1.6800925731658936 \n",
      "iteration:5010 loss:1.767027735710144 \n",
      "iteration:5011 loss:1.7215282917022705 \n",
      "iteration:5012 loss:1.6981202363967896 \n",
      "iteration:5013 loss:1.7120481729507446 \n",
      "iteration:5014 loss:1.7237094640731812 \n",
      "iteration:5015 loss:1.7463042736053467 \n",
      "iteration:5016 loss:1.763745903968811 \n",
      "iteration:5017 loss:1.732076644897461 \n",
      "iteration:5018 loss:1.706878900527954 \n",
      "iteration:5019 loss:1.6926106214523315 \n",
      "iteration:5020 loss:1.741047978401184 \n",
      "iteration:5021 loss:1.7662345170974731 \n",
      "iteration:5022 loss:1.7358392477035522 \n",
      "iteration:5023 loss:1.7339054346084595 \n",
      "iteration:5024 loss:1.7442106008529663 \n",
      "iteration:5025 loss:1.687334656715393 \n",
      "iteration:5026 loss:1.754787802696228 \n",
      "iteration:5027 loss:1.7352358102798462 \n",
      "iteration:5028 loss:1.7117444276809692 \n",
      "iteration:5029 loss:1.70610511302948 \n",
      "iteration:5030 loss:1.7342993021011353 \n",
      "iteration:5031 loss:1.7166410684585571 \n",
      "iteration:5032 loss:1.7466267347335815 \n",
      "iteration:5033 loss:1.7529115676879883 \n",
      "iteration:5034 loss:1.7302602529525757 \n",
      "iteration:5035 loss:1.7510203123092651 \n",
      "iteration:5036 loss:1.7217292785644531 \n",
      "iteration:5037 loss:1.7348943948745728 \n",
      "iteration:5038 loss:1.6885164976119995 \n",
      "iteration:5039 loss:1.780855655670166 \n",
      "iteration:5040 loss:1.707060694694519 \n",
      "iteration:5041 loss:1.7228655815124512 \n",
      "iteration:5042 loss:1.7574259042739868 \n",
      "iteration:5043 loss:1.7316595315933228 \n",
      "iteration:5044 loss:1.7366414070129395 \n",
      "iteration:5045 loss:1.7256282567977905 \n",
      "iteration:5046 loss:1.7747007608413696 \n",
      "iteration:5047 loss:1.745802640914917 \n",
      "iteration:5048 loss:1.7515671253204346 \n",
      "iteration:5049 loss:1.7198750972747803 \n",
      "iteration:5050 loss:1.7361613512039185 \n",
      "iteration:5051 loss:1.7002289295196533 \n",
      "iteration:5052 loss:1.7355525493621826 \n",
      "iteration:5053 loss:1.7684277296066284 \n",
      "iteration:5054 loss:1.74555504322052 \n",
      "iteration:5055 loss:1.7511122226715088 \n",
      "iteration:5056 loss:1.7441304922103882 \n",
      "iteration:5057 loss:1.7180110216140747 \n",
      "iteration:5058 loss:1.7039637565612793 \n",
      "iteration:5059 loss:1.7381824254989624 \n",
      "iteration:5060 loss:1.7276155948638916 \n",
      "iteration:5061 loss:1.727738857269287 \n",
      "iteration:5062 loss:1.6979213953018188 \n",
      "iteration:5063 loss:1.698717474937439 \n",
      "iteration:5064 loss:1.710977554321289 \n",
      "iteration:5065 loss:1.7214795351028442 \n",
      "iteration:5066 loss:1.7103344202041626 \n",
      "iteration:5067 loss:1.7576788663864136 \n",
      "iteration:5068 loss:1.7019705772399902 \n",
      "iteration:5069 loss:1.739234447479248 \n",
      "iteration:5070 loss:1.7407824993133545 \n",
      "iteration:5071 loss:1.7210426330566406 \n",
      "iteration:5072 loss:1.7506310939788818 \n",
      "iteration:5073 loss:1.698785662651062 \n",
      "iteration:5074 loss:1.6985194683074951 \n",
      "Epoch-35 lr: 0.00020610737385376337\n",
      "iteration:5075 loss:1.6854920387268066 \n",
      "iteration:5076 loss:1.7453299760818481 \n",
      "iteration:5077 loss:1.7766345739364624 \n",
      "iteration:5078 loss:1.720126748085022 \n"
     ]
    }
   ],
   "source": [
    "from featureblocks import FeatureBlock3\n",
    "# The trinity of models\n",
    "model = FeatureBlock3()\n",
    "#model = FeatureBlockGT()\n",
    "# This is the losss function\n",
    "#loss_function = RMSLELoss()\n",
    "loss_function = torch.nn.CrossEntropyLoss()\n",
    "# This is what controls the gradient descent\n",
    "#optimizer = torch.optim.Adadelta(model.parameters(),lr=0.00001)\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=0.001)\n",
    "#scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=2000, gamma=0.1)\n",
    "#scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=2, threshold=0.0001, threshold_mode='rel', cooldown=0,min_lr=0, eps=1e-08, verbose=False)\n",
    "T_max = 50\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max)\n",
    "iteration = 0\n",
    "losses = []\n",
    "\n",
    "\n",
    "#wandb.init(project='end2end1D')\n",
    "#config = wandb.config\n",
    "\n",
    "#wandb.watch(model)\n",
    "#model.train()\n",
    "\n",
    "for epoch in range(100):\n",
    "    \n",
    "    print('Epoch-{0} lr: {1}'.format(epoch, optimizer.param_groups[0]['lr']))\n",
    "    \n",
    "    for index,(x,y) in enumerate(audio_dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        #print(x.size())\n",
    "        x = model(x.float())\n",
    "        #print(x.size())\n",
    "        #print(y.size())\n",
    "        # Use argmax to get class with max probability value from softmax\n",
    "        #x = x.argmax(dim=-1) \n",
    "        x = x.float()\n",
    "        y = y.squeeze(1)\n",
    "        \n",
    "        loss = loss_function(x,y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        print(\"iteration:{} loss:{} \".format(iteration, loss.item()))\n",
    "        losses.append(loss)\n",
    "        iteration += 1\n",
    "    scheduler.step()\n",
    "        \n",
    "        #wandb.log({\"loss\": loss, \"epoch\": epoch})\n",
    "        \n",
    "        \n",
    "#using wandb to visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shape = [16,16,64]\n",
    "filters = generate_filters(shape[2],shape[0],sample_rate,min_center_freq,order)\n",
    "filters = filters.reshape(filters.shape[1],1,filters.shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-aceb45ca6d43>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlosses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlosses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "y = np.array(losses, dtype=float)\n",
    "x = np.arange(len(losses))\n",
    "\n",
    "plt.xlabel(\"iteration\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.suptitle(\"CrossEntropy loss f, Adam optimizer, lr=variable, epochs=100, batch-size=300\", fontsize=9)\n",
    "\n",
    "m, b = np.polyfit(x, y, 1)\n",
    "plt.plot(x, y, 'o')\n",
    "plt.plot(x, m*x+b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
