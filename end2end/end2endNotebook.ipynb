{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import librosa\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from model import EnvNet\n",
    "from train import train_model\n",
    "from data_preprocess import make_frames,make_frames_folder\n",
    "from tensorflow.keras.models import Sequential,Model\n",
    "from tensorflow.keras.layers import Conv1D,MaxPooling1D,Dense,Flatten,BatchNormalization,Dropout, Activation\n",
    "from gammatone_init import GammatoneInit\n",
    "from gammatone_init import generate_filters\n",
    "from model_config import *\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import pandas as pd\n",
    "import ast\n",
    "import wandb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([54033, 16001])\n"
     ]
    }
   ],
   "source": [
    "frame_length = 16000\n",
    "overlapping_fraction = 0.5\n",
    "data = torch.load('./torch_dataset_16khz/all_audio_data.pt')\n",
    "print(data.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([54033, 16001])\n",
      "54033\n"
     ]
    }
   ],
   "source": [
    "def to_categorical(tensors, num_classes=10):\n",
    "    return torch.eye(num_classes)[y.int()]\n",
    "print(data.size())\n",
    "tensor_size = (data.size())[0]\n",
    "print(tensor_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43226\n",
      "torch.Size([43226, 16000])\n",
      "torch.Size([43226, 16, 1000])\n",
      "torch.Size([43226, 1])\n",
      "tensor([[3.],\n",
      "        [3.],\n",
      "        [3.],\n",
      "        ...,\n",
      "        [2.],\n",
      "        [2.],\n",
      "        [2.]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "X_train = data[:,0:frame_length].clone()\n",
    "test_portion = int(0.8*((data.size())[0]))\n",
    "print(test_portion)\n",
    "X_train = data[:test_portion, 0:frame_length].clone()\n",
    "print(X_train.size())\n",
    "Y_train = data[:test_portion,frame_length:].clone()\n",
    "\n",
    "X_train = X_train.reshape(-1,16,1000)\n",
    "\n",
    "print(X_train.size())\n",
    "print(Y_train.size())\n",
    "print(Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3],\n",
      "        [3],\n",
      "        [3],\n",
      "        ...,\n",
      "        [2],\n",
      "        [2],\n",
      "        [2]])\n"
     ]
    }
   ],
   "source": [
    "Y_train = Y_train.type(torch.LongTensor)\n",
    "Y_train_one_hot = F.one_hot(Y_train)\n",
    "#print(Y_train_one_hot)\n",
    "print(Y_train)\n",
    "audio_dataset = TensorDataset (X_train, Y_train)\n",
    "audio_dataloader = DataLoader (audio_dataset, batch_size = 100, shuffle= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_rate = 16000\n",
    "min_center_freq = 100\n",
    "order = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMSLELoss(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.mse = torch.nn.MSELoss()\n",
    "        \n",
    "    def forward(self, pred, actual):\n",
    "        return self.mse(torch.log(pred.float() + 1), torch.log(actual.float() + 1))\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 10])\n",
      "torch.Size([100])\n"
     ]
    }
   ],
   "source": [
    "loss_function = torch.nn.CrossEntropyLoss()\n",
    "input = torch.randn(100, 10, requires_grad=True)\n",
    "target = torch.empty(100, dtype=torch.long).random_(5)\n",
    "print(input.size())\n",
    "print(target.size())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration:0 loss:2.306471824645996 \n",
      "iteration:1 loss:2.3028924465179443 \n",
      "iteration:2 loss:2.300689458847046 \n",
      "iteration:3 loss:2.302952527999878 \n",
      "iteration:4 loss:2.2994871139526367 \n",
      "iteration:5 loss:2.3021552562713623 \n",
      "iteration:6 loss:2.2985341548919678 \n",
      "iteration:7 loss:2.2980194091796875 \n",
      "iteration:8 loss:2.2954187393188477 \n",
      "iteration:9 loss:2.2970099449157715 \n",
      "iteration:10 loss:2.2973315715789795 \n",
      "iteration:11 loss:2.2904937267303467 \n",
      "iteration:12 loss:2.2926065921783447 \n",
      "iteration:13 loss:2.2910759449005127 \n",
      "iteration:14 loss:2.289609432220459 \n",
      "iteration:15 loss:2.28837513923645 \n",
      "iteration:16 loss:2.282341957092285 \n",
      "iteration:17 loss:2.283125400543213 \n",
      "iteration:18 loss:2.2791924476623535 \n",
      "iteration:19 loss:2.2787892818450928 \n",
      "iteration:20 loss:2.2827463150024414 \n",
      "iteration:21 loss:2.261875629425049 \n",
      "iteration:22 loss:2.275815010070801 \n",
      "iteration:23 loss:2.2833023071289062 \n",
      "iteration:24 loss:2.2808725833892822 \n",
      "iteration:25 loss:2.2615749835968018 \n",
      "iteration:26 loss:2.2475621700286865 \n",
      "iteration:27 loss:2.263596534729004 \n",
      "iteration:28 loss:2.2367818355560303 \n",
      "iteration:29 loss:2.2497334480285645 \n",
      "iteration:30 loss:2.2271406650543213 \n",
      "iteration:31 loss:2.2419681549072266 \n",
      "iteration:32 loss:2.2701573371887207 \n",
      "iteration:33 loss:2.2573232650756836 \n",
      "iteration:34 loss:2.237865686416626 \n",
      "iteration:35 loss:2.2377755641937256 \n",
      "iteration:36 loss:2.2454679012298584 \n",
      "iteration:37 loss:2.236685276031494 \n",
      "iteration:38 loss:2.2558486461639404 \n",
      "iteration:39 loss:2.2097368240356445 \n",
      "iteration:40 loss:2.229295492172241 \n",
      "iteration:41 loss:2.263571262359619 \n",
      "iteration:42 loss:2.2232322692871094 \n",
      "iteration:43 loss:2.219280481338501 \n",
      "iteration:44 loss:2.2583532333374023 \n",
      "iteration:45 loss:2.1837263107299805 \n",
      "iteration:46 loss:2.2360572814941406 \n",
      "iteration:47 loss:2.157212734222412 \n",
      "iteration:48 loss:2.274130344390869 \n",
      "iteration:49 loss:2.243393659591675 \n",
      "iteration:50 loss:2.2643349170684814 \n",
      "iteration:51 loss:2.217379331588745 \n",
      "iteration:52 loss:2.2368247509002686 \n",
      "iteration:53 loss:2.2083511352539062 \n",
      "iteration:54 loss:2.181260824203491 \n",
      "iteration:55 loss:2.202390193939209 \n",
      "iteration:56 loss:2.2362289428710938 \n",
      "iteration:57 loss:2.2082247734069824 \n",
      "iteration:58 loss:2.186459541320801 \n",
      "iteration:59 loss:2.195525884628296 \n",
      "iteration:60 loss:2.2087225914001465 \n",
      "iteration:61 loss:2.220829725265503 \n",
      "iteration:62 loss:2.1924190521240234 \n",
      "iteration:63 loss:2.2821059226989746 \n",
      "iteration:64 loss:2.170958995819092 \n",
      "iteration:65 loss:2.18601131439209 \n",
      "iteration:66 loss:2.2303433418273926 \n",
      "iteration:67 loss:2.181394100189209 \n",
      "iteration:68 loss:2.24088978767395 \n",
      "iteration:69 loss:2.191283941268921 \n",
      "iteration:70 loss:2.180983781814575 \n",
      "iteration:71 loss:2.222230911254883 \n",
      "iteration:72 loss:2.1753787994384766 \n",
      "iteration:73 loss:2.1728029251098633 \n",
      "iteration:74 loss:2.159226179122925 \n",
      "iteration:75 loss:2.2378623485565186 \n",
      "iteration:76 loss:2.158229112625122 \n",
      "iteration:77 loss:2.236666679382324 \n",
      "iteration:78 loss:2.2309916019439697 \n",
      "iteration:79 loss:2.2166342735290527 \n",
      "iteration:80 loss:2.2094016075134277 \n",
      "iteration:81 loss:2.218698024749756 \n",
      "iteration:82 loss:2.1779210567474365 \n",
      "iteration:83 loss:2.2094342708587646 \n",
      "iteration:84 loss:2.1201071739196777 \n",
      "iteration:85 loss:2.201725959777832 \n",
      "iteration:86 loss:2.1539862155914307 \n",
      "iteration:87 loss:2.2160773277282715 \n",
      "iteration:88 loss:2.1047415733337402 \n",
      "iteration:89 loss:2.214479446411133 \n",
      "iteration:90 loss:2.137434482574463 \n",
      "iteration:91 loss:2.170475959777832 \n",
      "iteration:92 loss:2.1138057708740234 \n",
      "iteration:93 loss:2.165081262588501 \n",
      "iteration:94 loss:2.118820905685425 \n",
      "iteration:95 loss:2.1583664417266846 \n",
      "iteration:96 loss:2.194403648376465 \n",
      "iteration:97 loss:2.160935163497925 \n",
      "iteration:98 loss:2.192793369293213 \n",
      "iteration:99 loss:2.170494556427002 \n",
      "iteration:100 loss:2.151737689971924 \n",
      "iteration:101 loss:2.200730323791504 \n",
      "iteration:102 loss:2.1851820945739746 \n",
      "iteration:103 loss:2.086834669113159 \n",
      "iteration:104 loss:2.1604535579681396 \n",
      "iteration:105 loss:2.1484591960906982 \n",
      "iteration:106 loss:2.2494921684265137 \n",
      "iteration:107 loss:2.1359920501708984 \n",
      "iteration:108 loss:2.150545597076416 \n",
      "iteration:109 loss:2.176821708679199 \n",
      "iteration:110 loss:2.2114086151123047 \n",
      "iteration:111 loss:2.225726842880249 \n",
      "iteration:112 loss:2.221090316772461 \n",
      "iteration:113 loss:2.214106559753418 \n",
      "iteration:114 loss:2.1103594303131104 \n",
      "iteration:115 loss:2.175482749938965 \n",
      "iteration:116 loss:2.137070894241333 \n",
      "iteration:117 loss:2.1628408432006836 \n",
      "iteration:118 loss:2.1713552474975586 \n",
      "iteration:119 loss:2.1976287364959717 \n",
      "iteration:120 loss:2.1511964797973633 \n",
      "iteration:121 loss:2.1666693687438965 \n",
      "iteration:122 loss:2.1228113174438477 \n",
      "iteration:123 loss:2.1739134788513184 \n",
      "iteration:124 loss:2.134040594100952 \n",
      "iteration:125 loss:2.0926668643951416 \n",
      "iteration:126 loss:2.1512510776519775 \n",
      "iteration:127 loss:2.150827169418335 \n",
      "iteration:128 loss:2.1277577877044678 \n",
      "iteration:129 loss:2.1395368576049805 \n",
      "iteration:130 loss:2.1017215251922607 \n",
      "iteration:131 loss:2.0877764225006104 \n",
      "iteration:132 loss:2.2356185913085938 \n",
      "iteration:133 loss:2.1723122596740723 \n",
      "iteration:134 loss:2.1364333629608154 \n",
      "iteration:135 loss:2.1716411113739014 \n",
      "iteration:136 loss:2.142481803894043 \n",
      "iteration:137 loss:2.0994529724121094 \n",
      "iteration:138 loss:2.135300397872925 \n",
      "iteration:139 loss:2.1453676223754883 \n",
      "iteration:140 loss:2.125784158706665 \n",
      "iteration:141 loss:2.134117841720581 \n",
      "iteration:142 loss:2.1823790073394775 \n",
      "iteration:143 loss:2.1699912548065186 \n",
      "iteration:144 loss:2.167050361633301 \n",
      "iteration:145 loss:2.14851450920105 \n",
      "iteration:146 loss:2.1564059257507324 \n",
      "iteration:147 loss:2.1368722915649414 \n",
      "iteration:148 loss:2.125147819519043 \n",
      "iteration:149 loss:2.162132978439331 \n",
      "iteration:150 loss:2.1231539249420166 \n",
      "iteration:151 loss:2.132397413253784 \n",
      "iteration:152 loss:2.086735725402832 \n",
      "iteration:153 loss:2.118741750717163 \n",
      "iteration:154 loss:2.0965025424957275 \n",
      "iteration:155 loss:2.1532042026519775 \n",
      "iteration:156 loss:2.069246530532837 \n",
      "iteration:157 loss:2.1166210174560547 \n",
      "iteration:158 loss:2.132864475250244 \n",
      "iteration:159 loss:2.161886215209961 \n",
      "iteration:160 loss:2.1988837718963623 \n",
      "iteration:161 loss:2.1254284381866455 \n",
      "iteration:162 loss:2.138040542602539 \n",
      "iteration:163 loss:2.099212408065796 \n",
      "iteration:164 loss:2.154592275619507 \n",
      "iteration:165 loss:2.203730583190918 \n",
      "iteration:166 loss:2.1088638305664062 \n",
      "iteration:167 loss:2.1390631198883057 \n",
      "iteration:168 loss:2.1175713539123535 \n",
      "iteration:169 loss:2.170578718185425 \n",
      "iteration:170 loss:2.125234603881836 \n",
      "iteration:171 loss:2.1224989891052246 \n",
      "iteration:172 loss:2.1496357917785645 \n",
      "iteration:173 loss:2.1019411087036133 \n",
      "iteration:174 loss:2.1349754333496094 \n",
      "iteration:175 loss:2.158163547515869 \n",
      "iteration:176 loss:2.1169939041137695 \n",
      "iteration:177 loss:2.161529064178467 \n",
      "iteration:178 loss:2.102113962173462 \n",
      "iteration:179 loss:2.144887924194336 \n",
      "iteration:180 loss:2.1591005325317383 \n",
      "iteration:181 loss:2.109198570251465 \n",
      "iteration:182 loss:2.108114242553711 \n",
      "iteration:183 loss:2.1484999656677246 \n",
      "iteration:184 loss:2.083334445953369 \n",
      "iteration:185 loss:2.1532299518585205 \n",
      "iteration:186 loss:2.0815956592559814 \n",
      "iteration:187 loss:2.116852045059204 \n",
      "iteration:188 loss:2.1288912296295166 \n",
      "iteration:189 loss:2.1342155933380127 \n",
      "iteration:190 loss:2.1143243312835693 \n",
      "iteration:191 loss:2.0407419204711914 \n",
      "iteration:192 loss:2.1374683380126953 \n",
      "iteration:193 loss:2.0788543224334717 \n",
      "iteration:194 loss:2.115344762802124 \n",
      "iteration:195 loss:2.1295197010040283 \n",
      "iteration:196 loss:2.1638762950897217 \n",
      "iteration:197 loss:2.142507553100586 \n",
      "iteration:198 loss:2.108192205429077 \n",
      "iteration:199 loss:2.1952104568481445 \n",
      "iteration:200 loss:2.1185696125030518 \n",
      "iteration:201 loss:2.1229119300842285 \n",
      "iteration:202 loss:2.0741004943847656 \n",
      "iteration:203 loss:2.1023035049438477 \n",
      "iteration:204 loss:2.037102222442627 \n",
      "iteration:205 loss:2.1356778144836426 \n",
      "iteration:206 loss:2.1027870178222656 \n",
      "iteration:207 loss:2.1279070377349854 \n",
      "iteration:208 loss:2.1798830032348633 \n",
      "iteration:209 loss:2.1266798973083496 \n",
      "iteration:210 loss:2.0915606021881104 \n",
      "iteration:211 loss:2.1601569652557373 \n",
      "iteration:212 loss:2.105839729309082 \n",
      "iteration:213 loss:2.0760769844055176 \n",
      "iteration:214 loss:2.0799641609191895 \n",
      "iteration:215 loss:2.182110548019409 \n",
      "iteration:216 loss:2.1125221252441406 \n",
      "iteration:217 loss:2.1548328399658203 \n",
      "iteration:218 loss:2.1544103622436523 \n",
      "iteration:219 loss:2.143688917160034 \n",
      "iteration:220 loss:2.09968900680542 \n",
      "iteration:221 loss:2.055748701095581 \n",
      "iteration:222 loss:2.1345582008361816 \n",
      "iteration:223 loss:2.1291723251342773 \n",
      "iteration:224 loss:2.1550405025482178 \n",
      "iteration:225 loss:2.148245334625244 \n",
      "iteration:226 loss:2.170158863067627 \n",
      "iteration:227 loss:2.1421427726745605 \n",
      "iteration:228 loss:2.089918613433838 \n",
      "iteration:229 loss:2.1503450870513916 \n",
      "iteration:230 loss:2.1466524600982666 \n",
      "iteration:231 loss:2.115060806274414 \n",
      "iteration:232 loss:2.1983447074890137 \n",
      "iteration:233 loss:2.200749397277832 \n",
      "iteration:234 loss:2.142963409423828 \n",
      "iteration:235 loss:2.1133766174316406 \n",
      "iteration:236 loss:2.085448741912842 \n",
      "iteration:237 loss:2.094951629638672 \n",
      "iteration:238 loss:2.202807664871216 \n",
      "iteration:239 loss:2.1214303970336914 \n",
      "iteration:240 loss:2.10184907913208 \n",
      "iteration:241 loss:2.087343215942383 \n",
      "iteration:242 loss:2.2183008193969727 \n",
      "iteration:243 loss:2.1269071102142334 \n",
      "iteration:244 loss:2.1445088386535645 \n",
      "iteration:245 loss:2.161787748336792 \n",
      "iteration:246 loss:2.1283113956451416 \n",
      "iteration:247 loss:2.1362645626068115 \n",
      "iteration:248 loss:2.1878662109375 \n",
      "iteration:249 loss:2.0955965518951416 \n",
      "iteration:250 loss:2.1250312328338623 \n",
      "iteration:251 loss:2.179692506790161 \n",
      "iteration:252 loss:2.1676323413848877 \n",
      "iteration:253 loss:2.07637882232666 \n",
      "iteration:254 loss:2.0866594314575195 \n",
      "iteration:255 loss:2.190319299697876 \n",
      "iteration:256 loss:2.0996854305267334 \n",
      "iteration:257 loss:2.170299530029297 \n",
      "iteration:258 loss:2.2070963382720947 \n",
      "iteration:259 loss:2.12937331199646 \n",
      "iteration:260 loss:2.124133825302124 \n",
      "iteration:261 loss:2.1544249057769775 \n",
      "iteration:262 loss:2.078993558883667 \n",
      "iteration:263 loss:2.150632858276367 \n",
      "iteration:264 loss:2.180352210998535 \n",
      "iteration:265 loss:2.122704267501831 \n",
      "iteration:266 loss:2.110074520111084 \n",
      "iteration:267 loss:2.128434419631958 \n",
      "iteration:268 loss:2.1507091522216797 \n",
      "iteration:269 loss:2.1300618648529053 \n",
      "iteration:270 loss:2.192869186401367 \n",
      "iteration:271 loss:2.103537082672119 \n",
      "iteration:272 loss:2.1431610584259033 \n",
      "iteration:273 loss:2.078781843185425 \n",
      "iteration:274 loss:2.10530686378479 \n",
      "iteration:275 loss:2.2078778743743896 \n",
      "iteration:276 loss:2.114105463027954 \n",
      "iteration:277 loss:2.126059055328369 \n",
      "iteration:278 loss:2.181086778640747 \n",
      "iteration:279 loss:2.0857348442077637 \n",
      "iteration:280 loss:2.144291877746582 \n",
      "iteration:281 loss:2.1652534008026123 \n",
      "iteration:282 loss:2.107358932495117 \n",
      "iteration:283 loss:2.133221387863159 \n",
      "iteration:284 loss:2.1967573165893555 \n",
      "iteration:285 loss:2.147796392440796 \n",
      "iteration:286 loss:2.1373331546783447 \n",
      "iteration:287 loss:2.081536293029785 \n",
      "iteration:288 loss:2.1345417499542236 \n",
      "iteration:289 loss:2.1614344120025635 \n",
      "iteration:290 loss:2.116925001144409 \n",
      "iteration:291 loss:2.1106700897216797 \n",
      "iteration:292 loss:2.0716354846954346 \n",
      "iteration:293 loss:2.0588135719299316 \n",
      "iteration:294 loss:2.1210427284240723 \n",
      "iteration:295 loss:2.127978563308716 \n",
      "iteration:296 loss:2.0360093116760254 \n",
      "iteration:297 loss:2.0443098545074463 \n",
      "iteration:298 loss:2.126833915710449 \n",
      "iteration:299 loss:2.127737283706665 \n",
      "iteration:300 loss:2.171807289123535 \n",
      "iteration:301 loss:2.1311028003692627 \n",
      "iteration:302 loss:2.1496076583862305 \n",
      "iteration:303 loss:2.0185723304748535 \n",
      "iteration:304 loss:2.1623804569244385 \n",
      "iteration:305 loss:2.1273725032806396 \n",
      "iteration:306 loss:2.072678327560425 \n",
      "iteration:307 loss:2.106555223464966 \n",
      "iteration:308 loss:2.096876859664917 \n",
      "iteration:309 loss:2.128835439682007 \n",
      "iteration:310 loss:2.138925075531006 \n",
      "iteration:311 loss:2.2240068912506104 \n",
      "iteration:312 loss:2.0294363498687744 \n",
      "iteration:313 loss:2.066807508468628 \n",
      "iteration:314 loss:2.101969003677368 \n",
      "iteration:315 loss:2.0695645809173584 \n",
      "iteration:316 loss:2.106684923171997 \n",
      "iteration:317 loss:2.098355531692505 \n",
      "iteration:318 loss:2.048515558242798 \n",
      "iteration:319 loss:2.0495223999023438 \n",
      "iteration:320 loss:2.0679514408111572 \n",
      "iteration:321 loss:2.0817883014678955 \n",
      "iteration:322 loss:2.113008737564087 \n",
      "iteration:323 loss:2.1295511722564697 \n",
      "iteration:324 loss:2.099775552749634 \n",
      "iteration:325 loss:2.0591771602630615 \n",
      "iteration:326 loss:2.1562414169311523 \n",
      "iteration:327 loss:2.1048734188079834 \n",
      "iteration:328 loss:2.0194361209869385 \n",
      "iteration:329 loss:2.1481218338012695 \n",
      "iteration:330 loss:2.1003074645996094 \n",
      "iteration:331 loss:2.0345065593719482 \n",
      "iteration:332 loss:2.0787813663482666 \n",
      "iteration:333 loss:2.081033706665039 \n",
      "iteration:334 loss:2.130518674850464 \n",
      "iteration:335 loss:2.106753349304199 \n",
      "iteration:336 loss:2.1503994464874268 \n",
      "iteration:337 loss:2.142782688140869 \n",
      "iteration:338 loss:2.06050968170166 \n",
      "iteration:339 loss:2.1880931854248047 \n",
      "iteration:340 loss:2.1067519187927246 \n",
      "iteration:341 loss:2.087222099304199 \n",
      "iteration:342 loss:2.126960039138794 \n",
      "iteration:343 loss:2.1520590782165527 \n",
      "iteration:344 loss:2.1224610805511475 \n",
      "iteration:345 loss:2.0310518741607666 \n",
      "iteration:346 loss:2.0262434482574463 \n",
      "iteration:347 loss:2.0701417922973633 \n",
      "iteration:348 loss:2.1563608646392822 \n",
      "iteration:349 loss:2.094200849533081 \n",
      "iteration:350 loss:2.0556206703186035 \n",
      "iteration:351 loss:2.1041336059570312 \n",
      "iteration:352 loss:2.1112701892852783 \n",
      "iteration:353 loss:2.176889419555664 \n",
      "iteration:354 loss:2.0021262168884277 \n",
      "iteration:355 loss:2.032378673553467 \n",
      "iteration:356 loss:2.074545383453369 \n",
      "iteration:357 loss:2.004549264907837 \n",
      "iteration:358 loss:2.097921848297119 \n",
      "iteration:359 loss:2.116276502609253 \n",
      "iteration:360 loss:2.1138689517974854 \n",
      "iteration:361 loss:2.1005005836486816 \n",
      "iteration:362 loss:2.1735548973083496 \n",
      "iteration:363 loss:2.0972719192504883 \n",
      "iteration:364 loss:2.051701545715332 \n",
      "iteration:365 loss:2.1276307106018066 \n",
      "iteration:366 loss:2.0590245723724365 \n",
      "iteration:367 loss:2.121122360229492 \n",
      "iteration:368 loss:2.0700020790100098 \n",
      "iteration:369 loss:2.1239824295043945 \n",
      "iteration:370 loss:2.1063196659088135 \n",
      "iteration:371 loss:2.0474371910095215 \n",
      "iteration:372 loss:2.0935795307159424 \n",
      "iteration:373 loss:2.0700740814208984 \n",
      "iteration:374 loss:2.184018850326538 \n",
      "iteration:375 loss:1.9965569972991943 \n",
      "iteration:376 loss:2.113610029220581 \n",
      "iteration:377 loss:2.0704708099365234 \n",
      "iteration:378 loss:2.1265676021575928 \n",
      "iteration:379 loss:2.0795295238494873 \n",
      "iteration:380 loss:2.1685280799865723 \n",
      "iteration:381 loss:2.1354990005493164 \n",
      "iteration:382 loss:2.162524700164795 \n",
      "iteration:383 loss:2.120979070663452 \n",
      "iteration:384 loss:2.138643741607666 \n",
      "iteration:385 loss:2.064802646636963 \n",
      "iteration:386 loss:2.061049461364746 \n",
      "iteration:387 loss:2.0296990871429443 \n",
      "iteration:388 loss:2.054394483566284 \n",
      "iteration:389 loss:2.109206199645996 \n",
      "iteration:390 loss:2.1337335109710693 \n",
      "iteration:391 loss:2.1349143981933594 \n",
      "iteration:392 loss:2.0675034523010254 \n",
      "iteration:393 loss:2.1689062118530273 \n",
      "iteration:394 loss:2.121340751647949 \n",
      "iteration:395 loss:2.09976863861084 \n",
      "iteration:396 loss:2.129056930541992 \n",
      "iteration:397 loss:2.134683132171631 \n",
      "iteration:398 loss:2.149695634841919 \n",
      "iteration:399 loss:2.0740439891815186 \n",
      "iteration:400 loss:2.090421438217163 \n",
      "iteration:401 loss:2.1474006175994873 \n",
      "iteration:402 loss:2.0955443382263184 \n",
      "iteration:403 loss:2.070323944091797 \n",
      "iteration:404 loss:2.0998785495758057 \n",
      "iteration:405 loss:2.1468257904052734 \n",
      "iteration:406 loss:2.1261487007141113 \n",
      "iteration:407 loss:2.125373125076294 \n",
      "iteration:408 loss:2.095165491104126 \n",
      "iteration:409 loss:2.0671253204345703 \n",
      "iteration:410 loss:2.091320514678955 \n",
      "iteration:411 loss:2.0683481693267822 \n",
      "iteration:412 loss:2.0845046043395996 \n",
      "iteration:413 loss:2.0657799243927 \n",
      "iteration:414 loss:2.1212317943573 \n",
      "iteration:415 loss:2.0857291221618652 \n",
      "iteration:416 loss:2.0930957794189453 \n",
      "iteration:417 loss:2.010702610015869 \n",
      "iteration:418 loss:2.1092607975006104 \n",
      "iteration:419 loss:2.025571584701538 \n",
      "iteration:420 loss:2.1292858123779297 \n",
      "iteration:421 loss:2.1321423053741455 \n",
      "iteration:422 loss:2.1334571838378906 \n",
      "iteration:423 loss:2.0940120220184326 \n",
      "iteration:424 loss:2.0567750930786133 \n",
      "iteration:425 loss:2.043994188308716 \n",
      "iteration:426 loss:2.1368627548217773 \n",
      "iteration:427 loss:2.1250555515289307 \n",
      "iteration:428 loss:2.1414101123809814 \n",
      "iteration:429 loss:2.0970144271850586 \n",
      "iteration:430 loss:2.0240373611450195 \n",
      "iteration:431 loss:2.0701348781585693 \n",
      "iteration:432 loss:2.2437243461608887 \n",
      "iteration:433 loss:2.0800185203552246 \n",
      "iteration:434 loss:2.0762887001037598 \n",
      "iteration:435 loss:2.142759323120117 \n",
      "iteration:436 loss:2.0531349182128906 \n",
      "iteration:437 loss:2.1211748123168945 \n",
      "iteration:438 loss:2.0822792053222656 \n",
      "iteration:439 loss:2.110887050628662 \n",
      "iteration:440 loss:1.9733834266662598 \n",
      "iteration:441 loss:2.0775675773620605 \n",
      "iteration:442 loss:2.118931770324707 \n",
      "iteration:443 loss:2.1204147338867188 \n",
      "iteration:444 loss:2.0128111839294434 \n",
      "iteration:445 loss:2.052332878112793 \n",
      "iteration:446 loss:2.1115529537200928 \n",
      "iteration:447 loss:2.0983152389526367 \n",
      "iteration:448 loss:2.081925392150879 \n",
      "iteration:449 loss:2.10498046875 \n",
      "iteration:450 loss:2.020089626312256 \n",
      "iteration:451 loss:2.087777853012085 \n",
      "iteration:452 loss:2.075237512588501 \n",
      "iteration:453 loss:2.010470390319824 \n",
      "iteration:454 loss:2.0957679748535156 \n",
      "iteration:455 loss:2.029454469680786 \n",
      "iteration:456 loss:2.1047043800354004 \n",
      "iteration:457 loss:2.0537664890289307 \n",
      "iteration:458 loss:1.9484902620315552 \n",
      "iteration:459 loss:2.1563189029693604 \n",
      "iteration:460 loss:2.100111961364746 \n",
      "iteration:461 loss:2.049607038497925 \n",
      "iteration:462 loss:2.044959783554077 \n",
      "iteration:463 loss:1.9813876152038574 \n",
      "iteration:464 loss:2.0471880435943604 \n",
      "iteration:465 loss:2.1093180179595947 \n",
      "iteration:466 loss:2.0738933086395264 \n",
      "iteration:467 loss:2.0865213871002197 \n",
      "iteration:468 loss:2.0310938358306885 \n",
      "iteration:469 loss:2.077838659286499 \n",
      "iteration:470 loss:2.042119264602661 \n",
      "iteration:471 loss:2.0675389766693115 \n",
      "iteration:472 loss:2.0742409229278564 \n",
      "iteration:473 loss:2.0226378440856934 \n",
      "iteration:474 loss:2.051168441772461 \n",
      "iteration:475 loss:2.0461127758026123 \n",
      "iteration:476 loss:2.087847948074341 \n",
      "iteration:477 loss:1.9979702234268188 \n",
      "iteration:478 loss:2.0291049480438232 \n",
      "iteration:479 loss:2.0952799320220947 \n",
      "iteration:480 loss:2.0608408451080322 \n",
      "iteration:481 loss:2.079380750656128 \n",
      "iteration:482 loss:2.1322243213653564 \n",
      "iteration:483 loss:2.043564796447754 \n",
      "iteration:484 loss:2.0256903171539307 \n",
      "iteration:485 loss:2.121173143386841 \n",
      "iteration:486 loss:2.053833484649658 \n",
      "iteration:487 loss:2.0359134674072266 \n",
      "iteration:488 loss:2.0936081409454346 \n",
      "iteration:489 loss:2.1005704402923584 \n",
      "iteration:490 loss:2.0827324390411377 \n",
      "iteration:491 loss:2.0897340774536133 \n",
      "iteration:492 loss:2.0893099308013916 \n",
      "iteration:493 loss:2.033769130706787 \n",
      "iteration:494 loss:2.0014448165893555 \n",
      "iteration:495 loss:2.081997871398926 \n",
      "iteration:496 loss:2.034362316131592 \n",
      "iteration:497 loss:2.086299180984497 \n",
      "iteration:498 loss:2.060772180557251 \n",
      "iteration:499 loss:2.162177085876465 \n",
      "iteration:500 loss:2.1306674480438232 \n",
      "iteration:501 loss:2.127859592437744 \n",
      "iteration:502 loss:2.057454824447632 \n",
      "iteration:503 loss:2.1320958137512207 \n",
      "iteration:504 loss:2.045645236968994 \n",
      "iteration:505 loss:2.0638608932495117 \n",
      "iteration:506 loss:2.082493543624878 \n",
      "iteration:507 loss:2.141810655593872 \n",
      "iteration:508 loss:2.1512644290924072 \n",
      "iteration:509 loss:2.0814218521118164 \n",
      "iteration:510 loss:2.0968313217163086 \n",
      "iteration:511 loss:2.0885627269744873 \n",
      "iteration:512 loss:2.0832903385162354 \n",
      "iteration:513 loss:1.9841302633285522 \n",
      "iteration:514 loss:2.0942494869232178 \n",
      "iteration:515 loss:1.9674609899520874 \n",
      "iteration:516 loss:2.117888927459717 \n",
      "iteration:517 loss:2.035655975341797 \n",
      "iteration:518 loss:2.0648534297943115 \n",
      "iteration:519 loss:2.0960681438446045 \n",
      "iteration:520 loss:1.9967716932296753 \n",
      "iteration:521 loss:2.0813090801239014 \n",
      "iteration:522 loss:1.9994189739227295 \n",
      "iteration:523 loss:2.1362667083740234 \n",
      "iteration:524 loss:2.0019562244415283 \n",
      "iteration:525 loss:2.07521390914917 \n",
      "iteration:526 loss:2.1140263080596924 \n",
      "iteration:527 loss:2.0626256465911865 \n",
      "iteration:528 loss:2.1048433780670166 \n",
      "iteration:529 loss:2.028809070587158 \n",
      "iteration:530 loss:2.086193323135376 \n",
      "iteration:531 loss:2.1263301372528076 \n",
      "iteration:532 loss:2.1674251556396484 \n",
      "iteration:533 loss:2.0719919204711914 \n",
      "iteration:534 loss:2.0417003631591797 \n",
      "iteration:535 loss:2.008941411972046 \n",
      "iteration:536 loss:2.0984647274017334 \n",
      "iteration:537 loss:2.0791945457458496 \n",
      "iteration:538 loss:2.101458787918091 \n",
      "iteration:539 loss:2.0581274032592773 \n",
      "iteration:540 loss:2.0262773036956787 \n",
      "iteration:541 loss:1.9966121912002563 \n",
      "iteration:542 loss:2.061204433441162 \n",
      "iteration:543 loss:2.093656539916992 \n",
      "iteration:544 loss:2.0628163814544678 \n",
      "iteration:545 loss:2.019212484359741 \n",
      "iteration:546 loss:2.0873231887817383 \n",
      "iteration:547 loss:2.038081407546997 \n",
      "iteration:548 loss:2.124643087387085 \n",
      "iteration:549 loss:2.0397183895111084 \n",
      "iteration:550 loss:2.0092415809631348 \n",
      "iteration:551 loss:2.0988073348999023 \n",
      "iteration:552 loss:2.008542060852051 \n",
      "iteration:553 loss:2.1146810054779053 \n",
      "iteration:554 loss:2.1065175533294678 \n",
      "iteration:555 loss:2.019476890563965 \n",
      "iteration:556 loss:2.1216139793395996 \n",
      "iteration:557 loss:2.0536177158355713 \n",
      "iteration:558 loss:2.173868417739868 \n",
      "iteration:559 loss:2.1526315212249756 \n",
      "iteration:560 loss:2.1302573680877686 \n",
      "iteration:561 loss:2.0585548877716064 \n",
      "iteration:562 loss:2.0948262214660645 \n",
      "iteration:563 loss:1.9814519882202148 \n",
      "iteration:564 loss:2.123378276824951 \n",
      "iteration:565 loss:2.143868923187256 \n",
      "iteration:566 loss:2.0707128047943115 \n",
      "iteration:567 loss:2.1333656311035156 \n",
      "iteration:568 loss:2.02970552444458 \n",
      "iteration:569 loss:2.1293108463287354 \n",
      "iteration:570 loss:2.1422836780548096 \n",
      "iteration:571 loss:2.1061036586761475 \n",
      "iteration:572 loss:2.0471127033233643 \n",
      "iteration:573 loss:2.0433061122894287 \n",
      "iteration:574 loss:2.029195785522461 \n",
      "iteration:575 loss:2.089463233947754 \n",
      "iteration:576 loss:2.0586047172546387 \n",
      "iteration:577 loss:2.1201467514038086 \n",
      "iteration:578 loss:1.953944444656372 \n",
      "iteration:579 loss:2.13692307472229 \n",
      "iteration:580 loss:2.0489556789398193 \n",
      "iteration:581 loss:2.0237514972686768 \n",
      "iteration:582 loss:2.1351630687713623 \n",
      "iteration:583 loss:2.1309592723846436 \n",
      "iteration:584 loss:2.0967764854431152 \n",
      "iteration:585 loss:2.0506865978240967 \n",
      "iteration:586 loss:2.131587505340576 \n",
      "iteration:587 loss:2.084179639816284 \n",
      "iteration:588 loss:2.0808115005493164 \n",
      "iteration:589 loss:2.0773684978485107 \n",
      "iteration:590 loss:2.1034908294677734 \n",
      "iteration:591 loss:2.0782997608184814 \n",
      "iteration:592 loss:2.084813117980957 \n",
      "iteration:593 loss:2.0548598766326904 \n",
      "iteration:594 loss:2.108734369277954 \n",
      "iteration:595 loss:1.986548662185669 \n",
      "iteration:596 loss:2.018395185470581 \n",
      "iteration:597 loss:2.0557122230529785 \n",
      "iteration:598 loss:2.089463233947754 \n",
      "iteration:599 loss:2.090306282043457 \n",
      "iteration:600 loss:2.0361645221710205 \n",
      "iteration:601 loss:2.0265722274780273 \n",
      "iteration:602 loss:2.0676653385162354 \n",
      "iteration:603 loss:1.9864776134490967 \n",
      "iteration:604 loss:1.9935564994812012 \n",
      "iteration:605 loss:2.0130486488342285 \n",
      "iteration:606 loss:2.0685884952545166 \n",
      "iteration:607 loss:1.9935308694839478 \n",
      "iteration:608 loss:2.143763780593872 \n",
      "iteration:609 loss:2.027468681335449 \n",
      "iteration:610 loss:2.05509090423584 \n",
      "iteration:611 loss:2.0680761337280273 \n",
      "iteration:612 loss:1.9747076034545898 \n",
      "iteration:613 loss:2.0784554481506348 \n",
      "iteration:614 loss:2.167525291442871 \n",
      "iteration:615 loss:2.039522409439087 \n",
      "iteration:616 loss:2.1002814769744873 \n",
      "iteration:617 loss:2.048149824142456 \n",
      "iteration:618 loss:2.117879867553711 \n",
      "iteration:619 loss:1.9918931722640991 \n",
      "iteration:620 loss:2.078124761581421 \n",
      "iteration:621 loss:2.0695345401763916 \n",
      "iteration:622 loss:2.009192943572998 \n",
      "iteration:623 loss:2.0195560455322266 \n",
      "iteration:624 loss:2.002272844314575 \n",
      "iteration:625 loss:2.0098421573638916 \n",
      "iteration:626 loss:2.1219704151153564 \n",
      "iteration:627 loss:2.0056049823760986 \n",
      "iteration:628 loss:1.9681730270385742 \n",
      "iteration:629 loss:2.006513833999634 \n",
      "iteration:630 loss:2.0511796474456787 \n",
      "iteration:631 loss:2.115405321121216 \n",
      "iteration:632 loss:2.1231791973114014 \n",
      "iteration:633 loss:2.0908515453338623 \n",
      "iteration:634 loss:2.076002597808838 \n",
      "iteration:635 loss:2.162991523742676 \n",
      "iteration:636 loss:2.1195340156555176 \n",
      "iteration:637 loss:1.9990921020507812 \n",
      "iteration:638 loss:2.0502166748046875 \n",
      "iteration:639 loss:2.086827278137207 \n",
      "iteration:640 loss:2.1251060962677 \n",
      "iteration:641 loss:2.115921974182129 \n",
      "iteration:642 loss:2.0669689178466797 \n",
      "iteration:643 loss:2.099564552307129 \n",
      "iteration:644 loss:2.097168445587158 \n",
      "iteration:645 loss:2.007406234741211 \n",
      "iteration:646 loss:2.0476443767547607 \n",
      "iteration:647 loss:2.0453929901123047 \n",
      "iteration:648 loss:2.0613086223602295 \n",
      "iteration:649 loss:2.0344038009643555 \n",
      "iteration:650 loss:2.1428160667419434 \n",
      "iteration:651 loss:2.0866198539733887 \n",
      "iteration:652 loss:2.1315174102783203 \n",
      "iteration:653 loss:2.083087682723999 \n",
      "iteration:654 loss:2.101095199584961 \n",
      "iteration:655 loss:2.1528890132904053 \n",
      "iteration:656 loss:2.020728826522827 \n",
      "iteration:657 loss:2.0762834548950195 \n",
      "iteration:658 loss:2.069638252258301 \n",
      "iteration:659 loss:2.104661464691162 \n",
      "iteration:660 loss:2.0453319549560547 \n",
      "iteration:661 loss:2.081723690032959 \n",
      "iteration:662 loss:2.130542039871216 \n",
      "iteration:663 loss:2.0813043117523193 \n",
      "iteration:664 loss:2.0496137142181396 \n",
      "iteration:665 loss:2.0599961280822754 \n",
      "iteration:666 loss:2.089550733566284 \n",
      "iteration:667 loss:2.082362651824951 \n",
      "iteration:668 loss:2.0315542221069336 \n",
      "iteration:669 loss:2.0440480709075928 \n",
      "iteration:670 loss:2.095874786376953 \n",
      "iteration:671 loss:2.061041831970215 \n",
      "iteration:672 loss:1.9939110279083252 \n",
      "iteration:673 loss:2.0580954551696777 \n",
      "iteration:674 loss:2.024583339691162 \n",
      "iteration:675 loss:2.0512704849243164 \n",
      "iteration:676 loss:2.039787530899048 \n",
      "iteration:677 loss:2.0314457416534424 \n",
      "iteration:678 loss:2.0870025157928467 \n",
      "iteration:679 loss:2.0990803241729736 \n",
      "iteration:680 loss:2.1221652030944824 \n",
      "iteration:681 loss:2.0545761585235596 \n",
      "iteration:682 loss:2.015758752822876 \n",
      "iteration:683 loss:1.9692894220352173 \n",
      "iteration:684 loss:2.135507106781006 \n",
      "iteration:685 loss:2.03627347946167 \n",
      "iteration:686 loss:2.0447998046875 \n",
      "iteration:687 loss:2.132911443710327 \n",
      "iteration:688 loss:2.0512242317199707 \n",
      "iteration:689 loss:1.9700524806976318 \n",
      "iteration:690 loss:2.003781795501709 \n",
      "iteration:691 loss:2.0777535438537598 \n",
      "iteration:692 loss:2.0371272563934326 \n",
      "iteration:693 loss:1.9953216314315796 \n",
      "iteration:694 loss:2.082984685897827 \n",
      "iteration:695 loss:2.1008739471435547 \n",
      "iteration:696 loss:2.003838539123535 \n",
      "iteration:697 loss:2.045992136001587 \n",
      "iteration:698 loss:1.9996129274368286 \n",
      "iteration:699 loss:2.096176862716675 \n",
      "iteration:700 loss:2.0390260219573975 \n",
      "iteration:701 loss:1.9922316074371338 \n",
      "iteration:702 loss:2.090951681137085 \n",
      "iteration:703 loss:2.0691564083099365 \n",
      "iteration:704 loss:2.0829415321350098 \n",
      "iteration:705 loss:1.8838720321655273 \n",
      "iteration:706 loss:2.0143613815307617 \n",
      "iteration:707 loss:1.9992303848266602 \n",
      "iteration:708 loss:2.0193984508514404 \n",
      "iteration:709 loss:2.053241729736328 \n",
      "iteration:710 loss:2.03798770904541 \n",
      "iteration:711 loss:2.1040139198303223 \n",
      "iteration:712 loss:2.114745855331421 \n",
      "iteration:713 loss:2.0765321254730225 \n",
      "iteration:714 loss:1.966307520866394 \n",
      "iteration:715 loss:2.1410160064697266 \n",
      "iteration:716 loss:2.03120493888855 \n",
      "iteration:717 loss:2.0456478595733643 \n",
      "iteration:718 loss:2.029012680053711 \n",
      "iteration:719 loss:2.102867364883423 \n",
      "iteration:720 loss:2.0520408153533936 \n",
      "iteration:721 loss:2.0031232833862305 \n",
      "iteration:722 loss:2.074951171875 \n",
      "iteration:723 loss:2.0018365383148193 \n",
      "iteration:724 loss:1.9501843452453613 \n",
      "iteration:725 loss:2.051546096801758 \n",
      "iteration:726 loss:2.1132233142852783 \n",
      "iteration:727 loss:2.061577081680298 \n",
      "iteration:728 loss:1.9947173595428467 \n",
      "iteration:729 loss:2.0462329387664795 \n",
      "iteration:730 loss:2.14311146736145 \n",
      "iteration:731 loss:2.1082353591918945 \n",
      "iteration:732 loss:1.9931988716125488 \n",
      "iteration:733 loss:2.0470051765441895 \n",
      "iteration:734 loss:2.1047933101654053 \n",
      "iteration:735 loss:1.9680782556533813 \n",
      "iteration:736 loss:2.0850179195404053 \n",
      "iteration:737 loss:2.0019888877868652 \n",
      "iteration:738 loss:2.072294235229492 \n",
      "iteration:739 loss:2.126147508621216 \n",
      "iteration:740 loss:2.1320152282714844 \n",
      "iteration:741 loss:2.1300437450408936 \n",
      "iteration:742 loss:2.0546674728393555 \n",
      "iteration:743 loss:2.073784112930298 \n",
      "iteration:744 loss:2.0771889686584473 \n",
      "iteration:745 loss:2.002049207687378 \n",
      "iteration:746 loss:2.1062116622924805 \n",
      "iteration:747 loss:2.0475175380706787 \n",
      "iteration:748 loss:2.1156725883483887 \n",
      "iteration:749 loss:2.0860328674316406 \n",
      "iteration:750 loss:2.055661201477051 \n",
      "iteration:751 loss:2.045565366744995 \n",
      "iteration:752 loss:2.1071994304656982 \n",
      "iteration:753 loss:2.141585350036621 \n",
      "iteration:754 loss:1.9953880310058594 \n",
      "iteration:755 loss:2.120342254638672 \n",
      "iteration:756 loss:2.065005302429199 \n",
      "iteration:757 loss:1.9863537549972534 \n",
      "iteration:758 loss:2.0587034225463867 \n",
      "iteration:759 loss:2.0531625747680664 \n",
      "iteration:760 loss:2.1038546562194824 \n",
      "iteration:761 loss:2.0989012718200684 \n",
      "iteration:762 loss:1.9401055574417114 \n",
      "iteration:763 loss:2.0911011695861816 \n",
      "iteration:764 loss:2.067257881164551 \n",
      "iteration:765 loss:2.147393226623535 \n",
      "iteration:766 loss:2.057619333267212 \n",
      "iteration:767 loss:2.066068410873413 \n",
      "iteration:768 loss:2.1073930263519287 \n",
      "iteration:769 loss:2.094489336013794 \n",
      "iteration:770 loss:2.0147945880889893 \n",
      "iteration:771 loss:2.072766065597534 \n",
      "iteration:772 loss:2.0121679306030273 \n",
      "iteration:773 loss:2.0423848628997803 \n",
      "iteration:774 loss:2.0122721195220947 \n",
      "iteration:775 loss:2.0675880908966064 \n",
      "iteration:776 loss:2.094357490539551 \n",
      "iteration:777 loss:2.1114003658294678 \n",
      "iteration:778 loss:2.126380205154419 \n",
      "iteration:779 loss:2.1079320907592773 \n",
      "iteration:780 loss:1.9362574815750122 \n",
      "iteration:781 loss:1.9545722007751465 \n",
      "iteration:782 loss:2.085627555847168 \n",
      "iteration:783 loss:2.043456792831421 \n",
      "iteration:784 loss:2.1052651405334473 \n",
      "iteration:785 loss:2.0139358043670654 \n",
      "iteration:786 loss:1.9670454263687134 \n",
      "iteration:787 loss:2.140955924987793 \n",
      "iteration:788 loss:2.0936789512634277 \n",
      "iteration:789 loss:2.064393997192383 \n",
      "iteration:790 loss:2.0710954666137695 \n",
      "iteration:791 loss:2.1353065967559814 \n",
      "iteration:792 loss:2.0558512210845947 \n",
      "iteration:793 loss:2.040809154510498 \n",
      "iteration:794 loss:2.082590103149414 \n",
      "iteration:795 loss:2.063946008682251 \n",
      "iteration:796 loss:2.0144715309143066 \n",
      "iteration:797 loss:2.1277475357055664 \n",
      "iteration:798 loss:2.088839292526245 \n",
      "iteration:799 loss:2.108088493347168 \n",
      "iteration:800 loss:2.0352256298065186 \n",
      "iteration:801 loss:2.111937999725342 \n",
      "iteration:802 loss:2.0054140090942383 \n",
      "iteration:803 loss:2.1010055541992188 \n",
      "iteration:804 loss:2.090282440185547 \n",
      "iteration:805 loss:2.064863920211792 \n",
      "iteration:806 loss:2.0586190223693848 \n",
      "iteration:807 loss:2.0388545989990234 \n",
      "iteration:808 loss:2.104372024536133 \n",
      "iteration:809 loss:1.985856533050537 \n",
      "iteration:810 loss:1.9911280870437622 \n",
      "iteration:811 loss:2.0909085273742676 \n",
      "iteration:812 loss:2.0772154331207275 \n",
      "iteration:813 loss:2.1373910903930664 \n",
      "iteration:814 loss:2.094914436340332 \n",
      "iteration:815 loss:2.090141534805298 \n",
      "iteration:816 loss:2.061246871948242 \n",
      "iteration:817 loss:1.9706050157546997 \n",
      "iteration:818 loss:2.1025283336639404 \n",
      "iteration:819 loss:2.094592571258545 \n",
      "iteration:820 loss:2.125619411468506 \n",
      "iteration:821 loss:2.0915443897247314 \n",
      "iteration:822 loss:1.956605076789856 \n",
      "iteration:823 loss:2.0327045917510986 \n",
      "iteration:824 loss:2.051964521408081 \n",
      "iteration:825 loss:1.9973974227905273 \n",
      "iteration:826 loss:2.0593221187591553 \n",
      "iteration:827 loss:2.0034170150756836 \n",
      "iteration:828 loss:1.949947476387024 \n",
      "iteration:829 loss:2.0295989513397217 \n",
      "iteration:830 loss:2.01948618888855 \n",
      "iteration:831 loss:2.0245566368103027 \n",
      "iteration:832 loss:2.07944917678833 \n",
      "iteration:833 loss:2.08766508102417 \n",
      "iteration:834 loss:2.1189873218536377 \n",
      "iteration:835 loss:2.0381968021392822 \n",
      "iteration:836 loss:2.085360527038574 \n",
      "iteration:837 loss:2.077342987060547 \n",
      "iteration:838 loss:2.0690996646881104 \n",
      "iteration:839 loss:2.046546697616577 \n",
      "iteration:840 loss:2.0602829456329346 \n",
      "iteration:841 loss:2.0100491046905518 \n",
      "iteration:842 loss:2.07700514793396 \n",
      "iteration:843 loss:2.043442964553833 \n",
      "iteration:844 loss:2.100034236907959 \n",
      "iteration:845 loss:2.0585479736328125 \n",
      "iteration:846 loss:2.0241968631744385 \n",
      "iteration:847 loss:1.987227439880371 \n",
      "iteration:848 loss:2.0615313053131104 \n",
      "iteration:849 loss:1.9999945163726807 \n",
      "iteration:850 loss:2.087432622909546 \n",
      "iteration:851 loss:2.0495922565460205 \n",
      "iteration:852 loss:2.0815415382385254 \n",
      "iteration:853 loss:2.0757896900177 \n",
      "iteration:854 loss:2.053898572921753 \n",
      "iteration:855 loss:2.044813632965088 \n",
      "iteration:856 loss:2.1234259605407715 \n",
      "iteration:857 loss:2.076106309890747 \n",
      "iteration:858 loss:2.026052474975586 \n",
      "iteration:859 loss:2.0215961933135986 \n",
      "iteration:860 loss:2.0731282234191895 \n",
      "iteration:861 loss:1.9907124042510986 \n",
      "iteration:862 loss:2.0366718769073486 \n",
      "iteration:863 loss:2.0027916431427 \n",
      "iteration:864 loss:2.0446395874023438 \n",
      "iteration:865 loss:1.994771122932434 \n",
      "iteration:866 loss:2.0063366889953613 \n",
      "iteration:867 loss:2.0713798999786377 \n",
      "iteration:868 loss:2.074532985687256 \n",
      "iteration:869 loss:2.010122060775757 \n",
      "iteration:870 loss:2.081181287765503 \n",
      "iteration:871 loss:2.0485427379608154 \n",
      "iteration:872 loss:2.0442025661468506 \n",
      "iteration:873 loss:2.0774264335632324 \n",
      "iteration:874 loss:2.075469493865967 \n",
      "iteration:875 loss:2.064537286758423 \n",
      "iteration:876 loss:2.0343260765075684 \n",
      "iteration:877 loss:1.9976458549499512 \n",
      "iteration:878 loss:2.008944511413574 \n",
      "iteration:879 loss:2.0547897815704346 \n",
      "iteration:880 loss:2.047528028488159 \n",
      "iteration:881 loss:1.9731297492980957 \n",
      "iteration:882 loss:2.052354574203491 \n",
      "iteration:883 loss:2.0647666454315186 \n",
      "iteration:884 loss:2.0417299270629883 \n",
      "iteration:885 loss:2.0683610439300537 \n",
      "iteration:886 loss:2.0079784393310547 \n",
      "iteration:887 loss:2.047609806060791 \n",
      "iteration:888 loss:2.08282732963562 \n",
      "iteration:889 loss:2.0745034217834473 \n",
      "iteration:890 loss:2.111882448196411 \n",
      "iteration:891 loss:2.079016923904419 \n",
      "iteration:892 loss:2.058945894241333 \n",
      "iteration:893 loss:2.107510566711426 \n",
      "iteration:894 loss:2.0702061653137207 \n",
      "iteration:895 loss:2.1122829914093018 \n",
      "iteration:896 loss:2.099956750869751 \n",
      "iteration:897 loss:2.094654083251953 \n",
      "iteration:898 loss:2.0902767181396484 \n",
      "iteration:899 loss:2.043905735015869 \n",
      "iteration:900 loss:2.02813720703125 \n",
      "iteration:901 loss:2.142791271209717 \n",
      "iteration:902 loss:2.0244946479797363 \n",
      "iteration:903 loss:2.0933055877685547 \n",
      "iteration:904 loss:1.9847426414489746 \n",
      "iteration:905 loss:1.976047158241272 \n",
      "iteration:906 loss:2.0249133110046387 \n",
      "iteration:907 loss:2.1049439907073975 \n",
      "iteration:908 loss:2.054755449295044 \n",
      "iteration:909 loss:2.036463975906372 \n",
      "iteration:910 loss:2.0507118701934814 \n",
      "iteration:911 loss:2.094512701034546 \n",
      "iteration:912 loss:2.0584638118743896 \n",
      "iteration:913 loss:2.1229746341705322 \n",
      "iteration:914 loss:2.042163610458374 \n",
      "iteration:915 loss:2.0788187980651855 \n",
      "iteration:916 loss:1.987312912940979 \n",
      "iteration:917 loss:2.00947642326355 \n",
      "iteration:918 loss:2.09663462638855 \n",
      "iteration:919 loss:2.109717607498169 \n",
      "iteration:920 loss:2.0245344638824463 \n",
      "iteration:921 loss:2.1337146759033203 \n",
      "iteration:922 loss:2.0208215713500977 \n",
      "iteration:923 loss:2.001976251602173 \n",
      "iteration:924 loss:2.015169143676758 \n",
      "iteration:925 loss:2.0542988777160645 \n",
      "iteration:926 loss:2.0196566581726074 \n",
      "iteration:927 loss:2.005925178527832 \n",
      "iteration:928 loss:2.0067262649536133 \n",
      "iteration:929 loss:2.054807662963867 \n",
      "iteration:930 loss:2.038198947906494 \n",
      "iteration:931 loss:2.095651149749756 \n",
      "iteration:932 loss:2.114567279815674 \n",
      "iteration:933 loss:2.058706283569336 \n",
      "iteration:934 loss:2.0366547107696533 \n",
      "iteration:935 loss:2.0747644901275635 \n",
      "iteration:936 loss:1.9520820379257202 \n",
      "iteration:937 loss:2.060692071914673 \n",
      "iteration:938 loss:2.0385773181915283 \n",
      "iteration:939 loss:2.0659291744232178 \n",
      "iteration:940 loss:2.025911331176758 \n",
      "iteration:941 loss:1.9592561721801758 \n",
      "iteration:942 loss:2.106403112411499 \n",
      "iteration:943 loss:1.941898226737976 \n",
      "iteration:944 loss:2.0938217639923096 \n",
      "iteration:945 loss:2.0107483863830566 \n",
      "iteration:946 loss:2.153303384780884 \n",
      "iteration:947 loss:2.1069235801696777 \n",
      "iteration:948 loss:2.086047887802124 \n",
      "iteration:949 loss:2.024475574493408 \n",
      "iteration:950 loss:2.0257108211517334 \n",
      "iteration:951 loss:1.9934353828430176 \n",
      "iteration:952 loss:2.101010799407959 \n",
      "iteration:953 loss:2.011014461517334 \n",
      "iteration:954 loss:1.983557105064392 \n",
      "iteration:955 loss:2.080942153930664 \n",
      "iteration:956 loss:1.9978322982788086 \n",
      "iteration:957 loss:2.0404622554779053 \n",
      "iteration:958 loss:2.023899793624878 \n",
      "iteration:959 loss:2.047105312347412 \n",
      "iteration:960 loss:2.0936121940612793 \n",
      "iteration:961 loss:2.0016396045684814 \n",
      "iteration:962 loss:2.034101963043213 \n",
      "iteration:963 loss:2.0404136180877686 \n",
      "iteration:964 loss:2.0332236289978027 \n",
      "iteration:965 loss:2.0450117588043213 \n",
      "iteration:966 loss:2.0875399112701416 \n",
      "iteration:967 loss:2.065201759338379 \n",
      "iteration:968 loss:2.0049331188201904 \n",
      "iteration:969 loss:2.040022373199463 \n",
      "iteration:970 loss:1.9402265548706055 \n",
      "iteration:971 loss:1.9649816751480103 \n",
      "iteration:972 loss:2.0222063064575195 \n",
      "iteration:973 loss:2.0405609607696533 \n",
      "iteration:974 loss:2.035501480102539 \n",
      "iteration:975 loss:2.0267553329467773 \n",
      "iteration:976 loss:2.0371856689453125 \n",
      "iteration:977 loss:2.063318967819214 \n",
      "iteration:978 loss:1.9985777139663696 \n",
      "iteration:979 loss:1.950974941253662 \n",
      "iteration:980 loss:2.1333162784576416 \n",
      "iteration:981 loss:1.9766066074371338 \n",
      "iteration:982 loss:2.000903606414795 \n",
      "iteration:983 loss:2.0350584983825684 \n",
      "iteration:984 loss:1.9412634372711182 \n",
      "iteration:985 loss:2.1058990955352783 \n",
      "iteration:986 loss:1.9792004823684692 \n",
      "iteration:987 loss:2.044614791870117 \n",
      "iteration:988 loss:1.9927486181259155 \n",
      "iteration:989 loss:2.114807367324829 \n",
      "iteration:990 loss:2.0762577056884766 \n",
      "iteration:991 loss:1.932256817817688 \n",
      "iteration:992 loss:2.096684217453003 \n",
      "iteration:993 loss:2.0270488262176514 \n",
      "iteration:994 loss:2.0663022994995117 \n",
      "iteration:995 loss:2.095388412475586 \n",
      "iteration:996 loss:1.9954054355621338 \n",
      "iteration:997 loss:2.147368907928467 \n",
      "iteration:998 loss:2.0886306762695312 \n",
      "iteration:999 loss:2.0378501415252686 \n",
      "iteration:1000 loss:2.061774492263794 \n",
      "iteration:1001 loss:2.066150426864624 \n",
      "iteration:1002 loss:1.9291895627975464 \n",
      "iteration:1003 loss:2.0393731594085693 \n",
      "iteration:1004 loss:1.9708706140518188 \n",
      "iteration:1005 loss:1.9974911212921143 \n",
      "iteration:1006 loss:2.019680976867676 \n",
      "iteration:1007 loss:2.1246352195739746 \n",
      "iteration:1008 loss:1.9901936054229736 \n",
      "iteration:1009 loss:1.9598466157913208 \n",
      "iteration:1010 loss:2.0322694778442383 \n",
      "iteration:1011 loss:2.0895421504974365 \n",
      "iteration:1012 loss:1.9751185178756714 \n",
      "iteration:1013 loss:2.041607141494751 \n",
      "iteration:1014 loss:2.053194284439087 \n",
      "iteration:1015 loss:2.140944004058838 \n",
      "iteration:1016 loss:2.029374599456787 \n",
      "iteration:1017 loss:1.930054783821106 \n",
      "iteration:1018 loss:1.9672024250030518 \n",
      "iteration:1019 loss:2.019876003265381 \n",
      "iteration:1020 loss:2.0192315578460693 \n",
      "iteration:1021 loss:2.047647476196289 \n",
      "iteration:1022 loss:2.1554505825042725 \n",
      "iteration:1023 loss:2.0052552223205566 \n",
      "iteration:1024 loss:2.074667453765869 \n",
      "iteration:1025 loss:2.025933265686035 \n",
      "iteration:1026 loss:2.027477741241455 \n",
      "iteration:1027 loss:2.0407955646514893 \n",
      "iteration:1028 loss:1.9992600679397583 \n",
      "iteration:1029 loss:2.0540857315063477 \n",
      "iteration:1030 loss:2.104163408279419 \n",
      "iteration:1031 loss:2.096086025238037 \n",
      "iteration:1032 loss:2.0602965354919434 \n",
      "iteration:1033 loss:2.028075933456421 \n",
      "iteration:1034 loss:2.0888965129852295 \n",
      "iteration:1035 loss:2.10551381111145 \n",
      "iteration:1036 loss:2.05328106880188 \n",
      "iteration:1037 loss:2.0356178283691406 \n",
      "iteration:1038 loss:2.0794525146484375 \n",
      "iteration:1039 loss:2.0323848724365234 \n",
      "iteration:1040 loss:2.0451979637145996 \n",
      "iteration:1041 loss:2.0584421157836914 \n",
      "iteration:1042 loss:2.012489080429077 \n",
      "iteration:1043 loss:2.104090690612793 \n",
      "iteration:1044 loss:2.0257859230041504 \n",
      "iteration:1045 loss:1.9846289157867432 \n",
      "iteration:1046 loss:1.9322680234909058 \n",
      "iteration:1047 loss:2.0080482959747314 \n",
      "iteration:1048 loss:2.010580539703369 \n",
      "iteration:1049 loss:2.0612051486968994 \n",
      "iteration:1050 loss:2.145599126815796 \n",
      "iteration:1051 loss:1.9787424802780151 \n",
      "iteration:1052 loss:1.932987093925476 \n",
      "iteration:1053 loss:2.0673491954803467 \n",
      "iteration:1054 loss:2.1128766536712646 \n",
      "iteration:1055 loss:1.976157546043396 \n",
      "iteration:1056 loss:1.9304548501968384 \n",
      "iteration:1057 loss:1.9859603643417358 \n",
      "iteration:1058 loss:2.0203676223754883 \n",
      "iteration:1059 loss:2.0522866249084473 \n",
      "iteration:1060 loss:2.031099319458008 \n",
      "iteration:1061 loss:2.0889663696289062 \n",
      "iteration:1062 loss:2.0791501998901367 \n",
      "iteration:1063 loss:2.0086655616760254 \n",
      "iteration:1064 loss:2.03598690032959 \n",
      "iteration:1065 loss:2.061508893966675 \n",
      "iteration:1066 loss:2.063744306564331 \n",
      "iteration:1067 loss:2.107893943786621 \n",
      "iteration:1068 loss:2.032463788986206 \n",
      "iteration:1069 loss:2.0363597869873047 \n",
      "iteration:1070 loss:1.9745128154754639 \n",
      "iteration:1071 loss:2.0567541122436523 \n",
      "iteration:1072 loss:2.025555372238159 \n",
      "iteration:1073 loss:1.9675670862197876 \n",
      "iteration:1074 loss:2.026456117630005 \n",
      "iteration:1075 loss:1.967157006263733 \n",
      "iteration:1076 loss:1.963442325592041 \n",
      "iteration:1077 loss:2.0091264247894287 \n",
      "iteration:1078 loss:2.0703208446502686 \n",
      "iteration:1079 loss:2.011605739593506 \n",
      "iteration:1080 loss:1.9884552955627441 \n",
      "iteration:1081 loss:2.0580339431762695 \n",
      "iteration:1082 loss:2.113243341445923 \n",
      "iteration:1083 loss:2.0946059226989746 \n",
      "iteration:1084 loss:2.0702896118164062 \n",
      "iteration:1085 loss:2.0097672939300537 \n",
      "iteration:1086 loss:1.910835862159729 \n",
      "iteration:1087 loss:1.9540395736694336 \n",
      "iteration:1088 loss:1.9299699068069458 \n",
      "iteration:1089 loss:2.0343306064605713 \n",
      "iteration:1090 loss:2.0056302547454834 \n",
      "iteration:1091 loss:2.029426336288452 \n",
      "iteration:1092 loss:1.9741027355194092 \n",
      "iteration:1093 loss:2.0034875869750977 \n",
      "iteration:1094 loss:2.1281445026397705 \n",
      "iteration:1095 loss:2.0518932342529297 \n",
      "iteration:1096 loss:2.029958486557007 \n",
      "iteration:1097 loss:2.0066001415252686 \n",
      "iteration:1098 loss:2.1057627201080322 \n",
      "iteration:1099 loss:2.0092227458953857 \n",
      "iteration:1100 loss:1.9932326078414917 \n",
      "iteration:1101 loss:2.010866641998291 \n",
      "iteration:1102 loss:2.0661940574645996 \n",
      "iteration:1103 loss:1.9901306629180908 \n",
      "iteration:1104 loss:2.0271310806274414 \n",
      "iteration:1105 loss:1.9510469436645508 \n",
      "iteration:1106 loss:2.0316436290740967 \n",
      "iteration:1107 loss:2.0583393573760986 \n",
      "iteration:1108 loss:2.111424207687378 \n",
      "iteration:1109 loss:2.0967042446136475 \n",
      "iteration:1110 loss:2.073422908782959 \n",
      "iteration:1111 loss:1.978670358657837 \n",
      "iteration:1112 loss:2.0441696643829346 \n",
      "iteration:1113 loss:2.070767879486084 \n",
      "iteration:1114 loss:2.1098084449768066 \n",
      "iteration:1115 loss:2.0861217975616455 \n",
      "iteration:1116 loss:1.969332218170166 \n",
      "iteration:1117 loss:2.0506742000579834 \n",
      "iteration:1118 loss:2.024399995803833 \n",
      "iteration:1119 loss:2.0280869007110596 \n",
      "iteration:1120 loss:2.067379951477051 \n",
      "iteration:1121 loss:1.9880365133285522 \n",
      "iteration:1122 loss:1.9719630479812622 \n",
      "iteration:1123 loss:2.0525219440460205 \n",
      "iteration:1124 loss:1.874837040901184 \n",
      "iteration:1125 loss:2.0640270709991455 \n",
      "iteration:1126 loss:2.0305840969085693 \n",
      "iteration:1127 loss:2.085444688796997 \n",
      "iteration:1128 loss:2.014448642730713 \n",
      "iteration:1129 loss:2.0458157062530518 \n",
      "iteration:1130 loss:2.0930440425872803 \n",
      "iteration:1131 loss:1.9726544618606567 \n",
      "iteration:1132 loss:2.0637001991271973 \n",
      "iteration:1133 loss:2.1056418418884277 \n",
      "iteration:1134 loss:1.9330416917800903 \n",
      "iteration:1135 loss:1.9947786331176758 \n",
      "iteration:1136 loss:2.030219078063965 \n",
      "iteration:1137 loss:2.067903518676758 \n",
      "iteration:1138 loss:1.9508070945739746 \n",
      "iteration:1139 loss:1.9600553512573242 \n",
      "iteration:1140 loss:2.0779225826263428 \n",
      "iteration:1141 loss:2.0387096405029297 \n",
      "iteration:1142 loss:1.9892396926879883 \n",
      "iteration:1143 loss:2.0162572860717773 \n",
      "iteration:1144 loss:1.9994490146636963 \n",
      "iteration:1145 loss:2.078200101852417 \n",
      "iteration:1146 loss:2.00492262840271 \n",
      "iteration:1147 loss:1.9618043899536133 \n",
      "iteration:1148 loss:2.0064308643341064 \n",
      "iteration:1149 loss:2.0388123989105225 \n",
      "iteration:1150 loss:1.988843560218811 \n",
      "iteration:1151 loss:1.9147099256515503 \n",
      "iteration:1152 loss:2.0446529388427734 \n",
      "iteration:1153 loss:2.0331244468688965 \n",
      "iteration:1154 loss:1.9857277870178223 \n",
      "iteration:1155 loss:2.146326780319214 \n",
      "iteration:1156 loss:2.0677804946899414 \n",
      "iteration:1157 loss:1.9698023796081543 \n",
      "iteration:1158 loss:2.0876662731170654 \n",
      "iteration:1159 loss:2.0448989868164062 \n",
      "iteration:1160 loss:2.0226221084594727 \n",
      "iteration:1161 loss:1.9878193140029907 \n",
      "iteration:1162 loss:2.043107509613037 \n",
      "iteration:1163 loss:1.9528937339782715 \n",
      "iteration:1164 loss:2.035865545272827 \n",
      "iteration:1165 loss:2.0154924392700195 \n",
      "iteration:1166 loss:1.9924653768539429 \n",
      "iteration:1167 loss:1.9945889711380005 \n",
      "iteration:1168 loss:2.0209057331085205 \n",
      "iteration:1169 loss:1.9645041227340698 \n",
      "iteration:1170 loss:2.0022988319396973 \n",
      "iteration:1171 loss:2.076486110687256 \n",
      "iteration:1172 loss:1.9371423721313477 \n",
      "iteration:1173 loss:1.9989875555038452 \n",
      "iteration:1174 loss:2.0877599716186523 \n",
      "iteration:1175 loss:2.097423553466797 \n",
      "iteration:1176 loss:2.0579378604888916 \n",
      "iteration:1177 loss:1.9813765287399292 \n",
      "iteration:1178 loss:2.0093982219696045 \n",
      "iteration:1179 loss:1.9907994270324707 \n",
      "iteration:1180 loss:1.9914155006408691 \n",
      "iteration:1181 loss:1.9656341075897217 \n",
      "iteration:1182 loss:1.9635858535766602 \n",
      "iteration:1183 loss:1.9618079662322998 \n",
      "iteration:1184 loss:2.0262749195098877 \n",
      "iteration:1185 loss:1.9700937271118164 \n",
      "iteration:1186 loss:2.064560890197754 \n",
      "iteration:1187 loss:2.0498244762420654 \n",
      "iteration:1188 loss:2.1007375717163086 \n",
      "iteration:1189 loss:2.023934841156006 \n",
      "iteration:1190 loss:2.066042423248291 \n",
      "iteration:1191 loss:1.954646110534668 \n",
      "iteration:1192 loss:2.0536930561065674 \n",
      "iteration:1193 loss:2.0450174808502197 \n",
      "iteration:1194 loss:1.9910061359405518 \n",
      "iteration:1195 loss:2.047274112701416 \n",
      "iteration:1196 loss:2.034104108810425 \n",
      "iteration:1197 loss:2.0508620738983154 \n",
      "iteration:1198 loss:2.0268404483795166 \n",
      "iteration:1199 loss:2.0305747985839844 \n",
      "iteration:1200 loss:1.9669065475463867 \n",
      "iteration:1201 loss:2.0023772716522217 \n",
      "iteration:1202 loss:1.9929014444351196 \n",
      "iteration:1203 loss:2.0438079833984375 \n",
      "iteration:1204 loss:2.1083953380584717 \n",
      "iteration:1205 loss:2.052403688430786 \n",
      "iteration:1206 loss:1.9870070219039917 \n",
      "iteration:1207 loss:2.0130515098571777 \n",
      "iteration:1208 loss:1.9935386180877686 \n",
      "iteration:1209 loss:2.0522916316986084 \n",
      "iteration:1210 loss:2.061519145965576 \n",
      "iteration:1211 loss:2.0265722274780273 \n",
      "iteration:1212 loss:1.9666650295257568 \n",
      "iteration:1213 loss:2.048973798751831 \n",
      "iteration:1214 loss:2.0471909046173096 \n",
      "iteration:1215 loss:2.0541231632232666 \n",
      "iteration:1216 loss:1.9323182106018066 \n",
      "iteration:1217 loss:2.169825792312622 \n",
      "iteration:1218 loss:2.004149913787842 \n",
      "iteration:1219 loss:1.9596165418624878 \n",
      "iteration:1220 loss:2.001650810241699 \n",
      "iteration:1221 loss:2.0653951168060303 \n",
      "iteration:1222 loss:1.9540395736694336 \n",
      "iteration:1223 loss:1.9701114892959595 \n",
      "iteration:1224 loss:2.0186116695404053 \n",
      "iteration:1225 loss:1.9921029806137085 \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-78aeb726146f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"iteration:{} loss:{} \"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miteration\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[0;32m    219\u001b[0m                 \u001b[0mretain_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    220\u001b[0m                 create_graph=create_graph)\n\u001b[1;32m--> 221\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    222\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    223\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[0;32m    130\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m    131\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 132\u001b[1;33m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[0;32m    133\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    134\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from featureblocks import FeatureBlock3\n",
    "from featureblocks import FeatureBlockGT\n",
    "\n",
    "# The trinity of models\n",
    "model = FeatureBlock3()\n",
    "#model = FeatureBlockGT()\n",
    "# This is the losss function\n",
    "#loss_function = RMSLELoss()\n",
    "loss_function = torch.nn.CrossEntropyLoss()\n",
    "# This is what controls the gradient descent\n",
    "#optimizer = torch.optim.Adadelta(model.parameters(),lr=1)\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=0.001)\n",
    "iteration = 0\n",
    "losses = []\n",
    "\n",
    "for epoch in range(100):\n",
    "    \n",
    "    for index,(x,y) in enumerate(audio_dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        #print(x.size())\n",
    "        x = model(x.float())\n",
    "        #print(x.size())\n",
    "        #print(y.size())\n",
    "        # Use argmax to get class with max probability value from softmax\n",
    "        #x = x.argmax(dim=-1) \n",
    "        x = x.float()\n",
    "        y = y.squeeze(1)\n",
    "        \n",
    "        loss = loss_function(x,y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        print(\"iteration:{} loss:{} \".format(iteration, loss.item()))\n",
    "        losses.append(loss)\n",
    "        iteration += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "shape = [16,16,64]\n",
    "filters = generate_filters(shape[2],shape[0],sample_rate,min_center_freq,order)\n",
    "filters = filters.reshape(filters.shape[1],1,filters.shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-822c80f0913f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlosses\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlosses\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "y = np.array(losses, dtype=float)\n",
    "x = np.arange(len(losses))\n",
    "print(x.dtype)\n",
    "print(y.dtype)\n",
    "\n",
    "m, b = np.polyfit(x, y, 1)\n",
    "plt.plot(x, y, 'o')\n",
    "plt.plot(x, m*x+b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
