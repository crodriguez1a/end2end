{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import librosa\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import pandas as pd\n",
    "import ast\n",
    "import wandb\n",
    "import torch.optim as optim\n",
    "from featureblocks_16khz import FeatureBlock3\n",
    "\n",
    "device = torch.device(\"cuda:2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# divide data into folds\n",
    "\n",
    "model_name = \"model_final_ala.torch\"\n",
    "num_epochs = 2000\n",
    "frame_length = 16000\n",
    "overlapping_fraction = 0.1\n",
    "num_epochs = 1\n",
    "data = torch.load('./torch_dataset_16khz/all_audio_data.pt')\n",
    "\n",
    "# How many folds you want, bitch?\n",
    "k_folds = 10\n",
    "folds = []\n",
    "step_size = int(len(data) / k_folds)\n",
    "\n",
    "# Let's generate some folds baby!\n",
    "for i in range(0, len(data), step_size):\n",
    "    limit = i + step_size\n",
    "    \n",
    "    if limit > len(data):\n",
    "        folds.append(data[i:])\n",
    "    else:\n",
    "        folds.append(data[i:limit])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "374570\n"
     ]
    }
   ],
   "source": [
    "# Load our dumbass model up\n",
    "model = FeatureBlock3().to(device)\n",
    "\n",
    "# This loss function gonna fuck us in the ass\n",
    "loss_function = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# This optimizer is the Jerry Smith of all optimizers.\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=0.001, weight_decay=1e-5)\n",
    "\n",
    "num_params = 0\n",
    "for param in model.parameters():\n",
    "    num_params += param.numel()\n",
    "\n",
    "print(num_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's start this ass spanking of a k-fold cross validation\n",
    "for i in range(len(folds)):\n",
    "    # fold i is the bitch fold here, we aren't touching this ass\n",
    "    data = []\n",
    "    for j in range(len(folds)):\n",
    "        if i != j:\n",
    "            data.extend(folds[j])\n",
    "    \n",
    "    # Set up the training data hoes\n",
    "    data = torch.vstack(data)\n",
    "    X_train = data[:,0:frame_length].clone()\n",
    "    X_train = X_train.reshape(-1, 16,1000)\n",
    "    Y_train = data[:,frame_length:].clone()\n",
    "    \n",
    "    audio_dataset = TensorDataset (X_train, Y_train)\n",
    "    audio_dataloader = DataLoader (audio_dataset, batch_size = 250)\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    print(\"#################################################################################################\")\n",
    "    print(\"#                                           FOLD {}                                              #\".format(i))\n",
    "    print(\"#################################################################################################\")\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss=0\n",
    "        correct=0\n",
    "        total=0\n",
    "\n",
    "        losses = []\n",
    "        for index,(x,y) in enumerate(audio_dataloader):\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            y_hat = y.to(device)\n",
    "            outputs = model(x.float().to(device))\n",
    "            \n",
    "            # Use argmax to get class with max probability value from softmax\n",
    "            outputs = outputs.float()\n",
    "            y_hat = y_hat.float()\n",
    "            \n",
    "            y_hat = y_hat.squeeze(1)\n",
    "            loss = loss_function(outputs, y_hat)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += y_hat.size(0)\n",
    "            correct += predicted.eq(y_hat).sum().item()\n",
    "\n",
    "            accu=100.*correct/total\n",
    "        \n",
    "            print(\"iteration:{} loss:{} | Accuracy: {} \".format(iteration, loss.item(), accu=100.*correct/total))\n",
    "            losses.append(loss.item())\n",
    "            iteration += 1\n",
    "            \n",
    "         # calculate avg loss\n",
    "        avg_loss = sum(losses) / len(losses)\n",
    "\n",
    "        train_loss=running_loss/len(audio_dataloader)\n",
    "        accu=100.*correct/total\n",
    "\n",
    "        train_accu.append(accu)\n",
    "        train_losses.append(train_loss)\n",
    "        print(\"Epoch {} : Train Loss: {} | Accuracy: \".format(i,train_loss,accu))\n",
    "        \n",
    "        # This little bitch ass fold is gonna be our eval fold now\n",
    "        # WHO'S YA PAPI NOW MAMA?\n",
    "        data = folds[i]\n",
    "        \n",
    "        model.eval()\n",
    "        \n",
    "        running_loss=0\n",
    "        correct=0\n",
    "        total=0\n",
    "        losses = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "\n",
    "            for index,(x,y) in enumerate(audio_testloader):\n",
    "\n",
    "                outputs = x.float()\n",
    "                y_hat = y.type(torch.LongTensor)\n",
    "                outputs = outputs.to(device)\n",
    "                y_hat = y_hat.to(device)\n",
    "\n",
    "                model.eval()\n",
    "                outputs = model(outputs)\n",
    "                y_hat = y_hat.squeeze(1)\n",
    "                loss = loss_function(outputs, y_hat)\n",
    "\n",
    "                running_loss += loss.item()\n",
    "                _, predicted = outputs.max(1)\n",
    "                total += y_hat.size(0)\n",
    "                correct += predicted.eq(y_hat).sum().item()\n",
    "\n",
    "                print(\"iteration:{} loss:{} correct: {} / {} \".format(iteration, loss.item(), correct, total))\n",
    "                losses.append(loss)\n",
    "                iteration += 1\n",
    "\n",
    "        test_loss=running_loss/iteration\n",
    "        accu=100.*correct/total\n",
    "\n",
    "        eval_losses.append(test_loss)\n",
    "        eval_accu.append(accu)\n",
    "\n",
    "        print('Test Loss: %.3f | Accuracy: %.3f'%(test_loss,accu)) \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from model_fold1.pt with loss 1.5268910595801024\n",
      "iteration:0 loss:2.021749496459961 correct: 22 / 50 \n",
      "iteration:1 loss:2.0450377464294434 correct: 43 / 100 \n",
      "iteration:2 loss:2.1817328929901123 correct: 57 / 150 \n",
      "iteration:3 loss:2.0585074424743652 correct: 77 / 200 \n",
      "iteration:4 loss:2.2004051208496094 correct: 90 / 250 \n",
      "iteration:5 loss:2.1024515628814697 correct: 108 / 300 \n",
      "iteration:6 loss:2.2134640216827393 correct: 120 / 350 \n",
      "iteration:7 loss:2.218045234680176 correct: 132 / 400 \n",
      "iteration:8 loss:2.1407861709594727 correct: 148 / 450 \n",
      "iteration:9 loss:2.1703293323516846 correct: 163 / 500 \n",
      "iteration:10 loss:2.017130136489868 correct: 185 / 550 \n",
      "iteration:11 loss:2.096034526824951 correct: 203 / 600 \n",
      "iteration:12 loss:2.100111484527588 correct: 221 / 650 \n",
      "iteration:13 loss:2.0910773277282715 correct: 239 / 700 \n",
      "iteration:14 loss:2.1044247150421143 correct: 257 / 750 \n",
      "iteration:15 loss:2.1004021167755127 correct: 275 / 800 \n",
      "iteration:16 loss:2.076277732849121 correct: 295 / 850 \n",
      "iteration:17 loss:2.0404505729675293 correct: 316 / 900 \n",
      "iteration:18 loss:2.141937732696533 correct: 332 / 950 \n",
      "iteration:19 loss:2.1175529956817627 correct: 349 / 1000 \n",
      "iteration:20 loss:2.070310592651367 correct: 368 / 1050 \n",
      "iteration:21 loss:2.139888048171997 correct: 384 / 1100 \n",
      "iteration:22 loss:2.0967650413513184 correct: 402 / 1150 \n",
      "iteration:23 loss:2.2806344032287598 correct: 411 / 1200 \n",
      "iteration:24 loss:2.0251903533935547 correct: 433 / 1250 \n",
      "iteration:25 loss:2.0516774654388428 correct: 453 / 1300 \n",
      "iteration:26 loss:2.1715569496154785 correct: 467 / 1350 \n",
      "iteration:27 loss:2.160320520401001 correct: 482 / 1400 \n",
      "iteration:28 loss:2.1540942192077637 correct: 497 / 1450 \n",
      "iteration:29 loss:2.0622220039367676 correct: 517 / 1500 \n",
      "iteration:30 loss:2.1545467376708984 correct: 532 / 1550 \n",
      "iteration:31 loss:2.0870094299316406 correct: 551 / 1600 \n",
      "iteration:32 loss:2.109595537185669 correct: 569 / 1650 \n",
      "iteration:33 loss:2.1686792373657227 correct: 584 / 1700 \n",
      "iteration:34 loss:2.100424289703369 correct: 602 / 1750 \n",
      "iteration:35 loss:2.2047150135040283 correct: 615 / 1800 \n",
      "iteration:36 loss:2.2102861404418945 correct: 627 / 1850 \n",
      "iteration:37 loss:2.192995071411133 correct: 641 / 1900 \n",
      "iteration:38 loss:2.1182875633239746 correct: 658 / 1950 \n",
      "iteration:39 loss:2.031125068664551 correct: 680 / 2000 \n",
      "iteration:40 loss:2.1330742835998535 correct: 696 / 2050 \n",
      "iteration:41 loss:2.1371190547943115 correct: 712 / 2100 \n",
      "iteration:42 loss:2.1401381492614746 correct: 728 / 2150 \n",
      "iteration:43 loss:2.0255117416381836 correct: 750 / 2200 \n",
      "iteration:44 loss:2.0576131343841553 correct: 771 / 2250 \n",
      "iteration:45 loss:1.9889203310012817 correct: 795 / 2300 \n",
      "iteration:46 loss:2.0454416275024414 correct: 816 / 2350 \n",
      "iteration:47 loss:2.204868793487549 correct: 829 / 2400 \n",
      "iteration:48 loss:1.9583109617233276 correct: 854 / 2450 \n",
      "iteration:49 loss:2.250981330871582 correct: 864 / 2500 \n",
      "iteration:50 loss:2.2004892826080322 correct: 877 / 2550 \n",
      "iteration:51 loss:2.199005365371704 correct: 890 / 2600 \n",
      "iteration:52 loss:2.0889015197753906 correct: 908 / 2650 \n",
      "iteration:53 loss:2.157184600830078 correct: 923 / 2700 \n",
      "iteration:54 loss:2.1447277069091797 correct: 939 / 2750 \n",
      "iteration:55 loss:2.219539165496826 correct: 951 / 2800 \n",
      "iteration:56 loss:2.081528663635254 correct: 970 / 2850 \n",
      "iteration:57 loss:2.1908118724823 correct: 984 / 2900 \n",
      "iteration:58 loss:2.1595828533172607 correct: 999 / 2950 \n",
      "iteration:59 loss:2.1679024696350098 correct: 1013 / 3000 \n",
      "iteration:60 loss:2.050112724304199 correct: 1034 / 3050 \n",
      "iteration:61 loss:2.176426410675049 correct: 1048 / 3100 \n",
      "iteration:62 loss:2.154453992843628 correct: 1063 / 3150 \n",
      "iteration:63 loss:2.1278083324432373 correct: 1065 / 3156 \n",
      "Test Loss: 2.123 | Accuracy: 33.745\n",
      "Loaded model from model_fold2.pt with loss 1.559504688313577\n",
      "iteration:0 loss:1.7191146612167358 correct: 37 / 50 \n",
      "iteration:1 loss:1.6642285585403442 correct: 77 / 100 \n",
      "iteration:2 loss:1.7393574714660645 correct: 113 / 150 \n",
      "iteration:3 loss:1.7051911354064941 correct: 151 / 200 \n",
      "iteration:4 loss:1.6347607374191284 correct: 192 / 250 \n",
      "iteration:5 loss:1.8333604335784912 correct: 223 / 300 \n",
      "iteration:6 loss:1.740752100944519 correct: 259 / 350 \n",
      "iteration:7 loss:1.757869839668274 correct: 294 / 400 \n",
      "iteration:8 loss:1.704487919807434 correct: 332 / 450 \n",
      "iteration:9 loss:1.6196942329406738 correct: 374 / 500 \n",
      "iteration:10 loss:1.7146034240722656 correct: 411 / 550 \n",
      "iteration:11 loss:1.7202825546264648 correct: 448 / 600 \n",
      "iteration:12 loss:1.7490296363830566 correct: 484 / 650 \n",
      "iteration:13 loss:1.7471939325332642 correct: 520 / 700 \n",
      "iteration:14 loss:1.6753710508346558 correct: 559 / 750 \n",
      "iteration:15 loss:1.719267725944519 correct: 596 / 800 \n",
      "iteration:16 loss:1.7205994129180908 correct: 633 / 850 \n",
      "iteration:17 loss:1.7382533550262451 correct: 669 / 900 \n",
      "iteration:18 loss:1.6708252429962158 correct: 708 / 950 \n",
      "iteration:19 loss:1.8243165016174316 correct: 740 / 1000 \n",
      "iteration:20 loss:1.772093653678894 correct: 775 / 1050 \n",
      "iteration:21 loss:1.6731303930282593 correct: 814 / 1100 \n",
      "iteration:22 loss:1.706063985824585 correct: 852 / 1150 \n",
      "iteration:23 loss:1.6634830236434937 correct: 892 / 1200 \n",
      "iteration:24 loss:1.5977222919464111 correct: 935 / 1250 \n",
      "iteration:25 loss:1.7679787874221802 correct: 969 / 1300 \n",
      "iteration:26 loss:1.7391068935394287 correct: 1005 / 1350 \n",
      "iteration:27 loss:1.631585955619812 correct: 1047 / 1400 \n",
      "iteration:28 loss:1.7371528148651123 correct: 1083 / 1450 \n",
      "iteration:29 loss:1.6979460716247559 correct: 1121 / 1500 \n",
      "iteration:30 loss:1.696584939956665 correct: 1159 / 1550 \n",
      "iteration:31 loss:1.764801025390625 correct: 1194 / 1600 \n",
      "iteration:32 loss:1.689414620399475 correct: 1233 / 1650 \n",
      "iteration:33 loss:1.6564033031463623 correct: 1273 / 1700 \n",
      "iteration:34 loss:1.6680370569229126 correct: 1313 / 1750 \n",
      "iteration:35 loss:1.7408164739608765 correct: 1349 / 1800 \n",
      "iteration:36 loss:1.8511499166488647 correct: 1379 / 1850 \n",
      "iteration:37 loss:1.6600629091262817 correct: 1419 / 1900 \n",
      "iteration:38 loss:1.6601637601852417 correct: 1459 / 1950 \n",
      "iteration:39 loss:1.760243535041809 correct: 1494 / 2000 \n",
      "iteration:40 loss:1.7410016059875488 correct: 1530 / 2050 \n",
      "iteration:41 loss:1.7456307411193848 correct: 1566 / 2100 \n",
      "iteration:42 loss:1.775193691253662 correct: 1601 / 2150 \n",
      "iteration:43 loss:1.8146709203720093 correct: 1633 / 2200 \n",
      "iteration:44 loss:1.6420992612838745 correct: 1674 / 2250 \n",
      "iteration:45 loss:1.681014895439148 correct: 1713 / 2300 \n",
      "iteration:46 loss:1.6740268468856812 correct: 1752 / 2350 \n",
      "iteration:47 loss:1.7361094951629639 correct: 1788 / 2400 \n",
      "iteration:48 loss:1.6202839612960815 correct: 1830 / 2450 \n",
      "iteration:49 loss:1.7804566621780396 correct: 1864 / 2500 \n",
      "iteration:50 loss:1.6610195636749268 correct: 1904 / 2550 \n",
      "iteration:51 loss:1.5710111856460571 correct: 1948 / 2600 \n",
      "iteration:52 loss:1.7027145624160767 correct: 1986 / 2650 \n",
      "iteration:53 loss:1.7021888494491577 correct: 2024 / 2700 \n",
      "iteration:54 loss:1.6216267347335815 correct: 2066 / 2750 \n",
      "iteration:55 loss:1.6993974447250366 correct: 2104 / 2800 \n",
      "iteration:56 loss:1.6992801427841187 correct: 2142 / 2850 \n",
      "iteration:57 loss:1.7797144651412964 correct: 2176 / 2900 \n",
      "iteration:58 loss:1.7598390579223633 correct: 2211 / 2950 \n",
      "iteration:59 loss:1.7122447490692139 correct: 2248 / 3000 \n",
      "iteration:60 loss:1.7413771152496338 correct: 2284 / 3050 \n",
      "iteration:61 loss:1.7800250053405762 correct: 2318 / 3100 \n",
      "iteration:62 loss:1.7665191888809204 correct: 2353 / 3150 \n",
      "iteration:63 loss:1.6278167963027954 correct: 2358 / 3156 \n",
      "Test Loss: 1.712 | Accuracy: 74.715\n",
      "Loaded model from model_fold3.pt with loss 1.5519925951957703\n",
      "iteration:0 loss:1.7772632837295532 correct: 34 / 50 \n",
      "iteration:1 loss:1.7221201658248901 correct: 71 / 100 \n",
      "iteration:2 loss:1.6509606838226318 correct: 112 / 150 \n",
      "iteration:3 loss:1.7572298049926758 correct: 148 / 200 \n",
      "iteration:4 loss:1.726769208908081 correct: 184 / 250 \n",
      "iteration:5 loss:1.7793606519699097 correct: 218 / 300 \n",
      "iteration:6 loss:1.6309351921081543 correct: 260 / 350 \n",
      "iteration:7 loss:1.7952275276184082 correct: 293 / 400 \n",
      "iteration:8 loss:1.6929289102554321 correct: 332 / 450 \n",
      "iteration:9 loss:1.736212968826294 correct: 368 / 500 \n",
      "iteration:10 loss:1.760427713394165 correct: 403 / 550 \n",
      "iteration:11 loss:1.6021056175231934 correct: 446 / 600 \n",
      "iteration:12 loss:1.7993459701538086 correct: 479 / 650 \n",
      "iteration:13 loss:1.6886496543884277 correct: 518 / 700 \n",
      "iteration:14 loss:1.6633803844451904 correct: 559 / 750 \n",
      "iteration:15 loss:1.7500839233398438 correct: 595 / 800 \n",
      "iteration:16 loss:1.7822117805480957 correct: 629 / 850 \n",
      "iteration:17 loss:1.6226428747177124 correct: 671 / 900 \n",
      "iteration:18 loss:1.7047581672668457 correct: 709 / 950 \n",
      "iteration:19 loss:1.7008670568466187 correct: 747 / 1000 \n",
      "iteration:20 loss:1.670345664024353 correct: 786 / 1050 \n",
      "iteration:21 loss:1.7802287340164185 correct: 820 / 1100 \n",
      "iteration:22 loss:1.7570252418518066 correct: 855 / 1150 \n",
      "iteration:23 loss:1.6731500625610352 correct: 894 / 1200 \n",
      "iteration:24 loss:1.6585874557495117 correct: 933 / 1250 \n",
      "iteration:25 loss:1.6214267015457153 correct: 975 / 1300 \n",
      "iteration:26 loss:1.6761727333068848 correct: 1014 / 1350 \n",
      "iteration:27 loss:1.7801192998886108 correct: 1048 / 1400 \n",
      "iteration:28 loss:1.8394105434417725 correct: 1079 / 1450 \n",
      "iteration:29 loss:1.7029577493667603 correct: 1117 / 1500 \n",
      "iteration:30 loss:1.760099172592163 correct: 1152 / 1550 \n",
      "iteration:31 loss:1.7383936643600464 correct: 1188 / 1600 \n",
      "iteration:32 loss:1.6981995105743408 correct: 1226 / 1650 \n",
      "iteration:33 loss:1.6918646097183228 correct: 1264 / 1700 \n",
      "iteration:34 loss:1.7719836235046387 correct: 1299 / 1750 \n",
      "iteration:35 loss:1.6597495079040527 correct: 1339 / 1800 \n",
      "iteration:36 loss:1.7155508995056152 correct: 1377 / 1850 \n",
      "iteration:37 loss:1.7596690654754639 correct: 1412 / 1900 \n",
      "iteration:38 loss:1.6920264959335327 correct: 1451 / 1950 \n",
      "iteration:39 loss:1.7603175640106201 correct: 1486 / 2000 \n",
      "iteration:40 loss:1.7007815837860107 correct: 1524 / 2050 \n",
      "iteration:41 loss:1.7421902418136597 correct: 1560 / 2100 \n",
      "iteration:42 loss:1.7566211223602295 correct: 1595 / 2150 \n",
      "iteration:43 loss:1.6786036491394043 correct: 1634 / 2200 \n",
      "iteration:44 loss:1.6222853660583496 correct: 1676 / 2250 \n",
      "iteration:45 loss:1.7181668281555176 correct: 1713 / 2300 \n",
      "iteration:46 loss:1.7791900634765625 correct: 1747 / 2350 \n",
      "iteration:47 loss:1.7060661315917969 correct: 1784 / 2400 \n",
      "iteration:48 loss:1.8006582260131836 correct: 1817 / 2450 \n",
      "iteration:49 loss:1.6750494241714478 correct: 1857 / 2500 \n",
      "iteration:50 loss:1.743988037109375 correct: 1893 / 2550 \n",
      "iteration:51 loss:1.7599889039993286 correct: 1928 / 2600 \n",
      "iteration:52 loss:1.7529935836791992 correct: 1963 / 2650 \n",
      "iteration:53 loss:1.7802035808563232 correct: 1997 / 2700 \n",
      "iteration:54 loss:1.7112998962402344 correct: 2035 / 2750 \n",
      "iteration:55 loss:1.8211232423782349 correct: 2067 / 2800 \n",
      "iteration:56 loss:1.63712739944458 correct: 2109 / 2850 \n",
      "iteration:57 loss:1.6729618310928345 correct: 2148 / 2900 \n",
      "iteration:58 loss:1.722029685974121 correct: 2185 / 2950 \n",
      "iteration:59 loss:1.6409387588500977 correct: 2226 / 3000 \n",
      "iteration:60 loss:1.6174262762069702 correct: 2268 / 3050 \n",
      "iteration:61 loss:1.5818208456039429 correct: 2312 / 3100 \n",
      "iteration:62 loss:1.7338787317276 correct: 2348 / 3150 \n",
      "iteration:63 loss:1.9611501693725586 correct: 2351 / 3156 \n",
      "Test Loss: 1.720 | Accuracy: 74.493\n",
      "Loaded model from model_fold4.pt with loss 1.5669887949313437\n",
      "iteration:0 loss:1.7033889293670654 correct: 38 / 50 \n",
      "iteration:1 loss:1.6795454025268555 correct: 77 / 100 \n",
      "iteration:2 loss:1.7057430744171143 correct: 115 / 150 \n",
      "iteration:3 loss:1.9089010953903198 correct: 143 / 200 \n",
      "iteration:4 loss:1.7579755783081055 correct: 178 / 250 \n",
      "iteration:5 loss:1.7073215246200562 correct: 216 / 300 \n",
      "iteration:6 loss:1.7170735597610474 correct: 253 / 350 \n",
      "iteration:7 loss:1.6659555435180664 correct: 293 / 400 \n",
      "iteration:8 loss:1.8067927360534668 correct: 325 / 450 \n",
      "iteration:9 loss:1.6811552047729492 correct: 364 / 500 \n",
      "iteration:10 loss:1.5651288032531738 correct: 409 / 550 \n",
      "iteration:11 loss:1.7205783128738403 correct: 446 / 600 \n",
      "iteration:12 loss:1.620527982711792 correct: 488 / 650 \n",
      "iteration:13 loss:1.6964409351348877 correct: 526 / 700 \n",
      "iteration:14 loss:1.7530254125595093 correct: 561 / 750 \n",
      "iteration:15 loss:1.7642812728881836 correct: 596 / 800 \n",
      "iteration:16 loss:1.6832785606384277 correct: 635 / 850 \n",
      "iteration:17 loss:1.7290507555007935 correct: 671 / 900 \n",
      "iteration:18 loss:1.897924780845642 correct: 699 / 950 \n",
      "iteration:19 loss:1.6630038022994995 correct: 739 / 1000 \n",
      "iteration:20 loss:1.7267314195632935 correct: 776 / 1050 \n",
      "iteration:21 loss:1.7352070808410645 correct: 812 / 1100 \n",
      "iteration:22 loss:1.8526666164398193 correct: 842 / 1150 \n",
      "iteration:23 loss:1.8208683729171753 correct: 874 / 1200 \n",
      "iteration:24 loss:1.745124340057373 correct: 910 / 1250 \n",
      "iteration:25 loss:1.7838616371154785 correct: 944 / 1300 \n",
      "iteration:26 loss:1.7248610258102417 correct: 981 / 1350 \n",
      "iteration:27 loss:1.8255982398986816 correct: 1013 / 1400 \n",
      "iteration:28 loss:1.8550751209259033 correct: 1043 / 1450 \n",
      "iteration:29 loss:1.660592794418335 correct: 1083 / 1500 \n",
      "iteration:30 loss:1.60115647315979 correct: 1126 / 1550 \n",
      "iteration:31 loss:1.6413557529449463 correct: 1167 / 1600 \n",
      "iteration:32 loss:1.7557549476623535 correct: 1202 / 1650 \n",
      "iteration:33 loss:1.6978801488876343 correct: 1240 / 1700 \n",
      "iteration:34 loss:1.7196296453475952 correct: 1277 / 1750 \n",
      "iteration:35 loss:1.6864949464797974 correct: 1316 / 1800 \n",
      "iteration:36 loss:1.7143064737319946 correct: 1353 / 1850 \n",
      "iteration:37 loss:1.7800586223602295 correct: 1387 / 1900 \n",
      "iteration:38 loss:1.7605907917022705 correct: 1422 / 1950 \n",
      "iteration:39 loss:1.7133580446243286 correct: 1460 / 2000 \n",
      "iteration:40 loss:1.6864770650863647 correct: 1499 / 2050 \n",
      "iteration:41 loss:1.843218207359314 correct: 1530 / 2100 \n",
      "iteration:42 loss:1.7604244947433472 correct: 1565 / 2150 \n",
      "iteration:43 loss:1.7271437644958496 correct: 1602 / 2200 \n",
      "iteration:44 loss:1.7009501457214355 correct: 1640 / 2250 \n",
      "iteration:45 loss:1.6959904432296753 correct: 1678 / 2300 \n",
      "iteration:46 loss:1.7093596458435059 correct: 1715 / 2350 \n",
      "iteration:47 loss:1.775537371635437 correct: 1749 / 2400 \n",
      "iteration:48 loss:1.7712852954864502 correct: 1783 / 2450 \n",
      "iteration:49 loss:1.701614260673523 correct: 1821 / 2500 \n",
      "iteration:50 loss:1.8252267837524414 correct: 1852 / 2550 \n",
      "iteration:51 loss:1.6608657836914062 correct: 1892 / 2600 \n",
      "iteration:52 loss:1.8030608892440796 correct: 1925 / 2650 \n",
      "iteration:53 loss:1.8200558423995972 correct: 1957 / 2700 \n",
      "iteration:54 loss:1.7603305578231812 correct: 1992 / 2750 \n",
      "iteration:55 loss:1.6800177097320557 correct: 2031 / 2800 \n",
      "iteration:56 loss:1.6998977661132812 correct: 2069 / 2850 \n",
      "iteration:57 loss:1.7748059034347534 correct: 2103 / 2900 \n",
      "iteration:58 loss:1.720718264579773 correct: 2140 / 2950 \n",
      "iteration:59 loss:1.738420009613037 correct: 2176 / 3000 \n",
      "iteration:60 loss:1.6809899806976318 correct: 2215 / 3050 \n",
      "iteration:61 loss:1.7919669151306152 correct: 2248 / 3100 \n",
      "iteration:62 loss:1.7211577892303467 correct: 2285 / 3150 \n",
      "iteration:63 loss:1.7944823503494263 correct: 2289 / 3156 \n",
      "Test Loss: 1.736 | Accuracy: 72.529\n",
      "Loaded model from model_fold5.pt with loss 1.558563940292966\n",
      "iteration:0 loss:1.7746994495391846 correct: 34 / 50 \n",
      "iteration:1 loss:1.7125493288040161 correct: 71 / 100 \n",
      "iteration:2 loss:1.6815532445907593 correct: 110 / 150 \n",
      "iteration:3 loss:1.74080228805542 correct: 146 / 200 \n",
      "iteration:4 loss:1.880531668663025 correct: 175 / 250 \n",
      "iteration:5 loss:1.7632811069488525 correct: 210 / 300 \n",
      "iteration:6 loss:1.621230125427246 correct: 252 / 350 \n",
      "iteration:7 loss:1.739071249961853 correct: 288 / 400 \n",
      "iteration:8 loss:1.8946080207824707 correct: 316 / 450 \n",
      "iteration:9 loss:1.7194637060165405 correct: 353 / 500 \n",
      "iteration:10 loss:1.7648231983184814 correct: 388 / 550 \n",
      "iteration:11 loss:1.7807917594909668 correct: 422 / 600 \n",
      "iteration:12 loss:1.7714935541152954 correct: 457 / 650 \n",
      "iteration:13 loss:1.71834397315979 correct: 494 / 700 \n",
      "iteration:14 loss:1.8211268186569214 correct: 526 / 750 \n",
      "iteration:15 loss:1.760848045349121 correct: 561 / 800 \n",
      "iteration:16 loss:1.7204735279083252 correct: 598 / 850 \n",
      "iteration:17 loss:1.625614047050476 correct: 640 / 900 \n",
      "iteration:18 loss:1.7158310413360596 correct: 677 / 950 \n",
      "iteration:19 loss:1.6783446073532104 correct: 716 / 1000 \n",
      "iteration:20 loss:1.6909348964691162 correct: 754 / 1050 \n",
      "iteration:21 loss:1.797071099281311 correct: 787 / 1100 \n",
      "iteration:22 loss:1.7833198308944702 correct: 821 / 1150 \n",
      "iteration:23 loss:1.7906899452209473 correct: 854 / 1200 \n",
      "iteration:24 loss:1.7818551063537598 correct: 888 / 1250 \n",
      "iteration:25 loss:1.7611825466156006 correct: 923 / 1300 \n",
      "iteration:26 loss:1.7301642894744873 correct: 959 / 1350 \n",
      "iteration:27 loss:1.7782739400863647 correct: 993 / 1400 \n",
      "iteration:28 loss:1.8399494886398315 correct: 1024 / 1450 \n",
      "iteration:29 loss:1.7598015069961548 correct: 1059 / 1500 \n",
      "iteration:30 loss:1.7007707357406616 correct: 1097 / 1550 \n",
      "iteration:31 loss:1.8077409267425537 correct: 1130 / 1600 \n",
      "iteration:32 loss:1.824141263961792 correct: 1162 / 1650 \n",
      "iteration:33 loss:1.8013489246368408 correct: 1195 / 1700 \n",
      "iteration:34 loss:1.7788949012756348 correct: 1229 / 1750 \n",
      "iteration:35 loss:1.7242097854614258 correct: 1266 / 1800 \n",
      "iteration:36 loss:1.8040217161178589 correct: 1299 / 1850 \n",
      "iteration:37 loss:1.7197233438491821 correct: 1336 / 1900 \n",
      "iteration:38 loss:1.619045376777649 correct: 1378 / 1950 \n",
      "iteration:39 loss:1.7225215435028076 correct: 1415 / 2000 \n",
      "iteration:40 loss:2.0004630088806152 correct: 1438 / 2050 \n",
      "iteration:41 loss:1.7696970701217651 correct: 1472 / 2100 \n",
      "iteration:42 loss:1.7072970867156982 correct: 1510 / 2150 \n",
      "iteration:43 loss:1.681150197982788 correct: 1549 / 2200 \n",
      "iteration:44 loss:1.832074761390686 correct: 1580 / 2250 \n",
      "iteration:45 loss:1.7805850505828857 correct: 1614 / 2300 \n",
      "iteration:46 loss:1.782075047492981 correct: 1648 / 2350 \n",
      "iteration:47 loss:1.7598103284835815 correct: 1683 / 2400 \n",
      "iteration:48 loss:1.8792588710784912 correct: 1712 / 2450 \n",
      "iteration:49 loss:1.7899055480957031 correct: 1746 / 2500 \n",
      "iteration:50 loss:1.7189886569976807 correct: 1783 / 2550 \n",
      "iteration:51 loss:1.6957957744598389 correct: 1821 / 2600 \n",
      "iteration:52 loss:1.7382615804672241 correct: 1857 / 2650 \n",
      "iteration:53 loss:1.6350451707839966 correct: 1898 / 2700 \n",
      "iteration:54 loss:1.7244198322296143 correct: 1935 / 2750 \n",
      "iteration:55 loss:1.7492033243179321 correct: 1970 / 2800 \n",
      "iteration:56 loss:1.7412850856781006 correct: 2006 / 2850 \n",
      "iteration:57 loss:1.724868655204773 correct: 2043 / 2900 \n",
      "iteration:58 loss:1.6424254179000854 correct: 2084 / 2950 \n",
      "iteration:59 loss:1.6750340461730957 correct: 2123 / 3000 \n",
      "iteration:60 loss:1.7235934734344482 correct: 2160 / 3050 \n",
      "iteration:61 loss:1.8356938362121582 correct: 2191 / 3100 \n",
      "iteration:62 loss:1.7808125019073486 correct: 2225 / 3150 \n",
      "iteration:63 loss:1.6279388666152954 correct: 2230 / 3156 \n",
      "Test Loss: 1.752 | Accuracy: 70.659\n",
      "Loaded model from model_fold6.pt with loss 1.5597555689644396\n",
      "iteration:0 loss:1.7606030702590942 correct: 35 / 50 \n",
      "iteration:1 loss:1.7177000045776367 correct: 72 / 100 \n",
      "iteration:2 loss:1.6772091388702393 correct: 112 / 150 \n",
      "iteration:3 loss:1.7404903173446655 correct: 148 / 200 \n",
      "iteration:4 loss:1.7282260656356812 correct: 185 / 250 \n",
      "iteration:5 loss:1.6951777935028076 correct: 223 / 300 \n",
      "iteration:6 loss:1.818514347076416 correct: 255 / 350 \n",
      "iteration:7 loss:1.7204304933547974 correct: 292 / 400 \n",
      "iteration:8 loss:1.6711355447769165 correct: 332 / 450 \n",
      "iteration:9 loss:1.881250023841858 correct: 361 / 500 \n",
      "iteration:10 loss:1.7594035863876343 correct: 396 / 550 \n",
      "iteration:11 loss:1.701279878616333 correct: 434 / 600 \n",
      "iteration:12 loss:1.7334518432617188 correct: 470 / 650 \n",
      "iteration:13 loss:1.619535207748413 correct: 512 / 700 \n",
      "iteration:14 loss:1.8311635255813599 correct: 544 / 750 \n",
      "iteration:15 loss:1.7000561952590942 correct: 582 / 800 \n",
      "iteration:16 loss:1.6811680793762207 correct: 621 / 850 \n",
      "iteration:17 loss:1.7761684656143188 correct: 655 / 900 \n",
      "iteration:18 loss:1.746809482574463 correct: 691 / 950 \n",
      "iteration:19 loss:1.8028922080993652 correct: 724 / 1000 \n",
      "iteration:20 loss:1.6625394821166992 correct: 764 / 1050 \n",
      "iteration:21 loss:1.721196174621582 correct: 801 / 1100 \n",
      "iteration:22 loss:1.6896088123321533 correct: 839 / 1150 \n",
      "iteration:23 loss:1.6946717500686646 correct: 877 / 1200 \n",
      "iteration:24 loss:1.8293150663375854 correct: 908 / 1250 \n",
      "iteration:25 loss:1.7224650382995605 correct: 945 / 1300 \n",
      "iteration:26 loss:1.692920207977295 correct: 983 / 1350 \n",
      "iteration:27 loss:1.7603622674942017 correct: 1018 / 1400 \n",
      "iteration:28 loss:1.7292556762695312 correct: 1054 / 1450 \n",
      "iteration:29 loss:1.7043662071228027 correct: 1092 / 1500 \n",
      "iteration:30 loss:1.5810036659240723 correct: 1136 / 1550 \n",
      "iteration:31 loss:1.734243631362915 correct: 1172 / 1600 \n",
      "iteration:32 loss:1.7001256942749023 correct: 1210 / 1650 \n",
      "iteration:33 loss:1.6223362684249878 correct: 1252 / 1700 \n",
      "iteration:34 loss:1.7012077569961548 correct: 1290 / 1750 \n",
      "iteration:35 loss:1.6406015157699585 correct: 1331 / 1800 \n",
      "iteration:36 loss:1.7963603734970093 correct: 1364 / 1850 \n",
      "iteration:37 loss:1.6778260469436646 correct: 1403 / 1900 \n",
      "iteration:38 loss:1.827500581741333 correct: 1435 / 1950 \n",
      "iteration:39 loss:1.6881235837936401 correct: 1473 / 2000 \n",
      "iteration:40 loss:1.7889751195907593 correct: 1506 / 2050 \n",
      "iteration:41 loss:1.6583346128463745 correct: 1546 / 2100 \n",
      "iteration:42 loss:1.6837990283966064 correct: 1585 / 2150 \n",
      "iteration:43 loss:1.7211494445800781 correct: 1622 / 2200 \n",
      "iteration:44 loss:1.6616746187210083 correct: 1662 / 2250 \n",
      "iteration:45 loss:1.771134376525879 correct: 1696 / 2300 \n",
      "iteration:46 loss:1.798660159111023 correct: 1729 / 2350 \n",
      "iteration:47 loss:1.682795763015747 correct: 1768 / 2400 \n",
      "iteration:48 loss:1.7811567783355713 correct: 1802 / 2450 \n",
      "iteration:49 loss:1.8708806037902832 correct: 1831 / 2500 \n",
      "iteration:50 loss:1.6604514122009277 correct: 1871 / 2550 \n",
      "iteration:51 loss:1.8333064317703247 correct: 1902 / 2600 \n",
      "iteration:52 loss:1.6994346380233765 correct: 1940 / 2650 \n",
      "iteration:53 loss:1.7192425727844238 correct: 1977 / 2700 \n",
      "iteration:54 loss:1.752435564994812 correct: 2012 / 2750 \n",
      "iteration:55 loss:1.777998685836792 correct: 2046 / 2800 \n",
      "iteration:56 loss:1.7674381732940674 correct: 2081 / 2850 \n",
      "iteration:57 loss:1.819383978843689 correct: 2113 / 2900 \n",
      "iteration:58 loss:1.7532118558883667 correct: 2148 / 2950 \n",
      "iteration:59 loss:1.6215993165969849 correct: 2190 / 3000 \n",
      "iteration:60 loss:1.7208627462387085 correct: 2227 / 3050 \n",
      "iteration:61 loss:1.8216060400009155 correct: 2259 / 3100 \n",
      "iteration:62 loss:1.7654142379760742 correct: 2294 / 3150 \n",
      "iteration:63 loss:1.950574278831482 correct: 2297 / 3156 \n",
      "Test Loss: 1.735 | Accuracy: 72.782\n",
      "Loaded model from model_fold7.pt with loss 1.5571675426081608\n",
      "iteration:0 loss:1.6611109972000122 correct: 40 / 50 \n",
      "iteration:1 loss:1.8293288946151733 correct: 71 / 100 \n",
      "iteration:2 loss:1.701403021812439 correct: 109 / 150 \n",
      "iteration:3 loss:1.8141463994979858 correct: 141 / 200 \n",
      "iteration:4 loss:1.7415696382522583 correct: 177 / 250 \n",
      "iteration:5 loss:1.8009593486785889 correct: 210 / 300 \n",
      "iteration:6 loss:1.7155169248580933 correct: 247 / 350 \n",
      "iteration:7 loss:1.7231167554855347 correct: 283 / 400 \n",
      "iteration:8 loss:1.6320977210998535 correct: 323 / 450 \n",
      "iteration:9 loss:1.7389273643493652 correct: 359 / 500 \n",
      "iteration:10 loss:1.7965275049209595 correct: 392 / 550 \n",
      "iteration:11 loss:1.8608540296554565 correct: 421 / 600 \n",
      "iteration:12 loss:1.7292407751083374 correct: 458 / 650 \n",
      "iteration:13 loss:1.6605732440948486 correct: 498 / 700 \n",
      "iteration:14 loss:1.5848687887191772 correct: 542 / 750 \n",
      "iteration:15 loss:1.6422375440597534 correct: 583 / 800 \n",
      "iteration:16 loss:1.764671802520752 correct: 618 / 850 \n",
      "iteration:17 loss:1.7602014541625977 correct: 653 / 900 \n",
      "iteration:18 loss:1.7634598016738892 correct: 688 / 950 \n",
      "iteration:19 loss:1.742398738861084 correct: 724 / 1000 \n",
      "iteration:20 loss:1.5898624658584595 correct: 768 / 1050 \n",
      "iteration:21 loss:1.8188890218734741 correct: 800 / 1100 \n",
      "iteration:22 loss:1.694787621498108 correct: 838 / 1150 \n",
      "iteration:23 loss:1.6845955848693848 correct: 877 / 1200 \n",
      "iteration:24 loss:1.6583939790725708 correct: 917 / 1250 \n",
      "iteration:25 loss:1.7211816310882568 correct: 954 / 1300 \n",
      "iteration:26 loss:1.8187305927276611 correct: 986 / 1350 \n",
      "iteration:27 loss:1.763858675956726 correct: 1021 / 1400 \n",
      "iteration:28 loss:1.8632519245147705 correct: 1051 / 1450 \n",
      "iteration:29 loss:1.8364306688308716 correct: 1082 / 1500 \n",
      "iteration:30 loss:1.6212571859359741 correct: 1124 / 1550 \n",
      "iteration:31 loss:1.8061538934707642 correct: 1157 / 1600 \n",
      "iteration:32 loss:1.801028847694397 correct: 1190 / 1650 \n",
      "iteration:33 loss:1.798828125 correct: 1223 / 1700 \n",
      "iteration:34 loss:1.7844783067703247 correct: 1257 / 1750 \n",
      "iteration:35 loss:1.722713828086853 correct: 1294 / 1800 \n",
      "iteration:36 loss:1.7787845134735107 correct: 1328 / 1850 \n",
      "iteration:37 loss:1.6903871297836304 correct: 1367 / 1900 \n",
      "iteration:38 loss:1.7592211961746216 correct: 1402 / 1950 \n",
      "iteration:39 loss:1.6209228038787842 correct: 1444 / 2000 \n",
      "iteration:40 loss:1.716719388961792 correct: 1481 / 2050 \n",
      "iteration:41 loss:1.7219222784042358 correct: 1518 / 2100 \n",
      "iteration:42 loss:1.6790285110473633 correct: 1557 / 2150 \n",
      "iteration:43 loss:1.8570045232772827 correct: 1587 / 2200 \n",
      "iteration:44 loss:1.785651445388794 correct: 1621 / 2250 \n",
      "iteration:45 loss:1.6629222631454468 correct: 1661 / 2300 \n",
      "iteration:46 loss:1.7723788022994995 correct: 1695 / 2350 \n",
      "iteration:47 loss:1.8446316719055176 correct: 1726 / 2400 \n",
      "iteration:48 loss:1.7214138507843018 correct: 1763 / 2450 \n",
      "iteration:49 loss:1.620835781097412 correct: 1805 / 2500 \n",
      "iteration:50 loss:1.700607419013977 correct: 1843 / 2550 \n",
      "iteration:51 loss:1.7023429870605469 correct: 1881 / 2600 \n",
      "iteration:52 loss:1.6572352647781372 correct: 1922 / 2650 \n",
      "iteration:53 loss:1.792730689048767 correct: 1955 / 2700 \n",
      "iteration:54 loss:1.7318947315216064 correct: 1991 / 2750 \n",
      "iteration:55 loss:1.8282485008239746 correct: 2023 / 2800 \n",
      "iteration:56 loss:1.780124306678772 correct: 2057 / 2850 \n",
      "iteration:57 loss:1.7244024276733398 correct: 2094 / 2900 \n",
      "iteration:58 loss:1.7193423509597778 correct: 2131 / 2950 \n",
      "iteration:59 loss:1.7412428855895996 correct: 2167 / 3000 \n",
      "iteration:60 loss:1.7514104843139648 correct: 2202 / 3050 \n",
      "iteration:61 loss:1.8449243307113647 correct: 2233 / 3100 \n",
      "iteration:62 loss:1.7932593822479248 correct: 2267 / 3150 \n",
      "iteration:63 loss:1.6267746686935425 correct: 2272 / 3156 \n",
      "Test Loss: 1.739 | Accuracy: 71.990\n",
      "Loaded model from model_fold8.pt with loss 1.5664043727128403\n",
      "iteration:0 loss:1.8353285789489746 correct: 31 / 50 \n",
      "iteration:1 loss:1.821040391921997 correct: 63 / 100 \n",
      "iteration:2 loss:1.9021821022033691 correct: 91 / 150 \n",
      "iteration:3 loss:1.8320509195327759 correct: 122 / 200 \n",
      "iteration:4 loss:1.834324598312378 correct: 153 / 250 \n",
      "iteration:5 loss:1.9050768613815308 correct: 181 / 300 \n",
      "iteration:6 loss:1.9200091361999512 correct: 208 / 350 \n",
      "iteration:7 loss:1.846737027168274 correct: 239 / 400 \n",
      "iteration:8 loss:1.9024763107299805 correct: 267 / 450 \n",
      "iteration:9 loss:1.8537118434906006 correct: 297 / 500 \n",
      "iteration:10 loss:1.9109708070755005 correct: 325 / 550 \n",
      "iteration:11 loss:1.873225212097168 correct: 354 / 600 \n",
      "iteration:12 loss:1.8759217262268066 correct: 383 / 650 \n",
      "iteration:13 loss:2.03290057182312 correct: 404 / 700 \n",
      "iteration:14 loss:1.8463068008422852 correct: 435 / 750 \n",
      "iteration:15 loss:1.8631385564804077 correct: 465 / 800 \n",
      "iteration:16 loss:1.7790123224258423 correct: 499 / 850 \n",
      "iteration:17 loss:1.9165607690811157 correct: 526 / 900 \n",
      "iteration:18 loss:1.8621656894683838 correct: 556 / 950 \n",
      "iteration:19 loss:1.7680805921554565 correct: 591 / 1000 \n",
      "iteration:20 loss:1.826690673828125 correct: 623 / 1050 \n",
      "iteration:21 loss:1.8636301755905151 correct: 653 / 1100 \n",
      "iteration:22 loss:1.8388160467147827 correct: 684 / 1150 \n",
      "iteration:23 loss:1.8423118591308594 correct: 715 / 1200 \n",
      "iteration:24 loss:1.9328111410140991 correct: 741 / 1250 \n",
      "iteration:25 loss:1.8656386137008667 correct: 771 / 1300 \n",
      "iteration:26 loss:1.8035670518875122 correct: 803 / 1350 \n",
      "iteration:27 loss:1.8169997930526733 correct: 835 / 1400 \n",
      "iteration:28 loss:1.9151132106781006 correct: 862 / 1450 \n",
      "iteration:29 loss:1.7301133871078491 correct: 899 / 1500 \n",
      "iteration:30 loss:1.8197886943817139 correct: 931 / 1550 \n",
      "iteration:31 loss:1.68544602394104 correct: 970 / 1600 \n",
      "iteration:32 loss:1.906335473060608 correct: 998 / 1650 \n",
      "iteration:33 loss:2.008364200592041 correct: 1021 / 1700 \n",
      "iteration:34 loss:1.7841519117355347 correct: 1055 / 1750 \n",
      "iteration:35 loss:1.84482741355896 correct: 1086 / 1800 \n",
      "iteration:36 loss:1.9208555221557617 correct: 1113 / 1850 \n",
      "iteration:37 loss:1.8020089864730835 correct: 1146 / 1900 \n",
      "iteration:38 loss:1.872347354888916 correct: 1175 / 1950 \n",
      "iteration:39 loss:1.7769114971160889 correct: 1209 / 2000 \n",
      "iteration:40 loss:1.7817959785461426 correct: 1243 / 2050 \n",
      "iteration:41 loss:1.8934074640274048 correct: 1271 / 2100 \n",
      "iteration:42 loss:1.7722935676574707 correct: 1306 / 2150 \n",
      "iteration:43 loss:1.862127661705017 correct: 1336 / 2200 \n",
      "iteration:44 loss:1.8617444038391113 correct: 1366 / 2250 \n",
      "iteration:45 loss:1.87589430809021 correct: 1395 / 2300 \n",
      "iteration:46 loss:1.8316361904144287 correct: 1426 / 2350 \n",
      "iteration:47 loss:1.7224647998809814 correct: 1463 / 2400 \n",
      "iteration:48 loss:1.7748969793319702 correct: 1497 / 2450 \n",
      "iteration:49 loss:1.6463613510131836 correct: 1538 / 2500 \n",
      "iteration:50 loss:1.9611679315567017 correct: 1563 / 2550 \n",
      "iteration:51 loss:1.9198336601257324 correct: 1591 / 2600 \n",
      "iteration:52 loss:1.8710969686508179 correct: 1620 / 2650 \n",
      "iteration:53 loss:1.850874900817871 correct: 1651 / 2700 \n",
      "iteration:54 loss:1.8243616819381714 correct: 1683 / 2750 \n",
      "iteration:55 loss:1.8425756692886353 correct: 1714 / 2800 \n",
      "iteration:56 loss:1.9391143321990967 correct: 1740 / 2850 \n",
      "iteration:57 loss:1.8004403114318848 correct: 1773 / 2900 \n",
      "iteration:58 loss:1.986311674118042 correct: 1796 / 2950 \n",
      "iteration:59 loss:1.7935174703598022 correct: 1830 / 3000 \n",
      "iteration:60 loss:1.9170095920562744 correct: 1857 / 3050 \n",
      "iteration:61 loss:1.8558458089828491 correct: 1887 / 3100 \n",
      "iteration:62 loss:1.7801553010940552 correct: 1921 / 3150 \n",
      "iteration:63 loss:2.127816915512085 correct: 1923 / 3156 \n",
      "Test Loss: 1.855 | Accuracy: 60.932\n",
      "Loaded model from model_fold9.pt with loss 1.563192459574917\n",
      "iteration:0 loss:1.8207218647003174 correct: 32 / 50 \n",
      "iteration:1 loss:1.765017032623291 correct: 67 / 100 \n",
      "iteration:2 loss:1.8113930225372314 correct: 99 / 150 \n",
      "iteration:3 loss:1.8930832147598267 correct: 127 / 200 \n",
      "iteration:4 loss:1.7549248933792114 correct: 162 / 250 \n",
      "iteration:5 loss:1.7607917785644531 correct: 197 / 300 \n",
      "iteration:6 loss:1.87833833694458 correct: 226 / 350 \n",
      "iteration:7 loss:1.7486467361450195 correct: 262 / 400 \n",
      "iteration:8 loss:1.7990072965621948 correct: 295 / 450 \n",
      "iteration:9 loss:1.7662180662155151 correct: 330 / 500 \n",
      "iteration:10 loss:1.7851128578186035 correct: 363 / 550 \n",
      "iteration:11 loss:1.7487918138504028 correct: 398 / 600 \n",
      "iteration:12 loss:1.7572463750839233 correct: 433 / 650 \n",
      "iteration:13 loss:1.8583952188491821 correct: 463 / 700 \n",
      "iteration:14 loss:1.8445250988006592 correct: 494 / 750 \n",
      "iteration:15 loss:1.753471851348877 correct: 529 / 800 \n",
      "iteration:16 loss:1.7672311067581177 correct: 564 / 850 \n",
      "iteration:17 loss:1.9182946681976318 correct: 591 / 900 \n",
      "iteration:18 loss:1.8568813800811768 correct: 621 / 950 \n",
      "iteration:19 loss:1.7238306999206543 correct: 658 / 1000 \n",
      "iteration:20 loss:1.6687566041946411 correct: 698 / 1050 \n",
      "iteration:21 loss:1.7499816417694092 correct: 733 / 1100 \n",
      "iteration:22 loss:1.858557105064392 correct: 763 / 1150 \n",
      "iteration:23 loss:1.9020981788635254 correct: 791 / 1200 \n",
      "iteration:24 loss:1.7948869466781616 correct: 824 / 1250 \n",
      "iteration:25 loss:1.9567533731460571 correct: 849 / 1300 \n",
      "iteration:26 loss:1.8467991352081299 correct: 880 / 1350 \n",
      "iteration:27 loss:1.7439159154891968 correct: 916 / 1400 \n",
      "iteration:28 loss:1.8647593259811401 correct: 946 / 1450 \n",
      "iteration:29 loss:1.7217804193496704 correct: 983 / 1500 \n",
      "iteration:30 loss:1.9009690284729004 correct: 1011 / 1550 \n",
      "iteration:31 loss:1.8199087381362915 correct: 1043 / 1600 \n",
      "iteration:32 loss:1.7805225849151611 correct: 1077 / 1650 \n",
      "iteration:33 loss:1.8024429082870483 correct: 1110 / 1700 \n",
      "iteration:34 loss:1.8190668821334839 correct: 1142 / 1750 \n",
      "iteration:35 loss:1.7600795030593872 correct: 1177 / 1800 \n",
      "iteration:36 loss:1.7497104406356812 correct: 1213 / 1850 \n",
      "iteration:37 loss:1.8418564796447754 correct: 1244 / 1900 \n",
      "iteration:38 loss:1.8845715522766113 correct: 1273 / 1950 \n",
      "iteration:39 loss:1.8156589269638062 correct: 1306 / 2000 \n",
      "iteration:40 loss:1.6586570739746094 correct: 1346 / 2050 \n",
      "iteration:41 loss:1.7922712564468384 correct: 1379 / 2100 \n",
      "iteration:42 loss:1.7222000360488892 correct: 1416 / 2150 \n",
      "iteration:43 loss:1.6414135694503784 correct: 1457 / 2200 \n",
      "iteration:44 loss:1.7970130443572998 correct: 1490 / 2250 \n",
      "iteration:45 loss:1.6967204809188843 correct: 1528 / 2300 \n",
      "iteration:46 loss:1.7146931886672974 correct: 1565 / 2350 \n",
      "iteration:47 loss:1.714073896408081 correct: 1602 / 2400 \n",
      "iteration:48 loss:1.813210129737854 correct: 1634 / 2450 \n",
      "iteration:49 loss:1.8201205730438232 correct: 1666 / 2500 \n",
      "iteration:50 loss:1.8411078453063965 correct: 1697 / 2550 \n",
      "iteration:51 loss:1.7732826471328735 correct: 1732 / 2600 \n",
      "iteration:52 loss:1.8213450908660889 correct: 1764 / 2650 \n",
      "iteration:53 loss:1.9825471639633179 correct: 1788 / 2700 \n",
      "iteration:54 loss:1.6198724508285522 correct: 1830 / 2750 \n",
      "iteration:55 loss:1.8124765157699585 correct: 1862 / 2800 \n",
      "iteration:56 loss:1.781964898109436 correct: 1896 / 2850 \n",
      "iteration:57 loss:1.8217872381210327 correct: 1928 / 2900 \n",
      "iteration:58 loss:1.7009981870651245 correct: 1966 / 2950 \n",
      "iteration:59 loss:1.9009088277816772 correct: 1994 / 3000 \n",
      "iteration:60 loss:1.6860421895980835 correct: 2033 / 3050 \n",
      "iteration:61 loss:1.9400200843811035 correct: 2059 / 3100 \n",
      "iteration:62 loss:1.7024714946746826 correct: 2097 / 3150 \n",
      "iteration:63 loss:1.8177722692489624 correct: 2101 / 3156 \n",
      "Test Loss: 1.795 | Accuracy: 66.572\n",
      "Loaded model from model_fold10.pt with loss 1.5491295686939306\n",
      "iteration:0 loss:1.7169445753097534 correct: 37 / 50 \n",
      "iteration:1 loss:1.740935206413269 correct: 73 / 100 \n",
      "iteration:2 loss:1.76737642288208 correct: 108 / 150 \n",
      "iteration:3 loss:1.6621413230895996 correct: 148 / 200 \n",
      "iteration:4 loss:1.7571499347686768 correct: 183 / 250 \n",
      "iteration:5 loss:1.7783015966415405 correct: 217 / 300 \n",
      "iteration:6 loss:1.654964804649353 correct: 257 / 350 \n",
      "iteration:7 loss:1.7211991548538208 correct: 294 / 400 \n",
      "iteration:8 loss:1.7211300134658813 correct: 331 / 450 \n",
      "iteration:9 loss:1.7190077304840088 correct: 368 / 500 \n",
      "iteration:10 loss:1.8466755151748657 correct: 399 / 550 \n",
      "iteration:11 loss:1.7758334875106812 correct: 433 / 600 \n",
      "iteration:12 loss:1.7404677867889404 correct: 469 / 650 \n",
      "iteration:13 loss:1.5998378992080688 correct: 512 / 700 \n",
      "iteration:14 loss:1.6633820533752441 correct: 552 / 750 \n",
      "iteration:15 loss:1.7435717582702637 correct: 587 / 800 \n",
      "iteration:16 loss:1.7010685205459595 correct: 625 / 850 \n",
      "iteration:17 loss:1.6673283576965332 correct: 664 / 900 \n",
      "iteration:18 loss:1.6420187950134277 correct: 705 / 950 \n",
      "iteration:19 loss:1.7691177129745483 correct: 739 / 1000 \n",
      "iteration:20 loss:1.692387342453003 correct: 777 / 1050 \n",
      "iteration:21 loss:1.6683752536773682 correct: 817 / 1100 \n",
      "iteration:22 loss:1.6795164346694946 correct: 856 / 1150 \n",
      "iteration:23 loss:1.6487245559692383 correct: 897 / 1200 \n",
      "iteration:24 loss:1.802464485168457 correct: 930 / 1250 \n",
      "iteration:25 loss:1.6812920570373535 correct: 969 / 1300 \n",
      "iteration:26 loss:1.6497125625610352 correct: 1010 / 1350 \n",
      "iteration:27 loss:1.8001301288604736 correct: 1042 / 1400 \n",
      "iteration:28 loss:1.7366060018539429 correct: 1078 / 1450 \n",
      "iteration:29 loss:1.6827540397644043 correct: 1117 / 1500 \n",
      "iteration:30 loss:1.7002127170562744 correct: 1155 / 1550 \n",
      "iteration:31 loss:1.7003358602523804 correct: 1193 / 1600 \n",
      "iteration:32 loss:1.727623701095581 correct: 1230 / 1650 \n",
      "iteration:33 loss:1.6935237646102905 correct: 1268 / 1700 \n",
      "iteration:34 loss:1.6891475915908813 correct: 1307 / 1750 \n",
      "iteration:35 loss:1.7465980052947998 correct: 1343 / 1800 \n",
      "iteration:36 loss:1.6371607780456543 correct: 1384 / 1850 \n",
      "iteration:37 loss:1.8343411684036255 correct: 1416 / 1900 \n",
      "iteration:38 loss:1.6889004707336426 correct: 1454 / 1950 \n",
      "iteration:39 loss:1.6793140172958374 correct: 1493 / 2000 \n",
      "iteration:40 loss:1.750349760055542 correct: 1528 / 2050 \n",
      "iteration:41 loss:1.687441349029541 correct: 1567 / 2100 \n",
      "iteration:42 loss:1.6812015771865845 correct: 1606 / 2150 \n",
      "iteration:43 loss:1.7214818000793457 correct: 1643 / 2200 \n",
      "iteration:44 loss:1.6813284158706665 correct: 1682 / 2250 \n",
      "iteration:45 loss:1.8284966945648193 correct: 1714 / 2300 \n",
      "iteration:46 loss:1.7182711362838745 correct: 1751 / 2350 \n",
      "iteration:47 loss:1.7185606956481934 correct: 1788 / 2400 \n",
      "iteration:48 loss:1.6809837818145752 correct: 1827 / 2450 \n",
      "iteration:49 loss:1.7084834575653076 correct: 1865 / 2500 \n",
      "iteration:50 loss:1.7094085216522217 correct: 1903 / 2550 \n",
      "iteration:51 loss:1.6804096698760986 correct: 1942 / 2600 \n",
      "iteration:52 loss:1.6653326749801636 correct: 1982 / 2650 \n",
      "iteration:53 loss:1.6740198135375977 correct: 2021 / 2700 \n",
      "iteration:54 loss:1.7592872381210327 correct: 2056 / 2750 \n",
      "iteration:55 loss:1.6464054584503174 correct: 2096 / 2800 \n",
      "iteration:56 loss:1.7036503553390503 correct: 2134 / 2850 \n",
      "iteration:57 loss:1.7208998203277588 correct: 2171 / 2900 \n",
      "iteration:58 loss:1.7441859245300293 correct: 2207 / 2950 \n",
      "iteration:59 loss:1.7198948860168457 correct: 2244 / 3000 \n",
      "iteration:60 loss:1.629976749420166 correct: 2286 / 3050 \n",
      "iteration:61 loss:1.6739883422851562 correct: 2325 / 3100 \n",
      "iteration:62 loss:1.7788432836532593 correct: 2359 / 3150 \n",
      "iteration:63 loss:1.6278166770935059 correct: 2364 / 3156 \n",
      "Test Loss: 1.710 | Accuracy: 74.905\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import pandas as pd\n",
    "import ast\n",
    "from os import path\n",
    "import math\n",
    "import sys\n",
    "\n",
    "class RMSLELoss(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.mse = torch.nn.MSELoss()\n",
    "        \n",
    "    def forward(self, pred, actual):\n",
    "        return self.mse(torch.log(pred.float() + 1), torch.log(actual.float() + 1))\n",
    "\n",
    "from featureblocks_16khz import FeatureBlock3\n",
    "\n",
    "\n",
    "frame_length = 16000\n",
    "overlapping_fraction = 0.1\n",
    "data = torch.load('./folded_dataset_16khz/fold1.pt')\n",
    "\n",
    "\n",
    "# Shuffle dataset before we do anything\n",
    "#data=data[torch.randperm(data.size()[0])]\n",
    "#X_train = data[:,0:frame_length].clone()\n",
    "#test_portion = int(0.80*((data.size())[0]))\n",
    "\n",
    "X_eval = data[:,0:frame_length].clone()\n",
    "X_eval = X_eval.reshape(-1, 16, 1000)\n",
    "Y_eval = data[:,frame_length:].clone()\n",
    "\n",
    "\n",
    "audio_testset = TensorDataset (X_eval, Y_eval)\n",
    "audio_testloader = DataLoader (audio_testset, batch_size = 50, shuffle= True)\n",
    "\n",
    "test_accuracies = []\n",
    "test_losses = []\n",
    "for i in range(1, 11):\n",
    "    model_name = \"model_fold{}.pt\".format(i)\n",
    "    # The trinity of models\n",
    "    model = FeatureBlock3().to(device)\n",
    "    loss_function = torch.nn.CrossEntropyLoss()\n",
    "    T_max = 500\n",
    "    iteration = 0\n",
    "\n",
    "\n",
    "    # Load model if exists\n",
    "    if path.exists(model_name):\n",
    "        checkpoint = torch.load(model_name, map_location=lambda storage, loc: storage.cuda(2))\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        #scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "        best_loss = checkpoint['loss']\n",
    "        print(\"Loaded model from {} with loss {}\".format(model_name, checkpoint['loss']))\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    train_accu = []\n",
    "    train_losses = []\n",
    "\n",
    "    eval_losses=[]\n",
    "    eval_accu=[]\n",
    "\n",
    "    running_loss=0\n",
    "    correct=0\n",
    "    total=0\n",
    "    losses = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        for index,(x,y) in enumerate(audio_testloader):\n",
    "\n",
    "            outputs = x.float()\n",
    "            y_hat = y.type(torch.LongTensor)\n",
    "            outputs = outputs.to(device)\n",
    "            y_hat = y_hat.to(device)\n",
    "\n",
    "            model.eval()\n",
    "            outputs = model(outputs)\n",
    "            y_hat = y_hat.squeeze(1)\n",
    "            #print(\"y size:{} outputs size:{}\".format(y_hat.size(),outputs.size()))\n",
    "            loss = loss_function(outputs, y_hat)\n",
    "\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += y_hat.size(0)\n",
    "            correct += predicted.eq(y_hat).sum().item()\n",
    "\n",
    "            print(\"iteration:{} loss:{} correct: {} / {} \".format(iteration, loss.item(), correct, total))\n",
    "            losses.append(loss)\n",
    "            iteration += 1\n",
    "\n",
    "    test_loss=running_loss/iteration\n",
    "    accu=100.*correct/total\n",
    "\n",
    "    eval_losses.append(test_loss)\n",
    "    eval_accu.append(accu)\n",
    "\n",
    "    print('Test Loss: %.3f | Accuracy: %.3f'%(test_loss,accu)) \n",
    "    test_accuracies.append(accu)\n",
    "    test_losses.append(test_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[33.745247148288975,\n",
       " 74.7148288973384,\n",
       " 74.49302915082383,\n",
       " 72.52851711026616,\n",
       " 70.65906210392902,\n",
       " 72.78200253485424,\n",
       " 71.98986058301648,\n",
       " 60.93155893536122,\n",
       " 66.57160963244614,\n",
       " 74.90494296577947]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAagAAAETCAYAAABjv5J2AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8rg+JYAAAACXBIWXMAAAsTAAALEwEAmpwYAAAjnUlEQVR4nO3de5gU5Zn+8e8NyMlBEEFAwUCQ04CgOwR1ExWMB+LGcyAKi5rEkJhkN9G44u4vK+q6e5mTcV39rRoTgweErEZNjMRFRYwkMToqkTNoVFCIB0AEBBnm2T+qxjTjTE/DTE/XMPfnuuaiq+qtqrurh37mfau6WhGBmZlZ1rQpdQAzM7O6uECZmVkmuUCZmVkmuUCZmVkmuUCZmVkmuUCZmVkmtSt1AGu5KisrD2zXrt1twAj8x45lWzWwqKqq6sKKioo3Sx3GCuMCZXusXbt2t/Xu3XtYz549N7Rp08YfqLPMqq6u1ltvvVW+bt2624DTSp3HCuO/eq0xRvTs2XOTi5NlXZs2baJnz57vkvT2rYVwgbLGaOPiZC1F+rvq97wWxC+WtUjr1q1rO3To0PKhQ4eW9+jRY9SBBx44smZ627Ztyrfuk08+2fmCCy7o19A+jjjiiKFNl7hlOfLIIwffd999++XOu/rqqw+cPHnyIXW1HzNmzJAnn3yyM8Bxxx136Ntvv922dptLLrnkoCuuuKJXvv3eeeed3SorKzvWTH/rW9866IEHHuiyZ8/CWjqfg7KmM1MVTbq9SVFZ36LevXvvXLZs2RJI3vjKysp2Xn311X+pWb5jxw722WefOtc99thjtx577LFbG9r9888/v2wPUhfFVbqqSY/t9Jhe77EFmDBhwvp77rmn+9lnn72pZt59993X/dprr13T0Lbnz5+/ak9zPfDAA92qqqreraio2AZw/fXXv7Gn27KWzz0o22ucffbZ/SdNmnTIyJEjh1500UV9582b1/nwww8fOmzYsPIjjjhi6MKFCzsAPPTQQ13GjRt3KCTFbcKECf3HjBkzpG/fvoddc801B9Zsr3PnzkfUtB8zZsyQ8ePHf3zAgAHDTzvttAHV1dUAzJ49u+uAAQOGDx8+fNgFF1zQr2a7Ld2UKVM2PP74411reqPLly9v/+abb+5z1113dR8xYsSwQw89dPjFF198UF3rHnzwwYetXbu2HcC0adN69+/ff0RFRcWQlStXdqhp88Mf/rDHiBEjhg0ZMqT85JNPHvjee++1mTt37r6PPvpot+985zt9hw4dWr548eIOZ599dv/bb799f4AHH3ywy7Bhw8oHDx5cPmHChP7vv/++avZ38cUXH1ReXj5s8ODB5c8//3zHunJZy+MCZXuVtWvXtn/uueeW3XbbbWtGjRq17Zlnnlm2dOnSJdOnT3/9sssu61vXOqtWreo4f/78Fc8888zSH/zgBwdt3779I0OES5cu7XTTTTetXrVq1eLXXnutw9y5c8u2bt2qb37zmx+bM2fOysWLFy9955139poRiV69eu0cNWrUlnvvvbcrwIwZM7qfeuqpG6677rrXFy1atHTZsmWLFyxY0OXpp5/uVN82fvvb33a+//77u7/44otL5s6du3LhwoX71iybPHnyhkWLFi1dvnz5kiFDhrx/ww039DjxxBO3nHDCCRuvueaaNcuWLVsyfPjw7TXtt27dqq985SsDZs+e/dKKFSuWVFVV8f3vf79nzfIePXpULVmyZOkXv/jFt6699tq8w4jWcrhA2V7lrLPO2tCuXVIn1q9f3/aUU04ZOGjQoOGXXXZZvxUrVtT5l/VJJ520sVOnTtGnT5+q7t2771izZs1HCs1hhx22ZeDAgTvatm3L8OHDt7700kvtX3jhhY79+vXbPnTo0A8AzjnnnPVFfXLNbOLEietnz569P8AvfvGL7lOmTFk/Y8aM7uXl5cPKy8vLV65c2XHhwoX19lbmzZtXdsopp2zs0qVLdffu3atPOumkjTXLKisrO1VUVAwZPHhw+X333XfA4sWL8/Z6Fi5c2LFv377bR44cuR3gggsueOepp5768NzUpEmTNgCMGTNm6+rVqzvUtx1rWVygbK9SVlZWXfN42rRpBx933HHvrVy5cvGvfvWrVR988EGdv+8dOnT48ErEtm3bUlVV9ZEeVCFt9jaTJk3auGDBgv2eeuqpztu2bWvTs2fPqhtvvLHX/PnzV6xYsWLJ8ccf/+62bdv26D1k6tSpA2688cbXVqxYsWTatGlvbN++vVHvRR07dgyAdu3aRWt4bVoLFyjba23atKlt3759PwC45ZZbejT19keOHLlt9erVHZYvX94eYPbs2d2beh+l1LVr1+qjjz76vQsvvLD/mWeeuX7Dhg1tO3XqVN29e/edq1evbvfEE090zbf+8ccfv/nhhx/utnnzZm3YsKHN3Llzu9Us27p1a5tDDjlkx/bt2zVr1qwPj1tZWdnOTZs2feR9adSoUdtef/319osWLeoAcMcddxxwzDHHvNeET9cyyAXK9lrTpk1bd+WVV/YdNmxYeVVVVZNvv6ysLK677rpXx48fP2j48OHDysrKdnbp0mVnk++ohM4555z1y5cv73TeeeetP/roo98fMWLE1oEDB46YOHHixysqKjbnW/dTn/rU1jPPPHP9iBEjhp9wwgmDRo4cuaVm2eWXX/7GmDFjho0ePXrooEGDttXMnzx58vobbrih97Bhw8oXL1784VBd586d4+abb35lwoQJAwcPHlzepk0bLr300reK86wtK+SvfLc9tXDhwldGjRr1dqlzlNK7777bpmvXrtXV1dWcd955hwwaNGjb9OnTfa+3jFq4cGGPUaNG9S91DiuMe1BmjXD99df3GDp0aPmgQYOGb9q0qe0ll1zSqgu2WVNyD8r2mHtQ1tK4B9WyuAdlZmaZ5AJljVFdXV3tS3qtRUh/V6sbbGiZ4QJljbHorbfe6uoiZVmXfh9UV2BRqbNY4faaW7NY86uqqrpw3bp1t61bt87fqGtZ9+E36pY6iBXOF0mYmVkm+a9eMzPLJBcoMzPLJBcoMzPLJBcoMzPLJBcoMzPLJBcoMzPLJBcoMzPLJBcoMzPLJBcoMzPLJBcoMzPLJBcoMzPLJBcoMzPLJBcoMzPLJBcoMzPLJBcoazaS5kg6v6nbtjSSQtKh6eObJf1rIW33YD+TJf3vnuY0KzV/H5TlJWlzzmRnYDuwM53+SkTc3fypSkvSb4A/RsQVteafDtwC9I2IqjzrBzAoIlYVsK+C2krqD/wZ2CffvpuSpAHAS8AtEXFRc+zTWhf3oCyviCir+QFeA07NmfdhcZLUmr6deQbw95Jqf9X9FODu5ioQGXAesAH4vKQOzbljSW2bc39WGi5QtkckjZW0RtI0SeuA2yXtL+khSW9J2pA+7puzzhOSLkwfXyDpKUk/SNv+WdJn9rDtAElPSnpP0qOSbpJ0Vz25l0r6bM50uzTv30jqKOkuSe9I2ijpGUm96tjMA8ABwDE529kf+Cxwh6Qxkn6fbmOtpBslta8nz88kXZMz/U/pOm9I+mKttn8n6XlJmyStlnRlzuIn0383Stos6eia45az/t+mz+nd9N+/rXW8/03SgvQ4/q+kHnVlTtuLpEB9B9gBnFpr+emSXkizviRpfDq/u6Tb0+e3QdID6fxdsqbzcodCfybpvyU9LGkLMK6B44GkT0n6Xfo6rE738QlJf8ktcJLOkrSwvudqpeMCZY3RG+gOfAyYSvL7dHs6fQjwPnBjnvWPBJYDPYDvAT+po1dSSNuZwB9JisaVJD2Z+twDnJszfTLwdkQ8B5wPdAX6pdv6avocdhER7wM/J3mDrjERWBYRC0mGQC9Osx4NfBr4Wp5MAKRv4pcCJwKDgBNqNdmS7rMb8HfARZLOSJcdm/7bLe3d/r7WtrsDvwZuSJ/bdcCvJR2Q02wS8AXgQKB9mqU+nwL6ArNIjsWH5wsljQHuAP4pzXos8Eq6+E6SoeLh6X5+lGcftU0C/h3oAjxFnuMh6WPAHOC/gJ7A4cALEfEM8A5wUs52p6R5LWNcoKwxqoHpEbE9It6PiHci4r6I2BoR75G8mRyXZ/1XI+LHEbGTZNisD1BXj6XetpIOAT4BXBERH0TEU8Av8+xzJnCapM7p9CSSogVJT+AA4NCI2BkRlRGxqZ7tzAA+J6ljOn1eOo90vT9ERFVEvEJyXirfcagxEbg9IhZFxBaSYvuhiHgiIl6MiOqI+FOau5DtQvIGvjIi7kxz3QMsY9eez+0RsSKnAB+eZ3vnA3MiYgPJMR0v6cB02ZeAn0bE3DTr6xGxTFIf4DPAVyNiQ0TsiIj5BeYHeDAiFqTb3NbA8ZgEPBoR96T7eSciXkiXzQD+Hj4s3Cenz8EyxgXKGuOtiNhWMyGps6RbJL0qaRPJsFM31X++YF3Ng4jYmj4s2822BwHrc+YBrK4vcHqxwVLg1LRIncZf35zuBB4BZqVDUN+TtE8923kKeBs4Q9JAYEzNdiQNVjK8uS49Dv9B0ptqyEG1sr+au1DSkZLmpUOS75L08ArZbs22X60171Xg4JzpdTmPt1LPayGpEzABuBsg7a29RlIUIOmBvlTHqv1IXqsNBWaubZfXtYHjUV8GgLtIXv99Sf4o+G1ErN3DTFZELlDWGLUvAf02MAQ4MiL246/DTvUN2zWFtUD3nB4RJG9O+dQM850OLKm5Qi79S/uqiCgH/pbknNJ59W+GO9Llfw88EhF/Sef/N0nvZFB6HP6Fwo7B2lrZD6m1fCZJ77BfRHQFbs7ZbkOX475BMvSa6xDg9QJy1XYmsB/w/9MivI6k0NUM860GBtax3mqS16pbHcu2kAz9ASCpdx1taj/HfMejvgxExOvA74GzSIb37qyrnZWeC5Q1pS4k52w2pkMn04u9w4h4FXgWuFJSe0lHU+uEfR1mkZyDuIicoR1J4yQdlvb4NpEM+VXn2c4dJOeJvkw6vJfqkq6/WdLQdD+F+DlwgaTytODWPn5dSHog29LzPJNylr2VZv14Pdt+GBgsaZKSC0M+D5QDDxWYLdf5wE+Bw0iGAQ8HPgmMknQY8BPgC5I+LamNpIMlDU17KXNICtv+kvaRVPNHzEJguKTD02HTKwvIke943A2cIGli+nwPkHR4zvI7gMvS5/CLPTgG1gxcoKwpXQ90Ihn6+gPwm2ba72SSixHeAa4BZpN8XqtO6Rvl70l6SbNzFvUG7iUpLkuB+eT56zo9v/Q7YF92Pe91Kcmb5XvAj2vto14RMYfkGD4OrEr/zfU14GpJ7wFXkBS0mnW3kpzzW5BetXZUrW2/Q9Ij/DbJcboM+GxEvF1IthqSDia56OP6iFiX81NJ8nqfHxF/JLnY4kfAuyTHsab3NoWk8C8D3gS+leZbAVwNPAqsJLkIoiH5jsdrwCnp810PvACMyln3/jTT/bWGhy1D/EFd2+tImk1yRV3Re3DWckl6ieTD5o+WOovVzT0oa/HSz7YMTIeTxpOcW3qgxLEswySdTXJOq3Yv1TKkNX363/ZevUnOIxwArAEuiojnSxvJskrSEyTn36ZERL5zjFZiHuIzM7NM8hCfmZllkguUmZllUos4B9WjR4/o379/0fezZcsW9t1336Lvp1DOk5/z5Oc8+TlPfs2Zp7Ky8u2I6PmRBRGR+Z+KiopoDvPmzWuW/RTKefJznvycJz/nya858wDPRh3v/R7iMzOzTHKBMjOzTHKBMjOzTHKBMjOzTHKBMjOzTHKBMjOzTHKBMjOzTGoRH9RtEjML+ELTg+YVP4eZmRXEPSgzM8skFygzM8skFygzM8uk1nMOyvLzOTqz1qOF/H93gTKzxmshb3jWsrhAlYr/Q7csWXu9spYna7J2fLKWp4VwgbJs8n9os1bPF0mYmVkmuUCZmVkmuUCZmVkmuUCZmVkmuUCZmVkmuUCZmVkmuUCZmVkmuUCZmVkmuUCZmVkmuUCZmVkmuUCZmVkmuUCZmVkmFbVASbpY0mJJiyTdI6mjpAGSnpa0StJsSe2LmcHMzFqmohUoSQcD/wiMjogRQFvgHOC7wI8i4lBgA/ClYmUwM7OWq9hDfO2ATpLaAZ2BtcDxwL3p8hnAGUXOYGZmLVDRClREvA78AHiNpDC9C1QCGyOiKm22Bji4WBnMzKzlUkQUZ8PS/sB9wOeBjcD/kPScrkyH95DUD5iTDgHWXn8qMBWgV69eFbNmzWpcoPWVDTbZ3H4IZWVljdtPoZwnP+fJz3nyc578MpZn3LhxlRExuvb8YhaoCcD4iPhSOn0ecDQwAegdEVWSjiYpWCfn29bo0aPj2WefbVygAr6h9YmD5jF27NjG7adQzpOf8+TnPPk5T34ZyyOpzgJVzHNQrwFHSeosScCngSXAPOBzaZvzgQeLmMHMzFqoYp6DeppkSO854MV0X7cC04BLJK0CDgB+UqwMZmbWcrUr5sYjYjowvdbsl4ExxdyvmZm1fL6ThJmZZZILlJmZZZILlJmZZZILlJmZZZILlJmZZVJRr+KzxllbuZarxl2Vt830qH2RZPFkLY+ZFU8W/r+7B2VmZpnkAmVmZpnkIT6zvVQWhmjMGsMFylosvwGb7d1coMysWfgPCttdPgdlZmaZ5B6UWRNxD8GsabkHZWZmmeQCZWZmmeQCZWZmmeQCZWZmmeQCZWZmmeQCZWZmmeQCZWZmmeTPQZmZZYA/R/dR7kGZmVkmuUCZmVkmuUCZmVkmNVigJFVK+rqk/ZsjkJmZGRTWg/o8cBDwjKRZkk6WpCLnMjOzVq7BAhURqyLi/wGDgZnAT4FXJV0lqXuxA5qZWetU0DkoSSOBHwLfB+4DJgCbgMeLF83MzFqzBj8HJakS2Aj8BLg8Irani56W9MkG1u0G3AaMAAL4IrAcmA30B14BJkbEhj1Kb2Zme61CelATIuLTETEzpzgBEBFnNbDufwK/iYihwChgKXA58FhEDAIeS6fNzMx2UcidJC6U9L2I2AiQXs337Yj4Tr6VJHUFjgUuAIiID4APJJ0OjE2bzQCeAKbtQXYzsz3mOzdkXyE9qM/UFCeAdDjulALWGwC8Bdwu6XlJt0naF+gVEWvTNuuAXruZ2czMWgFFRP4G0p+AT9QM70nqBDwbEcMbWG808AfgkxHxtKT/JLmw4h8ioltOuw0R8ZHPWEmaCkwF6NWrV8WsWbN264l9xPrKBptsbj+EsrKyxu2nUAXkWb9jANvXbM/bpk9FH+dxHudxnhadZ9y4cZURMbr2/EKG+O4GHpN0ezr9BZKhuYasAdZExNPp9L0k55v+IqlPRKyV1Ad4s66VI+JW4FaA0aNHx9ixYwvYZR4zxzXY5Inu82j0fgpVQJ571s5kxaUr8rY5N851HudxHudp2Xnq0WCBiojvpr2oT6ez/i0iHilgvXWSVksaEhHL0/WXpD/nA9em/z64x+nNzGyvVdDXbUTEHGDOHmz/H4C7JbUHXibpfbUBfi7pS8CrwMQ92K6Zme3lCvkc1FHAfwHDgPZAW2BLROzX0LoR8QLwkXFF/tobMzMzq1MhV/HdCJwLrAQ6ARcCNxUzlJmZWUG3OoqIVUDbiNgZEbcD44sby8zMWrtCzkFtTc8hvSDpe8Ba/D1SZmZWZIUUmilpu28AW4B+wNnFDGVmZpa3ByWpLfAfETEZ2Abkvy+ImZlZE8nbg4qIncDH0iE+MzOzZlPIOaiXgQWSfkkyxAdARFxXtFRmZtbqFVKgXkp/2gBdihvHzMwsUcitjnzeyczMml0hd5KYR/JtuLuIiOOLksjMzIzChvguzXnckeQS86rixDEzM0sUMsRX+4tDFkj6Y5HymJmZAYUN8XXPmWwDVABdi5bIzMyMwob4KknOQYlkaO/PwJeKGcrMzKyQIb4BzRHEzMwsV4P34pP0dUndcqb3l/S1oqYyM7NWr5CbxX45IjbWTETEBuDLRUtkZmZGYQWqrSTVTKQ3kPW9+czMrKgKuUjiN8BsSbek019J55mZmRVNIQVqGjAVuCidngvcVrREZmZmFFagOgE/joib4cMhvg7A1mIGMzOz1q2Qc1CPkRSpGp2AR4sTx8zMLFFIgeoYEZtrJtLHnYsXyczMrLACtUXS39RMSKoA3i9eJDMzs8LOQX0L+B9Jb5Dc7qg38PlihjIzMyvkVkfPSBoKDElnLQe651nFzMys0QoZ4iMidgBrgCNJPgP1fDFDmZmZ5e1BSeoEnA5MAo4AugBnAE8WPZmZmbVq9fagJM0EVgAnAv8F9Ac2RMQTEVFd6A4ktZX0vKSH0ukBkp6WtErSbEm+bZKZmX1EviG+cmADsBRYGhE7Sb4Xand9M91Gje8CP4qIQ9Pt+7ulzMzsI+otUBFxODCRZFjvUUlPAV0k9Sp045L6An9Hemuk9KazxwP3pk1mkAwZmpmZ7SLvRRIRsSwipkfEUJKe0AzgGUm/K3D71wOXATVDggcAGyOiKp1eAxy826nNzGyvp4jdG7VLe0HHRETeCyUkfRY4JSK+JmkscClwAfCHdHgPSf2AORExoo71p5LcpJZevXpVzJo1a7dyfsT6ygabbG4/hLKyssbtp1AF5Fm/YwDb12zP26ZPRR/ncR7ncZ4WnWfcuHGVETG69vxCPqi7i0gqWiFX8X0SOE3SKUBHYD/gP4Fuktqlvai+wOv17OdW4FaA0aNHx9ixY3c36q5mjmuwyRPd59Ho/RSqgDz3rJ3JiktX5G1zbpzrPM7jPM7TsvPUo6DPQe2JiPjniOgbEf2Bc4DHI2IyMA/4XNrsfODBYmUwM7OWq2gFKo9pwCWSVpGck/pJCTKYmVnGFTzEJ+ko4EqS4brrI+KBQteNiCeAJ9LHLwNjdiOjmZm1QvUWKEm9I2JdzqxLgDNJbhj7NPBAcaOZmVlrlq8HdbOk54DvRcQ2YCPJuaNqYFMzZDMzs1Ys3wd1zyC5KexDks4j+dqNDiTnjc5ohmxmZtaKNfRB3V8BJwNdgfuBFRFxQ0S81RzhzMys9cp3s9jTJM0j+XqNRSRfUni6pFmSBjZXQDMza53ynYO6huRqu07AIxExBvi2pEHAv5N8tsnMzKwo8hWod4GzgM7AmzUzI2Ile2lxWlu5lqvGXZW3zfSY3kxpzMxat3znoM4kuSCiHckXFpqZmTWbentQEfE2yRcVmpmZNbtS3OrIzMysQS5QZmaWSS5QZmaWSS5QZmaWSS5QZmaWSS5QZmaWSS5QZmaWSS5QZmaWSS5QZmaWSS5QZmaWSS5QZmaWSS5QZmaWSS5QZmaWSS5QZmaWSS5QZmaWSS5QZmaWSS5QZmaWSS5QZmaWSUUrUJL6SZonaYmkxZK+mc7vLmmupJXpv/sXK4OZmbVcxexBVQHfjohy4Cjg65LKgcuBxyJiEPBYOm1mZraLohWoiFgbEc+lj98DlgIHA6cDM9JmM4AzipXBzMxarmY5ByWpP3AE8DTQKyLWpovWAb2aI4OZmbUsioji7kAqA+YD/x4Rv5C0MSK65SzfEBEfOQ8laSowFaBXr14Vs2bNalyQ9ZUNN9kxgO1rtudt06eiT+NyOI/zOI/zOM8uxo0bVxkRo2vPb9ckW6+HpH2A+4C7I+IX6ey/SOoTEWsl9QHerGvdiLgVuBVg9OjRMXbs2MaFmTmuwSb3rJ3JiktX5G1zbpzbuBzO4zzO4zzOU5BiXsUn4CfA0oi4LmfRL4Hz08fnAw8WK4OZmbVcxexBfRKYArwo6YV03r8A1wI/l/Ql4FVgYhEzmJlZC1W0AhURTwGqZ/Gni7VfMzPbO/hOEmZmlkkuUGZmlkkuUGZmlkkuUGZmlkkuUGZmlkkuUGZmlkkuUGZmlkkuUGZmlkkuUGZmlkkuUGZmlkkuUGZmlkkuUGZmlkkuUGZmlkkuUGZmlkkuUGZmlkkuUGZmlkkuUGZmlkkuUGZmlkkuUGZmlkkuUGZmlkkuUGZmlkkuUGZmlkkuUGZmlkkuUGZmlkkuUGZmlkkuUGZmlkkuUGZmlkkuUGZmlkklKVCSxktaLmmVpMtLkcHMzLKt2QuUpLbATcBngHLgXEnlzZ3DzMyyrRQ9qDHAqoh4OSI+AGYBp5cgh5mZZVgpCtTBwOqc6TXpPDMzsw8pIpp3h9LngPERcWE6PQU4MiK+UavdVGBqOjkEWN4M8XoAbzfDfgrlPPk5T37Ok5/z5NeceT4WET1rz2zXTDvP9TrQL2e6bzpvFxFxK3Brc4UCkPRsRIxuzn3m4zz5OU9+zpOf8+SXhTylGOJ7BhgkaYCk9sA5wC9LkMPMzDKs2XtQEVEl6RvAI0Bb4KcRsbi5c5iZWbaVYoiPiHgYeLgU+25Asw4pFsB58nOe/JwnP+fJr+R5mv0iCTMzs0L4VkdmZpZJe2WBkrRT0gs5P/3ztP1Zeul77fljJT1Ux/wDJM2TtFnSjRnIc6KkSkkvpv8eX+I8Y3K2u1DSmaXMk7P8kPQ1u7SUeST1l/R+zrZvLmWedNlISb+XtDj9PepYqjySJtfadrWkw0uYZx9JM9LjslTSP+fL0gx52ku6Pc2zUNLYEmSo9z1QUkWabZWkGySpvv0WoiTnoJrB+xFxeJG2vQ34V2BE+lPqPG8Dp0bEG5JGkFx80tAHn4uZZxEwOr0Ypg+wUNKvIqKqRHlqXAfMKbBtsfO8tJvbL1oeSe2Au4ApEbFQ0gHAjlLliYi7gbvTbIcBD0TEC6XKA0wAOkTEYZI6A0sk3RMRr5Qoz5cB0jwHAnMkfSIiqpsxQ773wP9OMz5Ncp3BeAr/f/cRe2UPqi6SDpf0B0l/knS/pP3raDNe0jJJzwFn1bWdiNgSEU+RvEhZyPN8RLyRTi4GOknqUMI8W3OKUUdgj05yNlWetN0ZwJ9Jjs8eaco8TaEJ85wE/CkiFgJExDsRsbOEeXKdS3IrtN3WhHkC2Dct5J2AD4BNJcxTDjwOEBFvAhuBgj6rVOz3wPQP0v0i4g+RXNxwB3BGIdnqs7cWqE45Xdv703l3ANMiYiTwIjA9dwUlwxo/Bk4FKoDeLTDP2cBzEbG9lHkkHSlpcbqdrzbQeypqHkllwDTgqgYyNEue1ABJz0uaL+mYEucZDISkRyQ9J+myEufJ9XngnhLnuRfYAqwFXgN+EBHrS5hnIXCapHaSBqRt+9XRrhTvgQeT3LquRqNvY9cqhvgkdQW6RcT8dNYM4H9qrTMU+HNErEzXuYu/3mop83kkDQe+S/IXcUnzRMTTwHBJw4AZkuZERL4eZzHzXAn8KCI2q/Dh8GLmWQscEhHvSKoAHpA0PCLy/VVezDztgE8BnwC2Ao9JqoyIx0qUp2abRwJbI2JRnhzNkWcMsBM4CNgf+K2kRyPi5RLl+SkwDHgWeBX4XZqvOTM0m721QLUqkvoC9wPnRcRLpc5TIyKWStpMMk79bIliHAl8TtL3gG5AtaRtEVHQBS5NLe3dbk8fV0p6iaQXU6rjswZ4MiLeBpD0MPA3QL4C1RzOobDeU7FNAn4TETuANyUtIBlSy1egiiYdjbi4ZlrS74AVpchSh9dJbl1Xo87b2O2OvXWIbxcR8S6wIWc4ZQowv1azZUB/SQPT6XNbQh5J3YBfA5dHxIIM5BmQjtcj6WMkf5W9Uqo8EXFMRPSPiP7A9cB/7G5xauLj01PJd6Ih6ePAIHbzza6Jf58fAQ6T1Dl93Y4DlpQwD5LaABPZw/NPTZznNeD4NNe+wFHpuiXJk75O+6aPTwSqIqLB16s53gMjYi2wSdJRSoYrzgMe3J1t1NaaelDnAzcruRLnZeALuQsjYpuSO6j/WtJW4LdAl7o2JOkVYD+gvZIT8CcV8ktSpDzfAA4FrpB0RTrvpPQEainyfAq4XNIOoBr4Ws1f5yXK01SaKs+xwNU5x+erBZzTKFqeiNgg6TqSe2QG8HBE/LpUeVLHAqsbGEZrrjw3AbcrOacq4PaI+FMJ8xwIPCKpmqR3MqUEGfK9B34N+BnJBSVzaMQVfOA7SZiZWUa1iiE+MzNreVygzMwsk1ygzMwsk1ygzMwsk1ygzMwsk1ygzIpMzXC3drO9UWv6HJRZqTTH3drN9jruQZmVgBpxZ2lJx+X0xp6XVMwPKJuVjAuUWfE19Z2lLwW+nvbKjgHeL3J+s5JwgTIrvvcj4vD058x67ix9bK11PryzdPrdOnflLFsAXCfpH9PtNPR1JmYtkguUWQsTEdcCF5Lc72yBpKEljmRWFC5QZs2ssXeWljQwIl6MiO+S3OTVBcr2Sr6Kz6w0GnNn6W9JGkdyN/TFNPKO0WZZ5buZm5lZJnmIz8zMMskFyszMMskFyszMMskFyszMMskFyszMMskFyszMMskFyszMMskFyszMMun/AL/cf+jyFQ/YAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Temporary renaming for test reasons\n",
    "# when ready, make train_accuracies = accuraies you got from CSV\n",
    "train_accuracies = [93.4603939350999, 90.18205001062549, 90.91623504197177, 89.42269742743353, 90.21801082929609,90.14386717788175,\n",
    "                    90.39316721959483,90.01683737898134, 89.79699539105654, 91.21712150059939 ]\n",
    "#fold7 90.39316721959483,\n",
    "#fold8 90.01683737898134\n",
    "#fold9 89.79699539105654\n",
    "#fold10 91.21712150059939\n",
    "labels = ['Fold 1', 'Fold 2', 'Fold 3', 'Fold 4','Fold 5', 'Fold 6', 'Fold 7', 'Fold 8', 'Fold 9','Fold 10']\n",
    "\n",
    "\n",
    "x = np.arange(len(labels))  # the label locations\n",
    "width = 0.25  # the width of the bars\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "rects1 = ax.bar(x - width/2, train_accuracies, width, label='Training', color='orange')\n",
    "rects2 = ax.bar(x + width/2, test_accuracies, width, label='Validation', color='purple')\n",
    "\n",
    "# Add some text for labels, title and custom x-axis tick labels, etc.\n",
    "ax.set_xlabel('Folds')\n",
    "ax.set_ylabel('% Accuracy')\n",
    "ax.set_title('Training vs Validation Accuracy')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(labels)\n",
    "plt.legend(loc=\"lower center\",bbox_to_anchor=(0.5, 1.15), ncol=2)\n",
    "#figtext(.5,.9,'Foo Bar', fontsize=18, ha='center')\n",
    "\n",
    "#ax.bar_label(rects1, padding=3)\n",
    "#ax.bar_label(rects2, padding=3)\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.grid()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
